%% contents/interpolation.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Interpolation models for \glsfmtlong{dfo}}

\section{Introduction and motivation}

As mentioned in \cref{ch:introduction}, model-based \gls{dfo} methods necessitate to approximate locally the functions involved in optimization problems by simple functions, referred to as \emph{models} or \emph{surrogates}.
These models are used to construct subproblems that are in turn approximately minimized.
Examples of such functions commonly used in the literature are polynomials and \glspl{rbf}~\cite{Powell_2004a}.
In this thesis, we focus on linear and quadratic polynomial models, i.e., on polynomials of degree at most one and two, respectively.

Let~$\lpoly$ and~$\qpoly$ denote the spaces of linear and quadratic polynomials on~$\R^n$, respectively.
In a \gls{dfo} context, models from~$\lpoly$ or~$\qpoly$ are built for a real-valued function~$\obj$ without using derivatives.
This can be done by interpolation schemes based on function values.
More specifically, given a finite set of points~$\xpt \subseteq \R^n$, we construct a model~$\objm$ that interpolates the function~$\obj$ on~$\xpt$, i.e.,
\begin{equation}
    \label{eq:interpolation-conditions}
    \objm(y) = \obj(y), \quad \text{for~$y \in \xpt$}.
\end{equation}

The conditions~\cref{eq:interpolation-conditions} may be inconsistent.
In such a case, models can be built using regression schemes.
For example, a least-square regression model~$\objm$ minimizes
\begin{equation*}
    \sum_{\mathclap{y \in \xpt}} [\obj(y) - \objm(y)]^2.
\end{equation*}
Although there are successful methods that use regression models (see, e.g.,~\cite{Billups_Larson_Graf_2013,Conn_Scheinberg_Vicente_2008b}), the \gls{dfo} methods we present and develop in this thesis use interpolation models and ensure that the interpolation conditions are consistent and well-conditioned (see \cref{ch:pdfo,ch:cobyqa-introduction}).

It is also possible to use polynomials of degree higher than two.
However, we do not consider such models in this thesis due to the following observations.
\begin{enumerate}
    \item As shown in~\cite[thm.~2.5]{Wendland_2005}, the space of polynomials on~$\R^n$ of degree at most~$k$ has a dimension of
    \begin{equation*}
        \binom{n + k}{n} = \frac{1}{k!} \prod_{i = 1}^k (n + i) \ge \frac{n^k}{k!}.
    \end{equation*}
    Therefore, to determine a model from this space merely by the interpolation conditions~\cref{eq:interpolation-conditions}, we need in general~$\bigo(n^k)$ function values.
    This amount is unacceptable in a \gls{dfo} context unless~$k$ is small.
    It is possible to reduce this number with underdetermined interpolation, which is used by several optimization methods for~$k \le 2$ (see \cref{sec:underdetermined-interpolation}), including the method \gls{cobyqa} developed in this thesis (see \cref{ch:cobyqa-introduction}).
    Using underdetermined interpolation models with~$k \ge 3$ is out of the scope of this thesis, although it is an interesting research direction.
    \item The \gls{dfo} methods need to solve approximately subproblems (e.g., trust-region subproblems) built upon these models.
    Sophisticated models usually lead to complicated subproblems to solve.
    On the other hand, even with models that are not quadratic, practical algorithms normally solve the subproblems based on first- or second-order approximations of the models, e.g., calculate the approximate Cauchy point discussed in~\cite[\S~6.3.3]{Conn_Gould_Toint_2000}.
    Therefore, building polynomial models of degree higher than two may not be necessary.
\end{enumerate}

Although we do not study \gls{rbf} models, we mention that there also exist many \gls{dfo} methods based on these models.
Examples of such methods include \gls{orbit}~\cite{Wild_Regis_Shoemaker_2008}, \gls{conorbit}~\cite{Regis_Wild_2017}, and \gls{boosters}~\cite{Oeuvray_Bierlaire_2009}.

\section{Elementary concepts of multivariate interpolation}
\label{sec:multivariate-interpolation}

Before studying properties of multivariate interpolation, we must introduce the following notion of poisedness, which defines uniquely interpolants.

\begin{definition}[Poisedness]
    The set~$\xpt$ is \emph{poised} if the interpolation system~\cref{eq:interpolation-conditions} is consistent and has a unique solution for a given space in which~$\objm$ must lie.
\end{definition}

Let of first consider the problem of finding a linear model~$\objm \in \lpoly$ satisfying the interpolation system~\cref{eq:interpolation-conditions} whenever~$\card \xpt = \dim \lpoly = n + 1$.
In the natural basis of~$\lpoly$, the system~\cref{eq:interpolation-conditions} can be reformulated as
\begin{equation}
    \label{eq:linear-interpolation-conditions}
    \alpha_0 + \sum_{i = 1}^n \alpha_i y_i = \obj(y), \quad \text{for~$y \in \xpt$},
\end{equation}
where~$y_i$ denotes the~$i$th component of~$y \in \xpt$, and where~$\alpha_i \in \R$ for~$i \in \set{0, 1, \dots, n}$.
If~$\xpt$ is poised for linear interpolation, given~$\set{\alpha_0^{\ast}, \alpha_1^{\ast}, \dots, \alpha_n^{\ast}}$ the unique solution to the system~\cref{eq:linear-interpolation-conditions}, the linear model~$\objm$ is defined for~$x \in \R^n$ by
\begin{equation*}
    \objm(x) = \alpha_0^{\ast} + \sum_{i = 1}^n \alpha_i^{\ast} x_i,
\end{equation*}
and the vector~$\nabla \objm \equiv (\alpha_i^{\ast})_{i = 1, 2, \dots, n}$ is referred to as the \emph{simplex gradient} of~$\obj$ for the interpolation set~$\xpt$.
Such models are used for instance by \gls{cobyla}~\cite{Powell_1994}, a \gls{dfo} method for nonlinearly-constrained optimization detailed in \cref{subsec:cobyla}.

The problem of finding a quadratic model~$\objm \in \qpoly$ satisfying the interpolation system~\cref{eq:interpolation-conditions} whenever~$\card \xpt = \dim \qpoly = (n + 1)(n + 2) / 2$ is very similar.
Unlike linear model, quadratic models capture curvature information of the function~$\obj$, and are hence more precise.
Most recent model-based \gls{dfo} methods based on polynomial models use in fact quadratic models.
This is the case for instance for \gls{uobyqa}~\cite{Powell_2002}, a \gls{dfo} method for unconstrained optimization detailed in \cref{subsec:uobyqa}, which use the quadratic interpolation models mentioned above.
The new method we introduce in \cref{ch:cobyqa-introduction} also uses quadratic models, described in \cref{subsec:symmetric-broyden-updates}.
Therefore, we focus further considerations in this chapter on quadratic models.

\section{Overview of the Lagrange polynomials}
\label{sec:lagrange-polynomials}

For simplicity, we present in this section the Lagrange quadratic polynomials only.
Note that the theory presented below can be generalized to polynomials of any degree.

\begin{definition}[Lagrange quadratic polynomials]
    \label{def:lagrange-polynomials}
    Given a poised interpolation set~$\xpt \subseteq \R^n$ of~$(n + 1)(n + 2) / 2$ points, the Lagrange polynomial~$\lagp[y] \in \qpoly$ for~$y \in \xpt$ is the unique quadratic polynomial satisfying~$\lagp[y](y) = 1$ and
    \begin{equation*}
        \lagp[y](x) = 0, \quad \text{for~$x \in \xpt \setminus \set{y}$}.
    \end{equation*}
\end{definition}

We note that \cref{def:lagrange-polynomials} is well-defined, because the poisedness of the interpolation ensures the existence and the uniqueness of the Lagrange polynomials.
Moreover, as decribed by \cref{thm:lagrange-polynomials-basis}, the interpolant~$\objm$ can be formulated as a linear combinaison of the Lagrange polynomials, where the coefficients of the linear combinaison are exactly the values of~$\obj$ at the interpolation points.

\begin{theorem}
    \label{thm:lagrange-polynomials-basis}
    Given a poised interpolation set~$\xpt \subseteq \R^n$ of~$(n + 1)(n + 2) / 2$ points, the Lagrange polynomials~$\set{\lagp[y]}_{y \in \xpt}$ form a basis of~$\qpoly$.
    Moreover, the quadratic interpolant~$\objm$ of~$\obj$ on~$\xpt$ is given for any~$x \in \R^n$ by
    \begin{equation*}
        \objm(x) = \sum_{y \in \xpt} \obj(y) \lagp[y](x).
    \end{equation*}
\end{theorem}

In the context of \gls{dfo}, one of the most important application of the Lagrange polynomials is to measure the well-poisedness of the interpolation set~$\xpt$.
More specifically, \citeauthor{Ciarlet_Raviart_1972}~\cite{Ciarlet_Raviart_1972} showed that
\begin{equation*}
    \sup_{x \in \conv(\xpt)} \abs{\obj(x) - \objm(x)} \le \frac{\nu}{6} \sum_{y \in \xpt} \abs{\lagp[y](x)} \norm{x - y}^3,
\end{equation*}
whenever~$\obj$ is thrice differentiable in~$\conv(\xpt)$, where~$\conv(\xpt)$ denotes the convex hull of~$\xpt$, and where~$\nu$ is an upper bound on the absolute value of the third-order directional derivatives of~$\obj$ in~$\conv(\xpt)$.
Let~$\diam(\xpt)$ be the diameter of~$\xpt$, that is
\begin{equation*}
    \diam(\xpt) \eqdef \max_{x, y \in \xpt} \norm{x - y}.
\end{equation*}
We then clearly have
\begin{equation*}
    \sup_{x \in \conv(\xpt)} \abs{\obj(x) - \objm(x)} \le \frac{\nu}{12} (n + 1) (n + 2) \Lambda \diam(\xpt)^3,
\end{equation*}
where~$\Lambda$ is intimately related to the Lebesgue constant of~$\xpt$, and is defined by
\begin{equation*}
    \Lambda \eqdef \max_{y \in \xpt} \max_{x \in \conv(\xpt)} \abs{\lagp[y](x)}.
\end{equation*}
In some sense,~$\Lambda$ measures, therefore, the well-poisedness of the interpolation set~$\xpt$.
More details on this observation is given in \cref{sec:poisedness}.

\section{Poisedness of interpolation sets}
\label{sec:poisedness}

As before, the theory we develop in this section assumes that~$\objm \in \qpoly$.
It may be, however, generalized to any space of polynomials.

\subsection{Measuring the well poisedness of interpolation sets}

\begin{definition}[$\Lambda$-poisedness]
    A poised interpolation set~$\xpt \subseteq \R^n$ is said to be~$\Lambda$-poised in a compact set~$\mathcal{C} \subseteq \R^n$, for some~$\Lambda > 0$, if
    \begin{equation*}
        \Lambda \ge \max_{y \in \xpt} \max_{x \in \mathcal{C}} \abs{\lagp[y](x)}.
    \end{equation*}
\end{definition}

As we discussed before,~$\Lambda$ measures the quality of the interpolation set.

\begin{itemize}
    \item Definition of the~$\Lambda$-poisedness.
    \item The~$\Lambda$-poisedness measures the quality of the interpolation set.
    \item In practice we do not use it.
\end{itemize}

\subsection{Relationship with the conditioning of the interpolation system}

\begin{itemize}
    \item We consider here only the natural basis.
    \item The condition number is bounded by something that depends on~$\Lambda$.
\end{itemize}

\section{Underdetermined interpolation systems}
\label{sec:underdetermined-interpolation}

\begin{itemize}
    \item We focus on quadratic polynomials.
    \item Building a quadratic interpolation model necessitates~$\mathcal{O}(n^2)$ function evaluations.
    \item We want to reduce this number to~$\mathcal{O}(n)$.
    \item Freedom is bequeathed by minimizing a functional that reflects the regularity of the models.
    \item Two examples are given hereafter.
    \item The new method we introduce in \cref{ch:cobyqa-introduction} also uses quadratic models obtained by underdetermined interpolation.
\end{itemize}

\subsection{Least Frobenius norm quadratic models}

\begin{definition}[$\Lambda$-poisedness]
    \label{def:lambda-poisedness-minimum-norm}
    A poised interpolation set~$\xpt \subseteq \R^n$ in the minimum-norm sense is said to be~$\Lambda$-poised in the minimum-norm sense in a compact set~$\mathcal{C} \subseteq \R^n$, for some~$\Lambda > 0$, if
    \begin{equation*}
        \Lambda \ge \max_{y \in \xpt} \max_{x \in \mathcal{C}} \abs{\lagp[y](x)}.
    \end{equation*}
\end{definition}

\begin{itemize}
    \item $\Lambda$-poisedness.
    \item In the definition it is an inequality. If~$\Lambda = \max \dots$, then it is~$\Lambda$-poised.
    \item Factorization of the symmetric Broyden matrix.
    \item Updating the symmetric Broyden matrix.
    \item Decompositions of the Hessian matrices of the quadratic models.
    \item \gls{mnh}~\cite{Wild_2008}
\end{itemize}

\subsection{Quadratic models based on symmetric Broyden updates}
\label{subsec:symmetric-broyden-updates}

\begin{itemize}
    \item Explain the name.
    \item \gls{newuoa}~\cite{Powell_2006}
\end{itemize}

\section{An optimal number of interpolation points}

We study in this section an interpolation set that we will use in \cref{ch:cobyqa-introduction} of this thesis, where we introduce a new model-based \gls{dfo} method.
This interpolation set is adapted from~\cite{Powell_2001} as follows.
Let~$\delta > 0$ be fixed and for~$j \in \set{1, 2, \dots, 2n + 1}$, let~$z^j \in \R^n$ be
\begin{subequations}
    \label{eq:initial-interpolation-set}
    \begin{empheq}[left={z^j \eqdef \empheqlbrace}]{alignat=2}
        & 0,                        && \quad \text{if~$j = 1$,}\\
        & \delta e_{j - 1},         && \quad \text{if~$2 \le j \le n + 1$,}\\
        & -\delta e_{j - n - 1},    && \quad \text{otherwise,}
    \end{empheq}
\end{subequations}
where~$e_j \in \R^n$ denotes the~$i$th standard coordinate vector.
We then define the interpolation set~$\mathcal{Z}_m \subseteq \R^n$ for each~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ by
\begin{equation*}
    \mathcal{Z}_m \eqdef \set{z^1, z^2, \dots, z^m}.
\end{equation*}
Further, we denote by~$\ball[p][\ast](\mathcal{Z}_m)$\todo{Rename it to~$\ball[p][m]$} the smallest~$\ell_p$-norm ball containing~$\mathcal{Z}_m$, for~$p \in [1, \infty]$.
Note that we allow~$p = \infty$.
By the construction of~$\mathcal{Z}_m$, we observe that
\begin{equation}
    \label{eq:ball-initial}
    \ball[p][\ast](\mathcal{Z}_m) \equiv \ball[p](\delta) \eqdef \set{x \in \R^n : \norm{x}_p \le \delta}.
\end{equation}

Throughout this section, we denote by~$\lagp[i]$ (instead of~$\lagp[z^i]$) the minimum-norm Lagrange polynomial associated with~$\mathcal{Z}_m$ for~$z^i \in \mathcal{Z}_m$.
According to the~\cref{def:lambda-poisedness-minimum-norm} and the equation~\cref{eq:ball-initial}, the set~$\mathcal{Z}_m$ is~$\Lambda_p$-poised in~$\ball[p][\ast](\mathcal{Z}_m)$ in the minimum-norm sense with
\begin{equation*}
    \Lambda_p \eqdef \max_{1 \le i \le m} \max_{x \in \ball[p](\delta)} \abs{\lagp[i](x)}.
\end{equation*}
In the following, we develop bounds for~$\Lambda_p$ and we establish its value in some special cases\todo{Improve the sentence}.

\Cref{lem:lagrange-polynomials-initial} provides explicit formulae for~$\lagp[i]$ for all~$i \in \set{1, 2, \dots, m}$.
These formulae are given in~\cite[\S~3]{Powell_2006}, without a proof.

\begin{lemma}
    \label{lem:lagrange-polynomials-initial}
    For each~$m \in \set{n + 2, n + 3, \dots,  2n + 1}$ and all~$x \in \R^n$, we have
    \begin{empheq}[left={\lagp[i](x) = \empheqlbrace}]{alignat*=2}
        & 1 - \frac{1}{\delta^2} \sum_{j = 1}^{m - n - 1} x_j^2 - \frac{1}{\delta} \sum_{j = m - n}^n x_j,  && \quad \text{if~$i = 1$,}\\
        & \frac{x_{i - 1}^2}{2 \delta^2} + \frac{x_{i - 1}}{2 \delta},                                      && \quad \text{if~$2 \le i \le m - n$,}\\
        & \frac{x_{i - 1}}{\delta},                                                                         && \quad \text{if~$m - n + 1 \le i \le n + 1$,}\\
        & \frac{x_{i - 1}^2}{2 \delta^2} - \frac{x_{i - 1}}{2 \delta}.                                      && \quad \text{otherwise.}
    \end{empheq}
    In the formulation of~$\lagp[1]$, if~$m = 2n + 1$, we define~$\sum_{j = m - n}^n x_j = 0$.
\end{lemma}

\begin{proof}
    Let~$i \in \set{1, 2, \dots, m}$ be fixed and let~$\lagp$ be a quadratic polynomial that satisfies
    \begin{subequations}
        \label{eq:lagrange-polynomials-initial-proof}
        \begin{empheq}[left={\lagp(z^j) = \empheqlbrace}]{alignat=2}
            & 1,    && \quad \text{if~$j = i$,}\\
            & 0,    && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
    First, it is straightforward to verify that~$\lagp[i]$ satisfies the interpolation conditions~\cref{eq:lagrange-polynomials-initial-proof}.
    Hence, it suffices to show that the Frobenius norm of its Hessian matrix is least.
    According to the equation~\cref{eq:initial-interpolation-set}, for any~$j \in \set{1, 2, \dots, m - n - 1}$, we have~$z^1 = 0$,~$z^{j + 1} = \delta e_j$, and~$z^{n + j + 1} = - \delta e_j$.
    Therefore,
    \begin{empheq}[left=\empheqlbrace]{alignat*=1}
        & \lagp(z^{j + 1}) = \lagp(z^1) + \delta \inner{\nabla \lagp(z^1), e_j} + \frac{\delta^2}{2} \inner{e_j, (\nabla^2 \lagp) e_j},\\
        & \lagp(z^{n + j + 1}) = \lagp(z^1) - \delta \inner{\nabla \lagp(z^1), e_j} + \frac{\delta^2}{2} \inner{e_j, (\nabla^2 \lagp) e_j},
    \end{empheq}
    and hence,
    \begin{equation*}
        \inner{e_j, (\nabla^2 \lagp) e_j} = \frac{\lagp(z^{j + 1}) + \lagp(z^{n + j + 1}) - 2 \lagp(z^1)}{\delta^2}.
    \end{equation*}
    This fixes the first~$m - n - 1$ diagonal entries of~$\nabla^2 \lagp$, which are exactly those of~$\nabla^2 \lagp[i]$.
    Since all the other entries of~$\nabla^2 \lagp[i]$ are zero, we have
    \begin{equation*}
        \norm{\nabla^2 \lagp[i]}_{\mathsf{F}}^2 \le \norm{\nabla^2 \lagp}_{\mathsf{F}}^2,
    \end{equation*}
    which completes the proof.
\end{proof}

\begin{lemma}
    \label{lem:lambda-poisedness-initial-simple}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation}
        \label{eq:lambda-poisedness-initial-simple}
        \Lambda_p = \max_{x \in \ball[p](\delta)} \abs{\lagp[1](x)}.
    \end{equation}
\end{lemma}

\begin{proof}
    For each~$i \in \set{2, 3, \dots, n + 1}$, according to~\cref{lem:lagrange-polynomials-initial},~$\lagp[i](x)$ depends only on~$x_{i - 1}$ for all~$x \in \R^n$, and hence
    \begin{equation*}
        \max_{x \in \ball[p](\delta)} \abs{\lagp[i](x)} = \max_{t \in [-\delta, \delta]} \abs{\lagp[i](t e_{i - 1})} = 1.
    \end{equation*}
    Similarly, for each~$i \in \set{n + 2, n + 3, \dots, m}$, since~$\lagp[i](x)$ depends only on~$x_{i - n - 1}$ for all~$x \in \R^n$, we have
    \begin{equation*}
        \max_{x \in \ball[p](\delta)} \abs{\lagp[i](x)} = \max_{t \in [-\delta, \delta]} \abs{\lagp[i](t e_{i - n - 1})} = 1.
    \end{equation*}
    Noting that~$\lagp[1](z^1) = 1$ and~$z^1 \in \ball[p](\delta)$, we thus have
    \begin{equation*}
        \Lambda_p = \max_{x \in \ball[p](\delta)} \abs{\lagp[1](x)}.
    \end{equation*}
\end{proof}

\begin{theorem}
    \label{thm:lambda-poisedness-initial}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation*}
        1 + (2n + 1 - m)^{\frac{p - 1}{p}} \le \Lambda_p \le n,
    \end{equation*}
    where we assume that~$0^0 = 0$ and that the lower bound is~$2n + 2 - m$ for~$p = \infty$.
\end{theorem}

\begin{proof}
    We will establish the bounds using the formulation of~$\Lambda_p$ in~\cref{lem:lambda-poisedness-initial-simple}.
    For the lower bound, by considering only the points in~$\R^n$ whose leading~$m - n - 1$ components are zeros and whose remaining~$2n + 1 - m$ components are equal, we have
    \begin{equation*}
        \Lambda_p \ge \max_{t \in \R} \set{1 - \delta^{-1} (2n + 1 - m) t : (2n + 1 - m) \abs{t}^p \le \delta^p} = 1 + (2n + 1 - m)^{\frac{p - 1}{p}}.
    \end{equation*}
    
    We now prove the upper bound.
    Note that for any~$p \ge 1$, we have~$\ball[p](\delta) \subseteq \ball[\infty](\delta)$, so that~$\Lambda_p \le \Lambda_{\infty}$.
    Therefore, we only need to show that~$\Lambda_{\infty} \le n$.
    Considering both~$\lagp[1]$ and~$-\lagp[1]$, we obtain
    \begin{equation*}
        \Lambda_{\infty} = \max_{x \in \ball[\infty](\delta)} \abs{\lagp[1](x)} = \max \set{2n + 2 - m, n - 1} \le n.
    \end{equation*}
\end{proof}

\begin{proposition}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{subequations}
        \label{eq:lambda-poisedness-1}
        \begin{empheq}[left={\Lambda_1 = \empheqlbrace}]{alignat=2}
            & 2,    && \quad \text{if~$n + 2 \le m \le 2n$,}\\
            & 1,    && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
\end{proposition}

\begin{proof}
    According to~\cref{thm:lambda-poisedness-initial},~$\Lambda_1$ is lower bounded by the right-hand side of~\cref{eq:lambda-poisedness-1}.
    Therefore, we only need to prove that this right-hand side is also a lower bound for~$\Lambda_1$, using the formulation in \cref{lem:lambda-poisedness-initial-simple}.

    For any~$x \in \ball[1](\delta)$, we have
    \begin{equation*}
        \lagp[1](x) \le 1 - \frac{1}{\delta} \sum_{j = m - n}^n x_j \le 1 + \frac{1}{\delta} \sum_{j = m - n}^n \abs{x_j}.
    \end{equation*}
    Therefore,
    \begin{subequations}
        \label{eq:eq:lambda-poisedness-1-proof-1}
        \begin{empheq}[left={\lagp[1](x) \le \empheqlbrace}]{alignat=2}
            & 2,    && \quad \text{if~$n + 2 \le m \le 2n$,}\\
            & 1,    && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
    On the other hand,
    \begin{subequations}
        \label{eq:eq:lambda-poisedness-1-proof-2}
        \begin{align}
            \lagp[1](x) &= 1 - \sum_{j = 1}^{m - n - 1} \frac{x_j^2}{\delta^2} - \sum_{j = m - n}^n \frac{x_j}{\delta}\\
                        &\ge 1 - \sum_{j = 1}^{m - n - 1} \frac{\abs{x_j}}{\delta} - \sum_{j = m - n}^n \frac{\abs{x_j}}{\delta}\\
                        &\ge 1 - \frac{\norm{x}_1}{\delta} \ge 0.
        \end{align}
    \end{subequations}
    We conclude the proof by combining~\cref{eq:eq:lambda-poisedness-1-proof-1,eq:eq:lambda-poisedness-1-proof-2} with~\cref{lem:lambda-poisedness-initial-simple}.
\end{proof}

\begin{proposition}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{equation}
        \label{eq:lambda-poisedness-2}
        \Lambda_2 = 1 + \sqrt{2n + 1 - m}.
    \end{equation}
\end{proposition}

\begin{proof}
    If~$m = 2n + 1$, \cref{lem:lagrange-polynomials-initial} tells us that~$\lagp[1](x) = 1 - \delta^{-2} \norm{x}_2^2$ for~$x \in \ball[2](\delta)$.
    Therefore, \cref{lem:lambda-poisedness-initial-simple} directly provides~$\Lambda_2 = 1$.
    We now focus on the case with~$n + 2 \le m < 2n + 1$.

    According to~\cref{thm:lambda-poisedness-initial},~$\Lambda_2$ is lower bounded by the right-hand side of~\cref{eq:lambda-poisedness-2}.
    Therefore, we only need to prove that this right-hand side is also a lower bound for~$\Lambda_2$, using the formulation in \cref{lem:lambda-poisedness-initial-simple}.
    
    For any~$x \in \ball[2](\delta)$, we have
    \begin{subequations}
        \begin{align}
            \lagp[1](x) &= 1 - \frac{1}{\delta^2} \sum_{j = 1}^{m - n - 1} x_j^2 - \frac{1}{\delta} \sum_{j = m - n}^n x_j\\
                        &\ge 1 - \frac{1}{\delta^2} \bigg( \delta^2 - \sum_{j = m - n}^n x_j^2 \bigg) - \frac{1}{\delta} \sum_{j = m - n}^n x_j = \sum_{j = m - n}^n \frac{x_j}{\delta} \bigg( 1 - \frac{x_j}{\delta} \bigg)\\
                        &\ge \min_{y \in \ball[2](1)} \sum_{j = m - n}^n y_j (1 - y_j). \label{eq:lambda-poisedness-initial-2-proof}
        \end{align}
    \end{subequations}
    Let~$y^{\ast} \in \ball[2](1)$ be a minimizer in~\cref{eq:lambda-poisedness-initial-2-proof}.
    \Cref{thm:first-order-necessary-conditions} ensures that there exists a Lagrange multiplier~$\lambda^{\ast} \ge 0$ such that~$1 - 2 y_j^{\ast} + 2 \lambda^{\ast} y_j^{\ast} = 0$ for all~$j \in \set{m - n, \dots, n}$.
    Therefore, the last~$(2n + 1 - m)$ components of~$y^{\ast}$ are equal, and hence\todo{The~$t(1 - t)$ looks strange},
    \begin{align*}
        \lagp[1](x) \ge \min_{y \in \ball[2](1)} \sum_{j = m - n}^n y_j (1 - y_j)   &= \min_{t \in \R} \set{(2n + 1 - m) t (1 - t) : (2n + 1 - m) t^2 \le 1}\\
                                                                                    &= -1 - \sqrt{2n + 1 - m}.
    \end{align*}

    Furthermore,
    \begin{equation*}
        \lagp[1](x) \le 1 + \sum_{j = m - n}^n \frac{\abs{x_j}}{\delta} \le 1 + \sqrt{2n + 1 - m} \sum_{j = m - n}^n \frac{x_j^2}{\delta^2} \le 1 + \sqrt{2n + 1 - m}.
    \end{equation*}
    Therefore,~$\abs{\lagp[1](x)} \le 1 + \sqrt{2n + 1 - m}$ and hence, according to~\cref{lem:lambda-poisedness-initial-simple,thm:lambda-poisedness-initial}, we have
    \begin{equation*}
        \Lambda_2 = \max_{x \in \ball[2](\delta)} \abs{\lagp[1](x)} = 1 + \sqrt{2n + 1 - m},
    \end{equation*}
    which concludes the proof.
\end{proof} 

\begin{lemma}
    For any~$p \ge 1$ and~$q \ge 1$, we have
    \begin{empheq}[left={\max\limits_{x \in \ball[q](1)} \norm{x}_p = \empheqlbrace}]{alignat*=2}
        & 1,                    && \quad \text{if~$p \ge q$,}\\
        & n^{\frac{q - p}{pq}}, && \quad \text{otherwise.}
    \end{empheq}
\end{lemma}

\begin{proof}
    To do.
\end{proof}

\begin{proposition}
    For any~$p \ge 1$, if~$m = 2n + 1$, then
    \begin{equation*}
        \Lambda_p = \max \set[\big]{1, n^{\frac{p - 2}{p}} - 1}.
    \end{equation*}
\end{proposition}

\begin{proof}
    It is clear that
    \begin{equation*}
        \max_{x \in \ball[p](\delta)} \lagp[1](x) = \max_{x \in \ball[p](\delta)} 1 - \frac{\norm{x}_2^2}{\delta^2} = 1.
    \end{equation*}
    Moreover, we have
    \begin{equation*}
        \max_{x \in \ball[p](\delta)} -\lagp[1](x) = \max_{x \in \ball[p](\delta)} \frac{\norm{x}_2^2}{\delta^2} - 1 = \max_{t \in \R} \set[\bigg]{\frac{n t^2}{\delta^2} - 1 : n \abs{t}^p \le \delta^p} = n^{\frac{p - 2}{p}} - 1,
    \end{equation*}
    which concludes the proof.
\end{proof}

\todo[noline]{Explain how these results could be generalized}
