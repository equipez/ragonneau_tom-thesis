%% contents/interpolation.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Interpolation models for \glsfmtlong{dfo}}

\section{Introduction and motivation}

As mentioned in \cref{ch:introduction}, model-based \gls{dfo} methods necessitate to approximate locally the functions involved in optimization problems by simple functions, referred to as \emph{models} or \emph{surrogates}.
These models are used to construct subproblems that are in turn approximately minimized.
Examples of such functions commonly used in the literature are polynomials and \glspl{rbf}~\cite{Powell_2004a}.
In this thesis, we focus on linear and quadratic polynomial models, i.e., on polynomials of degree at most one and two, respectively.

Let~$\lpoly$ and~$\qpoly$ denote the spaces of linear and quadratic polynomials on~$\R^n$, respectively.
In a \gls{dfo} context, models from~$\lpoly$ or~$\qpoly$ are built for a real-valued function~$\obj$ without using derivatives.
This can be done by interpolation schemes based on function values.
More specifically, given a finite set of points~$\xpt \subseteq \R^n$, we construct a model~$\objm$ that interpolates the function~$\obj$ on~$\xpt$, i.e.,
\begin{equation}
    \label{eq:interpolation-conditions}
    \objm(y) = \obj(y), \quad \text{for~$y \in \xpt$}.
\end{equation}

The conditions~\cref{eq:interpolation-conditions} may be inconsistent.
In such a case, models can be built using regression schemes.
For example, a least-square regression model~$\objm$ minimizes
\begin{equation*}
    \sum_{\mathclap{y \in \xpt}} [\obj(y) - \objm(y)]^2.
\end{equation*}
Although there are successful methods that use regression models (see, e.g.,~\cite{Billups_Larson_Graf_2013,Conn_Scheinberg_Vicente_2008b}), the \gls{dfo} methods we present and develop in this thesis use interpolation models and ensure that the interpolation conditions are consistent and well-conditioned (see \cref{ch:pdfo,ch:cobyqa-introduction}).

It is also possible to use polynomials of degree higher than two.
However, we do not consider such models in this thesis due to the following observations.
\begin{enumerate}
    \item As shown in~\cite[thm.~2.5]{Wendland_2005}, the space of polynomials on~$\R^n$ of degree at most~$k$ has a dimension of
    \begin{equation*}
        \binom{n + k}{n} = \frac{1}{k!} \prod_{i = 1}^k (n + i) \ge \frac{n^k}{k!}.
    \end{equation*}
    Therefore, to determine a model from this space merely by the interpolation conditions~\cref{eq:interpolation-conditions}, we need in general~$\bigo(n^k)$ function values.
    This amount is unacceptable in a \gls{dfo} context unless~$k$ is small.
    It is possible to reduce this number with underdetermined interpolation, which is used by several optimization methods for~$k \le 2$ (see \cref{sec:underdetermined-interpolation}), including the method \gls{cobyqa} developed in this thesis (see \cref{ch:cobyqa-introduction}).
    Using underdetermined interpolation models with~$k \ge 3$ is out of the scope of this thesis, although it is an interesting research direction.
    \item The \gls{dfo} methods need to solve approximately subproblems (e.g., trust-region subproblems) built upon these models.
    Sophisticated models usually lead to complicated subproblems to solve.
    On the other hand, even with models that are not quadratic, practical algorithms normally solve the subproblems based on first- or second-order approximations of the models, e.g., calculate the approximate Cauchy point discussed in~\cite[\S~6.3.3]{Conn_Gould_Toint_2000}.
    Therefore, building polynomial models of degree higher than two may not be necessary.
\end{enumerate}

Although we do not study \gls{rbf} models, we mention that there also exist many \gls{dfo} methods based on these models.
Examples of such methods include \gls{orbit}~\cite{Wild_Regis_Shoemaker_2008}, \gls{conorbit}~\cite{Regis_Wild_2017}, and \gls{boosters}~\cite{Oeuvray_Bierlaire_2009}.

\section{Elementary concepts of multivariate interpolation}
\label{sec:multivariate-interpolation}

Before studying properties of multivariate interpolation, we must introduce the following notion of poisedness, which defines uniquely interpolants.

\begin{definition}[Poisedness]
    Let the space in which~$\objm$ must lie be fixed.
    The set~$\xpt$ is \emph{poised} if the interpolation system~\cref{eq:interpolation-conditions} is consistent and has a unique solution for any function~$\obj$.
\end{definition}

Let of first consider the problem of finding a linear model~$\objm \in \lpoly$ satisfying the interpolation system~\cref{eq:interpolation-conditions} whenever~$\card \xpt = \dim \lpoly = n + 1$.
In the natural basis of~$\lpoly$, the system~\cref{eq:interpolation-conditions} can be reformulated as
\begin{equation}
    \label{eq:linear-interpolation-conditions}
    \alpha_0 + \sum_{i = 1}^n \alpha_i y_i = \obj(y), \quad \text{for~$y \in \xpt$},
\end{equation}
where~$y_i$ denotes the~$i$th component of~$y \in \xpt$, and where~$\alpha_i \in \R$ for~$i \in \set{0, 1, \dots, n}$.
If~$\xpt$ is poised for linear interpolation, given~$\set{\alpha_0^{\ast}, \alpha_1^{\ast}, \dots, \alpha_n^{\ast}}$ the unique solution to the system~\cref{eq:linear-interpolation-conditions}, the linear model~$\objm$ is defined for~$x \in \R^n$ by
\begin{equation*}
    \objm(x) = \alpha_0^{\ast} + \sum_{i = 1}^n \alpha_i^{\ast} x_i,
\end{equation*}
and the vector~$\nabla \objm \equiv (\alpha_1^{\ast}, \alpha_2^{\ast}, \dots, \alpha_n^{\ast})$ is referred to as the \emph{simplex gradient} of~$\obj$ for the interpolation set~$\xpt$.
Such models are used for instance by \gls{cobyla}~\cite{Powell_1994}, a \gls{dfo} method for nonlinearly-constrained optimization detailed in \cref{subsec:cobyla}.

The problem of finding a quadratic model~$\objm \in \qpoly$ satisfying the interpolation system~\cref{eq:interpolation-conditions} whenever~$\card \xpt = \dim \qpoly = (n + 1)(n + 2) / 2$ is very similar.
Unlike linear model, quadratic models capture curvature information of the function~$\obj$, and are hence more precise.
Most recent model-based \gls{dfo} methods based on polynomial models use in fact quadratic models.
This is the case for instance for \gls{uobyqa}~\cite{Powell_2002}, a \gls{dfo} method for unconstrained optimization detailed in \cref{subsec:uobyqa}, which use the quadratic interpolation models mentioned above.
The new method we introduce in \cref{ch:cobyqa-introduction} also uses quadratic models, which are described in \cref{subsec:symmetric-broyden-updates}.
Therefore, we focus further considerations in this chapter on quadratic models.

\section{Overview of the Lagrange polynomials}
\label{sec:lagrange-polynomials}

As mentioned above, we present in this section the Lagrange quadratic polynomials only.
Note that the theory presented below can be generalized to polynomials of any degree.

\begin{definition}[Lagrange quadratic polynomials]
    \label{def:lagrange-polynomials}
    Given a poised interpolation set~$\xpt \subseteq \R^n$ of~$(n + 1)(n + 2) / 2$ points, the Lagrange polynomial~$\lagp[y] \in \qpoly$ for~$y \in \xpt$ is the unique quadratic polynomial satisfying
    \begin{empheq}[left={\lagp[y](x) = \empheqlbrace}]{alignat*=2}
        & 1,    && \quad \text{if~$x = y$,}\\
        & 0,    && \quad \text{if~$x \in \xpt \setminus \set{y}$.}
    \end{empheq}
\end{definition}

We note that \cref{def:lagrange-polynomials} is well-defined, because the poisedness of the interpolation ensures the existence and the uniqueness of the Lagrange polynomials.
Moreover, as decribed by \cref{thm:lagrange-polynomials-basis}, the interpolant~$\objm$ can be formulated as a linear combinaison of the Lagrange polynomials, where the coefficients of the linear combinaison are exactly the values of~$\obj$ at the interpolation points.

\begin{theorem}
    \label{thm:lagrange-polynomials-basis}
    Given a poised interpolation set~$\xpt \subseteq \R^n$ of~$(n + 1)(n + 2) / 2$ points, the Lagrange polynomials~$\set{\lagp[y]}_{y \in \xpt}$ form a basis of~$\qpoly$.
    Moreover, the quadratic interpolant~$\objm$ of~$\obj$ on~$\xpt$ is given for any~$x \in \R^n$ by
    \begin{equation*}
        \objm(x) = \sum_{y \in \xpt} \obj(y) \lagp[y](x).
    \end{equation*}
\end{theorem}

In the context of \gls{dfo}, one of the most important application of the Lagrange polynomials is to measure the well-poisedness of the interpolation set~$\xpt$.
More specifically, \citeauthor{Ciarlet_Raviart_1972}~\cite{Ciarlet_Raviart_1972} showed that
\begin{equation}
    \label{eq:Ciarlet_Raviart_0}
    \sup_{x \in \conv \xpt} \abs{\obj(x) - \objm(x)} \le \frac{\nu}{6} \sum_{y \in \xpt} \abs{\lagp[y](x)} \norm{x - y}^3,
\end{equation}
whenever~$\obj$ is thrice differentiable in~$\conv \xpt$, where~$\conv \xpt$ denotes the convex hull of~$\xpt$, and where~$\nu$ is an upper bound on the absolute value of the third-order directional derivatives of~$\obj$ in~$\conv \xpt$.
We simplify this bound as follows.
Let~$\diam \xpt$ be the diameter of~$\xpt$, that is
\begin{equation*}
    \diam \xpt \eqdef \max_{x, y \in \xpt} \norm{x - y}.
\end{equation*}
We then clearly have
\begin{equation*}
    \sup_{x \in \conv \xpt} \abs{\obj(x) - \objm(x)} \le \frac{\nu \Lambda}{6} (\card \xpt) (\diam\xpt)^3,
\end{equation*}
where~$\Lambda$ is intimately related to the Lebesgue constant of~$\xpt$, and is defined by
\begin{equation}
    \label{eq:lambda-poisedness-convex-hull}
    \Lambda \eqdef \max_{y \in \xpt} \max_{x \in \conv \xpt} \abs{\lagp[y](x)}.
\end{equation}
In some sense,~$\Lambda$ measures, therefore, the well-poisedness of the interpolation set~$\xpt$.
This observation underlies a notion presented in \cref{sec:poisedness}, namely the~$\Lambda$-poisedness.

\section{Poisedness of interpolation sets}
\label{sec:poisedness}

As before, the theory we develop in this section assumes that~$\objm \in \qpoly$.
It may be, however, generalized to any space of polynomials.

\subsection{Measuring the well poisedness of interpolation sets}

\begin{definition}[$\Lambda$-poisedness]
    A poised interpolation set~$\xpt \subseteq \R^n$ is said to be~$\Lambda$-poised in a compact set~$\mathcal{C} \subseteq \R^n$, for some~$\Lambda > 0$, if
    \begin{equation*}
        \Lambda \ge \max_{y \in \xpt} \max_{x \in \mathcal{C}} \abs{\lagp[y](x)}.
    \end{equation*}
\end{definition}

If~$\xpt$ is~$\Lambda_0$-poised in~$\mathcal{C}$, it is obviously~$\Lambda$-poised in~$\mathcal{C}$ for any~$\Lambda \ge \Lambda_0$.
Note that the definition of~$\Lambda$-poisedness is closely related to the one in~\cref{eq:lambda-poisedness-convex-hull}, since~$\conv \xpt$ is compact.
Therefore, to some extend,~$\Lambda$ measures the quality (in a given compact set) of the interpolant that is related to the interpolation set.
Note that in this definition, the compact set~$\mathcal{C} \subseteq \R^n$ may or may not contain in the interpolation points in~$\xpt$.

In practice, however, we do not use the~$\Lambda$-poisedness of interpolation directly.
This is because determining whether a set is~$\Lambda$-poised is a difficult problem, even if the compact set~$\mathcal{C} \subseteq \R^n$ is simple (e.g., a ball).
It is a theoretical tool that we mostly use to justify some design choices of algorithms.
More discussion on the properties of a~$\Lambda$-poised interpolation set is given in~\cite[\S~3.3]{Conn_Scheinberg_Vicente_2009b}.

\subsection{Relationship with the conditioning of the interpolation system}

As in the linear case, a quadratic model~$\objm \in \qpoly$ satisfying the interpolation conditions~\cref{eq:interpolation-conditions} whenever~$\card \xpt = \dim \qpoly = (n + 1)(n + 2) / 2$ can be formulated as follows.
In the natural basis of~$\qpoly$, the system~\cref{eq:interpolation-conditions} can be formulated as
\begin{equation}
    \label{eq:quadratic-interpolation-conditions}
    \alpha_0 + \sum_{i = 1}^n \alpha_i y_i + \frac{1}{2} \sum_{i = 1}^n \alpha_{n + i} y_i^2 + \sum_{i = 2}^{n} \sum_{j = 1}^{i - 1} \alpha_{2n + \theta(i, j)} y_i y_j = \obj(y), \quad \text{for~$y \in \xpt$},
\end{equation}
where~$\theta(i, j) \eqdef j + (i - 1)(i - 2) / 2$.
We denote by~$M \in \R^{\card \xpt \times \card \xpt}$ the matrix whose rows are defined for~$y \in \xpt$ by
\begin{equation*}
    \begin{bmatrix}
        1   & y_1   & y_2   & \dots & y_n   & y_1^2 / 2 & y_2^2 / 2 & \dots & y_n^2 / 2 & y_2 y_1   & y_3 y_1   & \dots & y_n y_{n - 1}
    \end{bmatrix}.
\end{equation*}
The matrix~$M$ is, therefore, the matrix representation of the linear system~\cref{eq:quadratic-interpolation-conditions}.
In the following,~$\cond(M)$ denotes the condition number of~$M$, i.e.,
\begin{empheq}[left={\cond(M) \eqdef \empheqlbrace}]{alignat*=2}
    & \norm{M} \norm{M^{-1}},   && \quad \text{if~$M$ is nonsingular,}\\
    & \infty,                   && \quad \text{otherwise.}
\end{empheq}

The following theorem highlights the relationships between the~$\Lambda$-poisedness of the interpolation set~$\xpt$ with~$\cond(M)$.

\begin{theorem}
    \label{thm:lambda-poisedness-conditioning}
    We assume that the interpolation set~$\xpt$ satisfies~$0 \in \xpt \subseteq \ball[2](1)$ and
    \begin{equation*}
        \max_{y \in \xpt} \norm{y} = 1.
    \end{equation*}
    If the above-defined matrix~$M$ is nonsingular and~$\cond(M) \le \Lambda$, then the interpolation set~$\xpt$ is~$(\card \xpt)^{3/4} \Lambda$-poised in~$\ball[2](1)$.
    Moreover, if the interpolation set~$\xpt$ is~$\Lambda$-poised in~$\ball[2](1)$, then the matrix~$M$ is nonsingular and
    \begin{equation*}
        \cond(M) \le \theta (\card \xpt)^{3/4} \Lambda,
    \end{equation*}
    for some~$\theta > 0$ independent of~$\xpt$ and~$\Lambda$.
\end{theorem}

A proof of \cref{thm:lambda-poisedness-conditioning} is given in~\cite[\S~3.4]{Conn_Scheinberg_Vicente_2009b}.
Note that the assumptions on the interpolation set~$\xpt$ are weak, as any interpolation set can be shifted and scaled to satisfy such properties.
According to \cref{thm:lambda-poisedness-conditioning}, the lower is~$\Lambda$, the better is the conditioning of the interpolation system.

Long story short, if~$\xpt$ is~$\Lambda$-poised in a compact set~$\mathcal{C} \subseteq \R^n$ and~$\Lambda$ is reasonable low, then we can expect the quadratic model~$\objm$ to represent reasonably well~$\obj$ in~$\mathcal{C}$.
This is because we can expect the right-hand side of the inequality~\cref{eq:Ciarlet_Raviart_0} to be relatively low if~$\conv \xpt \subseteq \mathcal{C}$, and because the interpolation system is then well-conditioned.

\section{Underdetermined interpolation systems}
\label{sec:underdetermined-interpolation}

As before, we focus on quadratic interpolation in this section.
We saw that building a quadratic interpolant requires~$\mathcal{O}(n^2)$ function evaluations.
Therefore, if a \gls{dfo} method employs such models,~$\mathcal{O}(n^2)$ function evaluations must be made only to build the first quadratic model.
Moreover, the position of the points at which the function is evaluated are defined by~$\xpt$ before starting any evaluation.
Hence, they are unlikely to provide good iterates for the \gls{dfo} method.
To reduce this amount of function evaluations, we use underdetermined interpolations, as presented in this section.

Underdetermined interpolation works as follows.
Assume that we are given an interpolation set~$\xpt \subseteq \R^n$ such that the interpolation conditions~\cref{eq:interpolation-conditions} are consistent.
The number of interpolation may however be lower than~$\dim \qpoly$.
Further, let~$\mathcal{F}$ be a functional that reflects the regularity of the interpolants.
Two examples of such a functional are given in \cref{subsec:least-frobenius-norm-models,subsec:symmetric-broyden-updates}.
A quadratic model~$\objm$ of~$\obj$ can then be defined as a solution of
\begin{align*}
    \min        & \quad \mathcal{F}(Q)\\
    \text{s.t.} & \quad Q(y) = \obj(y), ~ y \in \xpt\\
                & \quad Q \in \qpoly.
\end{align*}

The notion of poisedness is adapted as follows.

\begin{definition}[Poisedness]
    The set~$\xpt$ is \emph{poised} if the interpolation system~\cref{eq:interpolation-conditions} is consistent for any function~$\obj$.
\end{definition}

Of course, we do not require the unicity of the solution to the interpolation system~\cref{eq:interpolation-conditions} because we would then have~$\card \xpt = \dim \qpoly$.

\subsection{Least Frobenius norm quadratic models}
\label{subsec:least-frobenius-norm-models}

One of the simplest functional~$\mathcal{F}$ mentioned above is the norm of the Hessian matrix of the interpolants.
In this context, a quadratic model~$\objm$ solves
\begin{align*}
    \min        & \quad \norm{\nabla^2 Q}_{\mathsf{F}}\\
    \text{s.t.} & \quad Q(y) = \obj(y), ~ y \in \xpt\\
                & \quad Q \in \qpoly,
\end{align*}
where~$\norm{\cdot}_{\mathsf{F}}$ denotes the Frobenius-norm operator.
This norm is chosen because it can be easily numerically computed, unlike the matrix~$\ell_2$-norms for example.

\begin{definition}[Minimum-norm Lagrange quadratic polynomials]
    \label{def:lagrange-polynomials-minimum-norm}
    For any point~$y \in \xpt$, given an interpolation set~$\xpt \subseteq \R^n$, the polynomial~$\lagp[y] \in \qpoly$ is referred to as the minimum-norm Lagrange quadratic polynomial if it solves
    \begin{align*}
        \min        & \quad \norm{\nabla^2 Q}_{\mathsf{F}}\\
        \text{s.t.} & \quad Q(y) = 1,\\
                    & \quad Q(x) = 0, ~ x \in \xpt \setminus \set{y},\\
                    & \quad Q \in \qpoly.
    \end{align*}
\end{definition}

\begin{definition}[$\Lambda$-poisedness]
    \label{def:lambda-poisedness-minimum-norm}
    A poised interpolation set~$\xpt \subseteq \R^n$ in the minimum-norm sense is said to be~$\Lambda$-poised in the minimum-norm sense in a compact set~$\mathcal{C} \subseteq \R^n$, for some~$\Lambda > 0$, if
    \begin{equation*}
        \Lambda \ge \max_{y \in \xpt} \max_{x \in \mathcal{C}} \abs{\lagp[y](x)}.
    \end{equation*}
\end{definition}

\begin{itemize}
    \item $\Lambda$-poisedness.
    \item In the definition it is an inequality. If~$\Lambda = \max \dots$, then it is~$\Lambda$-poised.
    \item Factorization of the symmetric Broyden matrix.
    \item Updating the symmetric Broyden matrix.
    \item Decompositions of the Hessian matrices of the quadratic models.
    \item \gls{mnh}~\cite{Wild_2008}
    \item The new method we introduce in \cref{ch:cobyqa-introduction} also uses quadratic models obtained by underdetermined interpolation.
\end{itemize}

\subsection{Quadratic models based on symmetric Broyden updates}
\label{subsec:symmetric-broyden-updates}

\begin{itemize}
    \item Explain the name.
    \item \gls{newuoa}~\cite{Powell_2006}
\end{itemize}

\section{An optimal number of interpolation points}

We study in this section an interpolation set that we will use in \cref{ch:cobyqa-introduction} of this thesis, where we introduce a new model-based \gls{dfo} method.
This interpolation set is adapted from~\cite{Powell_2001} as follows.
Let~$\delta > 0$ be fixed and for~$j \in \set{1, 2, \dots, 2n + 1}$, let~$z^j \in \R^n$ be
\begin{subequations}
    \label{eq:initial-interpolation-set}
    \begin{empheq}[left={z^j \eqdef \empheqlbrace}]{alignat=2}
        & 0,                        && \quad \text{if~$j = 1$,}\\
        & \delta e_{j - 1},         && \quad \text{if~$2 \le j \le n + 1$,}\\
        & -\delta e_{j - n - 1},    && \quad \text{otherwise,}
    \end{empheq}
\end{subequations}
where~$e_j \in \R^n$ denotes the~$i$th standard coordinate vector.
We then define the interpolation set~$\mathcal{Z}_m \subseteq \R^n$ for each~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ by
\begin{equation*}
    \mathcal{Z}_m \eqdef \set{z^1, z^2, \dots, z^m}.
\end{equation*}
Further, we denote by~$\ball[p][m]$ the smallest~$\ell_p$-norm ball containing~$\mathcal{Z}_m$, for~$p \in [1, \infty]$.
Note that we allow~$p = \infty$.
By the construction of~$\mathcal{Z}_m$, we observe that
\begin{equation}
    \label{eq:ball-initial}
    \ball[p][m] \equiv \ball[p](\delta) \eqdef \set{x \in \R^n : \norm{x}_p \le \delta}.
\end{equation}

Throughout this section, we denote by~$\lagp[i]$ (instead of~$\lagp[z^i]$) the minimum-norm Lagrange polynomial associated with~$\mathcal{Z}_m$ for~$z^i \in \mathcal{Z}_m$.
According to the~\cref{def:lambda-poisedness-minimum-norm} and the equation~\cref{eq:ball-initial}, the set~$\mathcal{Z}_m$ is~$\Lambda_p$-poised in~$\ball[p][m]$ in the minimum-norm sense with
\begin{equation*}
    \Lambda_p \eqdef \max_{1 \le i \le m} \max_{x \in \ball[p](\delta)} \abs{\lagp[i](x)}.
\end{equation*}
In the following, we develop bounds for~$\Lambda_p$ and we establish its value in some special cases\todo{Improve the sentence}.

\Cref{lem:lagrange-polynomials-initial} provides explicit formulae for~$\lagp[i]$ for all~$i \in \set{1, 2, \dots, m}$.
These formulae are given in~\cite[\S~3]{Powell_2006}, without a proof.

\begin{lemma}
    \label{lem:lagrange-polynomials-initial}
    For each~$m \in \set{n + 2, n + 3, \dots,  2n + 1}$ and all~$x \in \R^n$, we have
    \begin{empheq}[left={\lagp[i](x) = \empheqlbrace}]{alignat*=2}
        & 1 - \frac{1}{\delta^2} \sum_{j = 1}^{m - n - 1} x_j^2 - \frac{1}{\delta} \sum_{j = m - n}^n x_j,  && \quad \text{if~$i = 1$,}\\
        & \frac{x_{i - 1}^2}{2 \delta^2} + \frac{x_{i - 1}}{2 \delta},                                      && \quad \text{if~$2 \le i \le m - n$,}\\
        & \frac{x_{i - 1}}{\delta},                                                                         && \quad \text{if~$m - n + 1 \le i \le n + 1$,}\\
        & \frac{x_{i - 1}^2}{2 \delta^2} - \frac{x_{i - 1}}{2 \delta}.                                      && \quad \text{otherwise.}
    \end{empheq}
    In the formulation of~$\lagp[1]$, if~$m = 2n + 1$, we define~$\sum_{j = m - n}^n x_j = 0$.
\end{lemma}

\begin{proof}
    Let~$i \in \set{1, 2, \dots, m}$ be fixed and let~$\lagp$ be a quadratic polynomial that satisfies
    \begin{subequations}
        \label{eq:lagrange-polynomials-initial-proof}
        \begin{empheq}[left={\lagp(z^j) = \empheqlbrace}]{alignat=2}
            & 1,    && \quad \text{if~$j = i$,}\\
            & 0,    && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
    First, it is straightforward to verify that~$\lagp[i]$ satisfies the interpolation conditions~\cref{eq:lagrange-polynomials-initial-proof}.
    Hence, it suffices to show that the Frobenius norm of its Hessian matrix is least.
    According to the equation~\cref{eq:initial-interpolation-set}, for any~$j \in \set{1, 2, \dots, m - n - 1}$, we have~$z^1 = 0$,~$z^{j + 1} = \delta e_j$, and~$z^{n + j + 1} = - \delta e_j$.
    Therefore,
    \begin{empheq}[left=\empheqlbrace]{alignat*=1}
        & \lagp(z^{j + 1}) = \lagp(z^1) + \delta \inner{\nabla \lagp(z^1), e_j} + \frac{\delta^2}{2} \inner{e_j, (\nabla^2 \lagp) e_j},\\
        & \lagp(z^{n + j + 1}) = \lagp(z^1) - \delta \inner{\nabla \lagp(z^1), e_j} + \frac{\delta^2}{2} \inner{e_j, (\nabla^2 \lagp) e_j},
    \end{empheq}
    and hence,
    \begin{equation*}
        \inner{e_j, (\nabla^2 \lagp) e_j} = \frac{\lagp(z^{j + 1}) + \lagp(z^{n + j + 1}) - 2 \lagp(z^1)}{\delta^2}.
    \end{equation*}
    This fixes the first~$m - n - 1$ diagonal entries of~$\nabla^2 \lagp$, which are exactly those of~$\nabla^2 \lagp[i]$.
    Since all the other entries of~$\nabla^2 \lagp[i]$ are zero, we have
    \begin{equation*}
        \norm{\nabla^2 \lagp[i]}_{\mathsf{F}}^2 \le \norm{\nabla^2 \lagp}_{\mathsf{F}}^2,
    \end{equation*}
    which completes the proof.
\end{proof}

\begin{lemma}
    \label{lem:lambda-poisedness-initial-simple}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation}
        \label{eq:lambda-poisedness-initial-simple}
        \Lambda_p = \max_{x \in \ball[p](\delta)} \abs{\lagp[1](x)}.
    \end{equation}
\end{lemma}

\begin{proof}
    For each~$i \in \set{2, 3, \dots, n + 1}$, according to~\cref{lem:lagrange-polynomials-initial},~$\lagp[i](x)$ depends only on~$x_{i - 1}$ for all~$x \in \R^n$, and hence
    \begin{equation*}
        \max_{x \in \ball[p](\delta)} \abs{\lagp[i](x)} = \max_{t \in [-\delta, \delta]} \abs{\lagp[i](t e_{i - 1})} = 1.
    \end{equation*}
    Similarly, for each~$i \in \set{n + 2, n + 3, \dots, m}$, since~$\lagp[i](x)$ depends only on~$x_{i - n - 1}$ for all~$x \in \R^n$, we have
    \begin{equation*}
        \max_{x \in \ball[p](\delta)} \abs{\lagp[i](x)} = \max_{t \in [-\delta, \delta]} \abs{\lagp[i](t e_{i - n - 1})} = 1.
    \end{equation*}
    Noting that~$\lagp[1](z^1) = 1$ and~$z^1 \in \ball[p](\delta)$, we thus have
    \begin{equation*}
        \Lambda_p = \max_{x \in \ball[p](\delta)} \abs{\lagp[1](x)}.
    \end{equation*}
\end{proof}

\begin{theorem}
    \label{thm:lambda-poisedness-initial}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation*}
        1 + (2n + 1 - m)^{\frac{p - 1}{p}} \le \Lambda_p \le n,
    \end{equation*}
    where we assume that~$0^0 = 0$ and that the lower bound is~$2n + 2 - m$ for~$p = \infty$.
\end{theorem}

\begin{proof}
    We will establish the bounds using the formulation of~$\Lambda_p$ in~\cref{lem:lambda-poisedness-initial-simple}.
    For the lower bound, by considering only the points in~$\R^n$ whose leading~$m - n - 1$ components are zeros and whose remaining~$2n + 1 - m$ components are equal, we have
    \begin{equation*}
        \Lambda_p \ge \max_{t \in \R} \set{1 - \delta^{-1} (2n + 1 - m) t : (2n + 1 - m) \abs{t}^p \le \delta^p} = 1 + (2n + 1 - m)^{\frac{p - 1}{p}}.
    \end{equation*}
    
    We now prove the upper bound.
    Note that for any~$p \ge 1$, we have~$\ball[p](\delta) \subseteq \ball[\infty](\delta)$, so that~$\Lambda_p \le \Lambda_{\infty}$.
    Therefore, we only need to show that~$\Lambda_{\infty} \le n$.
    Considering both~$\lagp[1]$ and~$-\lagp[1]$, we obtain
    \begin{equation*}
        \Lambda_{\infty} = \max_{x \in \ball[\infty](\delta)} \abs{\lagp[1](x)} = \max \set{2n + 2 - m, n - 1} \le n.
    \end{equation*}
\end{proof}

\begin{proposition}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{subequations}
        \label{eq:lambda-poisedness-1}
        \begin{empheq}[left={\Lambda_1 = \empheqlbrace}]{alignat=2}
            & 2,    && \quad \text{if~$n + 2 \le m \le 2n$,}\\
            & 1,    && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
\end{proposition}

\begin{proof}
    According to~\cref{thm:lambda-poisedness-initial},~$\Lambda_1$ is lower bounded by the right-hand side of~\cref{eq:lambda-poisedness-1}.
    Therefore, we only need to prove that this right-hand side is also a lower bound for~$\Lambda_1$, using the formulation in \cref{lem:lambda-poisedness-initial-simple}.

    For any~$x \in \ball[1](\delta)$, we have
    \begin{equation*}
        \lagp[1](x) \le 1 - \frac{1}{\delta} \sum_{j = m - n}^n x_j \le 1 + \frac{1}{\delta} \sum_{j = m - n}^n \abs{x_j}.
    \end{equation*}
    Therefore,
    \begin{subequations}
        \label{eq:eq:lambda-poisedness-1-proof-1}
        \begin{empheq}[left={\lagp[1](x) \le \empheqlbrace}]{alignat=2}
            & 2,    && \quad \text{if~$n + 2 \le m \le 2n$,}\\
            & 1,    && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
    On the other hand,
    \begin{subequations}
        \label{eq:eq:lambda-poisedness-1-proof-2}
        \begin{align}
            \lagp[1](x) &= 1 - \sum_{j = 1}^{m - n - 1} \frac{x_j^2}{\delta^2} - \sum_{j = m - n}^n \frac{x_j}{\delta}\\
                        &\ge 1 - \sum_{j = 1}^{m - n - 1} \frac{\abs{x_j}}{\delta} - \sum_{j = m - n}^n \frac{\abs{x_j}}{\delta}\\
                        &\ge 1 - \frac{\norm{x}_1}{\delta} \ge 0.
        \end{align}
    \end{subequations}
    We conclude the proof by combining~\cref{eq:eq:lambda-poisedness-1-proof-1,eq:eq:lambda-poisedness-1-proof-2} with~\cref{lem:lambda-poisedness-initial-simple}.
\end{proof}

\begin{proposition}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{equation}
        \label{eq:lambda-poisedness-2}
        \Lambda_2 = 1 + \sqrt{2n + 1 - m}.
    \end{equation}
\end{proposition}

\begin{proof}
    If~$m = 2n + 1$, \cref{lem:lagrange-polynomials-initial} tells us that~$\lagp[1](x) = 1 - \delta^{-2} \norm{x}_2^2$ for~$x \in \ball[2](\delta)$.
    Therefore, \cref{lem:lambda-poisedness-initial-simple} directly provides~$\Lambda_2 = 1$.
    We now focus on the case with~$n + 2 \le m < 2n + 1$.

    According to~\cref{thm:lambda-poisedness-initial},~$\Lambda_2$ is lower bounded by the right-hand side of~\cref{eq:lambda-poisedness-2}.
    Therefore, we only need to prove that this right-hand side is also a lower bound for~$\Lambda_2$, using the formulation in \cref{lem:lambda-poisedness-initial-simple}.
    
    For any~$x \in \ball[2](\delta)$, we have
    \begin{subequations}
        \begin{align}
            \lagp[1](x) &= 1 - \frac{1}{\delta^2} \sum_{j = 1}^{m - n - 1} x_j^2 - \frac{1}{\delta} \sum_{j = m - n}^n x_j\\
                        &\ge 1 - \frac{1}{\delta^2} \bigg( \delta^2 - \sum_{j = m - n}^n x_j^2 \bigg) - \frac{1}{\delta} \sum_{j = m - n}^n x_j = \sum_{j = m - n}^n \frac{x_j}{\delta} \bigg( 1 - \frac{x_j}{\delta} \bigg)\\
                        &\ge \min_{y \in \ball[2](1)} \sum_{j = m - n}^n y_j (1 - y_j). \label{eq:lambda-poisedness-initial-2-proof}
        \end{align}
    \end{subequations}
    Let~$y^{\ast} \in \ball[2](1)$ be a minimizer in~\cref{eq:lambda-poisedness-initial-2-proof}.
    \Cref{thm:first-order-necessary-conditions} ensures that there exists a Lagrange multiplier~$\lambda^{\ast} \ge 0$ such that~$1 - 2 y_j^{\ast} + 2 \lambda^{\ast} y_j^{\ast} = 0$ for all~$j \in \set{m - n, \dots, n}$.
    Therefore, the last~$(2n + 1 - m)$ components of~$y^{\ast}$ are equal, and hence,
    \begin{align*}
        \lagp[1](x) \ge \min_{y \in \ball[2](1)} \sum_{j = m - n}^n y_j (1 - y_j)   &= \min_{t \in \R} \set{(2n + 1 - m) t (1 - t) : (2n + 1 - m) t^2 \le 1}\\
                                                                                    &= -1 - \sqrt{2n + 1 - m}.
    \end{align*}

    Furthermore,
    \begin{equation*}
        \lagp[1](x) \le 1 + \sum_{j = m - n}^n \frac{\abs{x_j}}{\delta} \le 1 + \sqrt{2n + 1 - m} \sum_{j = m - n}^n \frac{x_j^2}{\delta^2} \le 1 + \sqrt{2n + 1 - m}.
    \end{equation*}
    Therefore,~$\abs{\lagp[1](x)} \le 1 + \sqrt{2n + 1 - m}$ and hence, according to~\cref{lem:lambda-poisedness-initial-simple,thm:lambda-poisedness-initial}, we have
    \begin{equation*}
        \Lambda_2 = \max_{x \in \ball[2](\delta)} \abs{\lagp[1](x)} = 1 + \sqrt{2n + 1 - m},
    \end{equation*}
    which concludes the proof.
\end{proof} 

\begin{lemma}
    \label{lem:max-norm-pq}
    For any~$p \ge 1$ and~$q \ge 1$, we have
    \begin{subequations}
        \label{eq:max-norm-pq}
        \begin{empheq}[left={\max\limits_{x \in \ball[q](1)} \norm{x}_p = \empheqlbrace}]{alignat=2}
            & 1,                    && \quad \text{if~$p \ge q$,}\\
            & n^{\frac{q - p}{pq}}, && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
\end{lemma}

\begin{proof}
    Let us first consider the case where~$p \ge q$.
    For~$x \in \ball[q](1)$, we have~$\norm{x}_p \le 1$, and this bound is attained at the first coordinate vector~$e_1 \in \ball[q](1)$, so that
    \begin{equation*}
        \max_{x \in \ball[q](1)} \norm{x}_p = 1.
    \end{equation*}

    We now consider the case where~$p < q$.
    Let~$x^{\ast} \in \ball[q](1)$ be a minimizer in~\cref{eq:max-norm-pq}.
    \Cref{thm:first-order-necessary-conditions} ensures that there exists a Lagrange multiplier~$\lambda^{\ast} \in \R$ such that
    \begin{empheq}[left=\empheqlbrace]{alignat*=2}
        & \frac{\sgn(x_j^{\ast}) \abs{x_j^{\ast}}^{p - 1}}{\norm{x^{\ast}}_p^{p - 1}} + \frac{\lambda^{\ast} \sgn(x_j^{\ast}) \abs{x_j^{\ast}}^{q - 1}}{\norm{x^{\ast}}_q^{q - 1}} = 0, && \quad \text{for~$j \in \set{1, 2, \dots, n}$,}\\
        & \norm{x^{\ast}}_q \le 1,                                                                                                                                                      && \\
        & \lambda^{\ast} (\norm{x^{\ast}}_q - 1) = 0,                                                                                                                                   && \\
        & \lambda^{\ast} \le 0,                                                                                                                                                         &&
    \end{empheq}
    where~$\sgn$ denotes the sign function.
    Therefore, all the nonzero components of~$x^{\ast}$ share the same absolute value~$\alpha > 0$, i.e., for any~$j \in \set{1, 2, \dots, n}$, we have~$\abs{x_j^{\ast}} \in \set{0, \alpha}$.
    Moreover, it is clear that~$x^{\ast} \neq 0$, as such a point is a global minimizer of the objective function in~\cref{eq:max-norm-pq}, so that~$\norm{x^{\ast}}_q = 1$.
    If~$k$ denotes the number of nonzero components of~$x^{\ast}$, we thus have
    \begin{equation*}
        \norm{x^{\ast}}_p = k^{\frac{q - p}{pq}},
    \end{equation*}
    which is maximal for~$k = n$.
\end{proof}

\begin{proposition}
    For any~$p \ge 1$, if~$m = 2n + 1$, then
    \begin{equation*}
        \Lambda_p = \max \set[\big]{1, n^{\frac{p - 2}{p}} - 1}.
    \end{equation*}
\end{proposition}

\begin{proof}
    It is clear that
    \begin{equation*}
        \max_{x \in \ball[p](\delta)} \lagp[1](x) = \max_{x \in \ball[p](\delta)} 1 - \frac{\norm{x}_2^2}{\delta^2} = 1.
    \end{equation*}
    Moreover, according to \cref{lem:max-norm-pq}, we have
    \begin{empheq}[left={\max\limits_{x \in \ball[p](\delta)} -\lagp[1](x) = \max\limits_{x \in \ball[p](\delta)} \dfrac{\norm{x}_2^2}{\delta^2} - 1 = \empheqlbrace}]{alignat*=2}
        & 0,                        && \quad \text{if~$p \le 2$,}\\
        & n^{\frac{p - 2}{p}} - 1,  && \quad \text{otherwise.}
    \end{empheq}
\end{proof}

\todo[noline]{Explain how these results could be generalized}
