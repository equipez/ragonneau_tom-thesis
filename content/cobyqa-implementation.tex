%% contents/cobyqa-implementation.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{\glsfmttext{cobyqa} \textemdash\ description of the Python implementation and experiments}
\label{ch:cobyqa-implementation}

In this chapter, we provide details on the publicly available Python implementation of \gls{cobyqa}, which is open-source and easily readable.
This implementation is a very technical work.
In particular, we present in detail the management of the user's inputs, describe every stopping criterion, and exhibit a mechanism for returning the best iterate in \cref{sec:implementation-details}.
\Cref{sec:python-implementation} introduces the Python implementation, provides some examples of use, and present how the \gls{cobyqa} package is distributed.
Finally, we provide in \cref{sec:cobyqa-experiments} some numerical experiments, comparing in different scenarios \gls{cobyqa} with \gls{newuoa}, \gls{bobyqa}, \gls{lincoa}, and \gls{cobyla}.
These numerical experiments confirm that the method is working on the considered problems, although the implementation necessitates being fine-tuned in the future.

% We decided to implement the first version of \gls{cobyqa} in Python for its simplicity and its large amount of users.
% However, Python has the computational flows of an interpreted language, making it slow compared to compiled languages.
% Therefore, we implement the subproblem solvers of \gls{cobyqa} in Cython, a programming language that blends the advantages of both Python and C.
% Cython is a compiled language and is, therefore, much faster than Python alone.

\section{Additional implementation details}
\label{sec:implementation-details}

\subsection{Preprocessing of the arguments}

When presenting the framework of \gls{cobyqa} in \cref{ch:cobyqa-introduction}, we assumed that
\begin{enumerate}
    \item the bound constraints satisfy~$\xl < \xu$,
    \item the initial guess is feasible with respect to the bound constraints, and
    \item the initial trust-region radius satisfy
    \begin{equation}
        \label{eq:initial-trust-region-radius-condition}
        \rad[0] \le \frac{1}{2} \min_{1 \le i \le n} @@ (\xu_i - \xl_i).
    \end{equation}
\end{enumerate}
In the implementation of \gls{cobyqa}, we try to satisfy these conditions as long as possible.

First of all, if~$\xl_i > \xu_i$ for some~$i \in \set{1, 2, \dots, n}$, then no computation is attempted since the problem is infeasible.
However, all the constraints that satisfy~$\xl_i = \xu_i$ are excluded from the computations.
Moreover, if \cref{eq:initial-trust-region-radius-condition} does not hold, then the value of~$\rad[0]$ is reduced.

Now that~$\xl < \xu$, we must ensure that the initial guess is feasible.
If it is not, it must be projected onto the bounds constraints.
Note that this projection is already described in \cref{subsec:interpolation-based-quadratic-models}, because the initial is modified so that each component is either on a bound or keeps a distance of at least~$\rad[0]$ from the corresponding bounds.

Recall that the package \gls{pdfo} removes the linear equality constraints by defining a subspace of~$\R^n$ of lower dimension, and ensures that each iterate lies in this subspace.
This is not done in \gls{cobyqa}, because it handles directly equality constraints.
Moreover, the author plans to include \gls{cobyqa} into \gls{pdfo} in the future.
Hence, this feature will become available.

\subsection{Additional stopping criteria}

The only stopping criterion we presented so far is when the lower-bound~$\radlb[k]$ on the trust-region radius reaches a threshold value~$\radlb[\infty]$.
However, \gls{cobyqa} also stops the computations if
\begin{enumerate}
    \item the number of function evaluations reaches a threshold value,
    \item the number of iteration reaches a threshold value,
    \item a target value on the objective function is reached by a feasible iterate,
    \item an absolute tolerance on the objective values of two successive iterates is achieved,
    \item a relative tolerance on the objective values of two successive iterates is achieved,
    \item an absolute tolerance on two successive iterates in~$\ell_2$-norm is achieved, or
    \item a relative tolerance on two successive iterates in~$\ell_2$-norm is achieved.
\end{enumerate}
The only remaining stopping criterion is when the computations must stop due to computer rounding error.
In practice, this happens if the denominator of the updating formula discussed in \cref{ch:cobyqa-introduction} is zero.

\begin{table}[ht]
    \caption{Exit statuses of \gls{cobyqa}}
    \label{tab:exit-statuses}
    \centering
    \begin{tabularx}{\textwidth}{cX}
        \toprule
        $0$     & The lower bound for the trust-region radius has been reached.\\
        \midrule
        $1$     & A feasible iterate reached the target value on the objective values.\\
        \midrule
        $2$     & The desired absolute tolerance on the objective values of two successive iterates is reached.\\
        \midrule
        $3$     & The desired relative tolerance on the objective values of two successive iterates is reached.\\
        \midrule
        $4$     & The desired absolute tolerance on two successive iterates in~$\ell_2$-norm is reached.\\
        \midrule
        $5$     & The desired relative tolerance on two successive iterates in~$\ell_2$-norm is reached.\\
        \midrule
        $6$     & The maximum number of function evaluations has been exceeded.\\
        \midrule
        $7$     & The maximum number of iterations has been exceeded.\\
        \midrule
        $8$     & The denominator of the updating formula is zero.\\
        \midrule
        $9$     & All variables are fixed by the constraints.\\
        \midrule
        $-1$    & The bound constraints are infeasible.\\
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Retention of the history}

It is important to remark that~$\iter[k]$ may not be the best point ever encountered by \gls{cobyqa}.
Indeed, it is possible that a point close from feasibility (or even feasible) has been discarded because it had a large objective function value and only low penalty parameters were encountered.
Therefore, \gls{cobyqa} can store the history of all the points visited, together with the corresponding objective and constraint values.
Since this mechanism is expensive in terms of memory, it is executed only if the user desires it.

Amoung all the iterates, the best one is finally selected as follows.
First of all, only the points whose constraint violation is at most twice as large as the least one are considered.
Hence, if a feasible point is encountered, a feasible point will be returned by \gls{cobyqa}.
Among all these points, \gls{cobyqa} selects the one with the least merit value, using the last penalty parameter.
If several minimizers exist, we select the one with the least constraint violation and if several minimizers still exist, we select one with the least objective value.

Since this implementation is very expensive in terms of memory, the following technique will be implemented in the future.
The user will provide a maximum number~$N$ of iterate to retain, and only the last~$N$ iterates will be kept in memory, together with the interpolation set.
Finally, the procedure described in the previous paragraph will be applied.

\section{Description of the Python implementation}
\label{sec:python-implementation}

In this section, we provide details on the Python implementation of \gls{cobyqa}, its testing, and its distribution.

\subsection{Choice of programming languages}

We chose to develop \gls{cobyqa} in Python because it is a simple open-source language, perfectly adapted to experimentation.
Although the current version of \gls{cobyqa} is mostly written in Python, the author plans to develop a Fortran version in the future.
This would have the ability to be interfaced with several other languages, such a Python, MATLAB, or Julia.
However, developing software in Fortran is time-consuming, and hence, this work will be carried out in the future.

The initial version of \gls{cobyqa} was entirely written in pure Python.
However, its execution time was prohibitively long on problems of dimensions~$n = 50$ and above.
Since most of the work was done in the subproblem solvers, the author decided to implement them in Cython~\cite{Behnel_Etal_2011}.
It is a compiled language that aims at improving the performance of Python code.
All the functions implemented in the module \texttt{cobyqa.linalg} are implemented in Cython.

Other motivativations for the author to develop the initial version of \gls{cobyqa} in Python are the extensive librairies that are available.
\Gls{cobyqa} does not only rely on Cython, but also on NumPy~\cite{Harris_Etal_2020} and Scipy~\cite{Virtanen_Etal_2020}.
These packages provides array structures, together with basic mathematical operations on them, and basic scientific computing tools.
In particular, the subproblem solvers call BLAS~\cite{Blackford_Etal_2002} for making matrices and vectors operations, and the implementation of \gls{nnls} relies on the DGELSY subroutine of LAPACK~\cite{Anderson_Etal_1999} to solve its unconstrained linear least-squares subproblem.

\subsection{Examples of usage}

The package \gls{cobyqa} provides a function \texttt{minimize}, which takes as parameters
\begin{enumerate}
    \item the objective function to be minimized,
    \item the initial guess,
    \item bound constraints (optional),
    \item linear constriants (optional),
    \item nonlinear constraints (optional), and
    \item a dictionary of options (each being optional).
\end{enumerate}

We provide an list of the available options in \cref{tab:cobyqa-options}.

\begin{table}[ht]
    \caption{Options of \gls{cobyqa}}
    \label{tab:cobyqa-options}
    \centering
    \begin{tabularx}{\textwidth}{cX}
        \toprule
        \texttt{rhobeg}     & Initial trust-region radius.\\
        \midrule
        \texttt{rhoend}     & Final trust-region radius.\\
        \midrule
        \texttt{npt}        & Number of interpolation points.\\
        \midrule
        \texttt{maxfev}     & Maximum number of objective and constraint function evaluations.\\
        \midrule
        \texttt{maxiter}    & Maximum number of iterations.\\
        \midrule
        \texttt{target}     & Target value on the objective function.\\
        \midrule
        \texttt{ftol\_abs}  & Absolute tolerance on the objective function.\\
        \midrule
        \texttt{ftol\_rel}  & Relative tolerance on the objective function.\\
        \midrule
        \texttt{xtol\_abs}  & Absolute tolerance on the decision variables.\\
        \midrule
        \texttt{xtol\_rel}  & Relative tolerance on the decision variables.\\
        \midrule
        \texttt{disp}       & Whether to print pieces of information on the execution of the solver.\\
        \midrule
        \texttt{debug}      & Whether to make debugging tests during the execution.\\
        \bottomrule
    \end{tabularx}
\end{table}

\subsubsection{Examples of usage}

For example, consider the unconstrained minimization of the chained Rosenbrock function
\begin{equation*}
    \obj(x) = \sum_{i = 1}^{n - 1} 100(x_{i + 1} - x_i^2)^2 + (1 - x_i)^2, \quad \text{for~$x \in \R^n$.}
\end{equation*}
As those afterwards, it is clearly unreasonable to employ a \gls{dfo} method to solve such a problem and it only serves as an example.
\Cref{lst:cobyqa-rosenbrock} shows how to solve such a problem with~$n = 5$ using \gls{cobyqa}.

\begin{lstpython}[%
    caption=Solving the Rosenbrock problem using \gls{cobyqa},
    label=lst:cobyqa-rosenbrock,
]
    >>> from scipy.optimize import rosen
    >>> from cobyqa import minimize
    >>>
    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]  # or anything else
    >>> res = minimize(rosen, x0)
    >>> res.x
    array([1., 1., 1., 1., 1.]) 
\end{lstpython}

To see how to supply to \gls{cobyqa} bound and linear constraints, consider the Example 16.4 of~\cite{Nocedal_Wright_2006}, defined as
\begin{align*}
    \min        & \quad (\iter_1 - 1)^2 + (\iter_2 - 2.5)^2\\
    \text{s.t.} & \quad -\iter_1 + 2 \iter_2 \le 2,\\
                & \quad \iter_1 + 2 \iter_2 \le 6,\\
                & \quad \iter_1 - 2 \iter_2 \le 2,\\
                & \quad \iter_1 \ge 0,\\
                & \quad \iter_2 \ge 0,\\
                & \quad \iter \in \R^2.
\end{align*}
\Cref{lst:cobyqa-bound-linear} shows how to solve this problem using \gls{cobyqa}.

\begin{lstpython}[%
    caption=An example of \gls{cobyqa} with linear constraints,
    label=lst:cobyqa-bound-linear,
]
    >>> from cobyqa import minimize
    >>>
    >>> def obj(x):
    ...     return (x[0] - 1.0) ** 2.0 + (x[1] - 2.5) ** 2.0
    >>>
    >>> x0 = [2.0, 0.0]
    >>> xl = [0.0, 0.0]
    >>> Aub = [[-1.0, 2.0], [1.0, 2.0], [1.0, -2.0]]
    >>> bub = [2.0, 6.0, 2.0]
    >>> res = minimize(quadratic, x0, xl=xl, Aub=Aub, bub=bub)
    >>> res.x
    array([1.4, 1.7])
\end{lstpython}

Thus, to see how to supply nonlinear constraints to \gls{cobyqa}, consider the Problem G of~\cite{Powell_1994}, defined as
\begin{align*}
    \min        & \quad \iter_3\\
    \text{s.t.} & \quad -5 \iter_1 + \iter_2 - \iter_3 \le 0,\\
                & \quad 5 \iter_1 + \iter_2 - \iter_3 \le 0,\\
                & \quad \iter_1^2 + \iter_2^2 + 4 \iter_2 - \iter_3 \le 0,\\
                & \quad \iter \in \R^3.
\end{align*}
\Cref{lst:cobyqa-problem-g} shows how to solve such a problem using \gls{cobyqa}, starting from the initial guess~$\iter[0] = [1, 1, 1]^{\T}$.

\begin{lstpython}[%
    caption=An example of \gls{cobyqa} with nonlinear constraints,
    label=lst:cobyqa-problem-g,
]
    >>> from cobyqa import minimize
    >>>
    >>> def obj(x):
    ...     return x[2]
    >>>
    >>> def cub(x):
    ...     return x[0] ** 2.0 + x[1] ** 2.0 + 4.0 * x[1] - x[2] 
    >>>
    >>> x0 = [1.0, 1.0, 1.0]
    >>> Aub = [[-5.0, 1.0, -1.0], [5.0, 1.0, -1.0]]
    >>> bub = [0.0, 0.0]
    >>> res = minimize(obj, x0, Aub=Aub, bub=bub, cub=cub)
    >>> res.x
    array([ 0., -3., -3.])   
\end{lstpython}

\subsubsection{Details on the returned structure}

The function \texttt{minimize} returns a structure (that we named \texttt{res} in the examples above) whose attributes are details in \cref{tab:optimize-result}.

\begin{table}[ht]
    \caption{Structure for the result of the optimization algorithm}
    \label{tab:optimize-result}
    \centering
    \begin{tabularx}{\textwidth}{cX}
        \toprule
        \texttt{x}          & Solution point provided by the optimization solver.\\
        \midrule
        \texttt{success}    & Flag indicating whether the optimization solver terminated successfully.\\
        \midrule
        \texttt{status}     & Termination status of the optimization solver.\\
        \midrule
        \texttt{message}    & Description of the termination status of the optimization solver.\\
        \midrule
        \texttt{fun}        & Value of the objective function at the solution point provided by the optimization solver.\\
        \midrule
        \texttt{jac}        & Approximation of the gradient of the objective function at the solution point provided by the optimization solver. If the value of a component is unknown, it is replaced by NaN.\\
        \midrule
        \texttt{nfev}       & Number of objective and constraint function evaluations.\\
        \midrule
        \texttt{nit}        & Number of iterations performed by the optimization solver.\\
        \midrule
        \texttt{maxcv}      & Maximum constraint violation at the solution point provided by the optimization solver. It is set only if the problem is not declared unconstrained by the optimization solver.\\
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Distribution of the package}

The subproblem solvers of \gls{cobyqa} are written in Cython, which necessitate a C/C++ compiler to work.
However, we made wheel distributions available for
\begin{enumerate}
    \item Windows, for both 32-bit and 64-bit operating systems,
    \item macOS, for both x64 and ARM64 architectures, and
    \item Linux, for both x64 and ARM64 architectures.
\end{enumerate}
The current version of \gls{cobyqa} is available for Python 3.7, 3.8, 3.9, and 3.10 (i.e., all the Python version available as of today).
The compilation is made using GitHub Actions, and relies on the Python package cibuildwheel\footnote{See \url{https://cibuildwheel.readthedocs.io/}.}.

\section{Some numerical experiments}
\label{sec:cobyqa-experiments}

To conclude this chapter, we present some numerical results of \gls{cobyqa}.
The performance profiles presented in this section have been obtained on the CUTEst library~\cite{Gould_Orban_Toint_2015}.
See \cref{sec:benchmarking-tools} for the definition of the performance profiles in a \gls{dfo} context.

\subsection{Comparison on unconstrained problems}

We first compare the performance of \gls{cobyqa} with \gls{newuoa} on unconstrained problems.
\Cref{fig:cobyqa-newuoa-unconstrained} plots the performance profiles obtained during this experiment.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA"}}{plain-1-10-perf-cobyqa-newuoa-u.csv}{3}
        \caption{Dimension~$n \le 10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA"}}{plain-1-50-perf-cobyqa-newuoa-u.csv}{3}
        \caption{Dimension~$n \le 50$}
    \end{subfigure}
    \caption{Comparison of \gls{cobyqa} with \gls{newuoa}, with~$\tau = 10^{-3}$}
    \label{fig:cobyqa-newuoa-unconstrained}
\end{figure}

We observe that both software have comparable performance.
\Gls{cobyqa} solves more problem on small dimensional problems when no limit is imposed on the number of function evaluations.
However, on problems of dimension at most~$50$, we observe that \gls{newuoa} achieved the least number of function evaluations on more problems than \gls{cobyqa}.
All in all, these results are expected, because both solvers use very similar algorithms.

\subsection{Comparison on bound-constrained problems}

We now compare the performance of \gls{cobyqa} with \gls{bobyqa} on bound-constrained problems.
\Cref{fig:cobyqa-bobyqa-bound-constrained} plots the performance profiles obtained during this experiment.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA"}}{plain-1-10-perf-bobyqa-cobyqa-b.csv}{3}
        \caption{Dimension~$n \le 10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA"}}{plain-1-50-perf-bobyqa-cobyqa-b.csv}{3}
        \caption{Dimension~$n \le 50$}
    \end{subfigure}
    \caption{Comparison of \gls{cobyqa} with \gls{bobyqa}, with~$\tau = 10^{-3}$}
    \label{fig:cobyqa-bobyqa-bound-constrained}
\end{figure}

In both experiments, \gls{cobyqa} solved more problems using the least number of function evaluations.
However, when the dimension of the problems is at most~$50$, \gls{bobyqa} solves more problems when an infinite budget of function evaluations is provided.

\subsection{Comparison on linearly-constrained problems}
\label{subsec:comparison-cobyqa-lincoa}

We now compare the performance of \gls{cobyqa} with \gls{lincoa} on linearly-constrained problems.
\Cref{fig:cobyqa-lincoa-linearly-constrained} plots the performance profiles obtained during this experiment.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA"}}{plain-1-10-perf-cobyqa-lincoa-nl.csv}{3}
        \caption{Dimension~$n \le 10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA"}}{plain-1-50-perf-cobyqa-lincoa-nl.csv}{3}
        \caption{Dimension~$n \le 50$}
    \end{subfigure}
    \caption{Comparison of \gls{cobyqa} with \gls{lincoa}, with~$\tau = 10^{-3}$}
    \label{fig:cobyqa-lincoa-linearly-constrained}
\end{figure}

On these plots, it is clear that the performance of \gls{cobyqa} is worse than that of \gls{lincoa}, although it finally solves the same amount (or more) of problems.
However, both algorithms use similar techniques.
After invetigation, the author observed that the implementation of \gls{cobyqa} (as well as those of Powell's solvers) is extremely sensible to the way it is implemented.
For example, changing the order of the summation in an inner product changed the performance of the algorithm.
In other words, we need in the future to investigate each section of the source code of \gls{cobyqa}, to try to find the best configuration.
This hard work has not been done yet, as it is an extremely high time-consuming work.
This will however be done in the future.

\subsection{Comparison on nonlinearly-constrained problems}

We finally compare the performance of \gls{cobyqa} with \gls{cobyla} on general nonlinearly-constrained problems.
\Cref{fig:cobyqa-cobyla-nonlinearly-constrained} plots the performance profiles obtained during this experiment.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-10-perf-cobyla-cobyqa-qo.csv}{3}
        \caption{Dimension~$n \le 10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo.csv}{3}
        \caption{Dimension~$n \le 50$}
    \end{subfigure}
    \caption{Comparison of \gls{cobyqa} with \gls{cobyla}, with~$\tau = 10^{-3}$}
    \label{fig:cobyqa-cobyla-nonlinearly-constrained}
\end{figure}

Once again, \gls{cobyla} showed slightly better performance than \gls{cobyqa} during these experiments.
The sensitivity of the implementation explained in \cref{fig:cobyqa-cobyla-nonlinearly-constrained} is likely to be the reason.
However, the performance are not drastically worse, which indicates that the method seem to work.
This is important to us, because one of the future work is to develop convergence theory for \gls{cobyqa}.
In the future, the author will work on the implementation of \gls{cobyqa} to try and make its performance better.

The results are nonetheless very encouraging, as shown on \cref{fig:cobyqa-cobyla-linearly-constrained}, which compares the performances of \gls{cobyqa} with \gls{cobyla} on linearly-constrained problems.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-10-perf-cobyla-cobyqa-nl.csv}{6}
        \caption{Dimension~$n \le 10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-nl.csv}{6}
        \caption{Dimension~$n \le 50$}
    \end{subfigure}
    \caption{Comparison of \gls{cobyqa} with \gls{cobyla}, with~$\tau = 10^{-6}$}
    \label{fig:cobyqa-cobyla-linearly-constrained}
\end{figure}

In a word, the numerical experiments show that the framework of \gls{cobyqa} seems to work.
Future work will include improving the performance that are below our expectations.

\section{Summary and remarks}

In this chapter, we presented the Python implementation of \gls{cobyqa}, and some numerical experiments.
In particular, we saw that the implementation avoids as much as possible to raise exceptions if the inputs do not satisfies the preconditions.
The stopping criteria employed by \gls{cobyqa} are usual for a \gls{dfo} method.
Normally, the computations of \gls{cobyqa} stop whenever the indicator of the resolution of the algorithm reached a final value.
The implementation of \gls{cobyqa} has been highly challenging, because the nature of \gls{dfo} necessitate highly dedicated pieces of code.

Moreover, we strove to make the user's interface as simple as possible, to make its use easy.
A complete user's guide for using \gls{cobyqa} is available on the website of the solver\footnote{Available at \url{https://cobyqa.readthedocs.io/}.}.
We implemented the subproblem solvers in Cython, a programming language that blends the advantages of both Python and C.
Cython is a compiled language and is, therefore, much faster than Python alone.
The compilation of \gls{cobyqa} requires several dependencies (depending on the operating system), but compiled distributions are provided to ease the installation.

Finally, the numerical results obtained on \gls{cobyqa} are very encouraging.
They show that the method is working on most CUTEst problems, although a theoretical analysis of its convergence analysis needs to be done in the future.
The performance of \gls{cobyqa} is comparable to those of \gls{newuoa} and \gls{bobyqa} on unconstrained and bound-constrained problems, respectively.
However, the performance of \gls{cobyqa} on linearly- and nonlinearly-constrained problems does not entirely keep up with our expectations.
This is likely due to the sensitivity of the implementation, which necessitates to be improved.
This will be corrected in the near future.
