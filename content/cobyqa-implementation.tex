%% contents/cobyqa-implementation.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{\glsfmttext{cobyqa} \textemdash\ description of the Python implementation and experiments}
\label{ch:cobyqa-implementation}

In this chapter, we provide details on the publicly available Python implementation of \gls{cobyqa}, which is open-source and easily readable.
This implementation is a very technical work.
In particular, we present in detail the management of the user's inputs, describe every stopping criterion, and exhibit a mechanism for returning the best iterate in \cref{sec:implementation-details}.
\Cref{sec:python-implementation} introduces the Python implementation, provides some examples of use, and present how the \gls{cobyqa} package is distributed.
Finally, we provide in \cref{sec:cobyqa-experiments} some numerical experiments, comparing in different scenarios \gls{cobyqa} with \gls{newuoa}, \gls{bobyqa}, \gls{lincoa}, and \gls{cobyla}.
These numerical experiments confirm that the method is working on the considered problems, although the implementation necessitates being fine-tuned in the future.

% We decided to implement the first version of \gls{cobyqa} in Python for its simplicity and its large amount of users.
% However, Python has the computational flows of an interpreted language, making it slow compared to compiled languages.
% Therefore, we implement the subproblem solvers of \gls{cobyqa} in Cython, a programming language that blends the advantages of both Python and C.
% Cython is a compiled language and is, therefore, much faster than Python alone.

\section{Additional implementation details}
\label{sec:implementation-details}

\subsection{Preprocessing of the arguments}

When presenting the framework of \gls{cobyqa} in \cref{ch:cobyqa-introduction}, we assumed that
\begin{enumerate}
    \item the bound constraints satisfy~$\xl < \xu$,
    \item the initial guess is feasible with respect to the bound constraints, and
    \item the initial trust-region radius satisfy
    \begin{equation}
        \label{eq:initial-trust-region-radius-condition}
        \rad[0] \le \frac{1}{2} \min_{1 \le i \le n} @@ (\xu_i - \xl_i).
    \end{equation}
\end{enumerate}
In the implementation of \gls{cobyqa}, we try to satisfy these conditions as long as possible.

First of all, if~$\xl_i > \xu_i$ for some~$i \in \set{1, 2, \dots, n}$, then no computation is attempted since the problem is infeasible.
However, all the constraints that satisfy~$\xl_i = \xu_i$ are excluded from the computations.
Moreover, if \cref{eq:initial-trust-region-radius-condition} does not hold, then the value of~$\rad[0]$ is reduced.

Now that~$\xl < \xu$, we must ensure that the initial guess is feasible.
If it is not, it must be projected onto the bounds constraints.
Note that this projection is already described in \cref{subsec:interpolation-based-quadratic-models}, because the initial is modified so that each component is either on a bound or keeps a distance of at least~$\rad[0]$ from the corresponding bounds.

Recall that the package \gls{pdfo} removes the linear equality constraints by defining a subspace of~$\R^n$ of lower dimension, and ensures that each iterate lies in this subspace.
This is not done in \gls{cobyqa}, because it handles directly equality constraints.
Moreover, the author plans to include \gls{cobyqa} into \gls{pdfo} in the future.
Hence, this feature will become available.

\subsection{Additional stopping criteria}

The only stopping criterion we presented so far is when the lower-bound~$\radlb[k]$ on the trust-region radius reaches a threshold value~$\radlb[\infty]$.
However, \gls{cobyqa} also stops the computations if
\begin{enumerate}
    \item the number of function evaluations reaches a threshold value,
    \item the number of iteration reaches a threshold value,
    \item a target value on the objective function is reached by a feasible iterate,
    \item an absolute tolerance on the objective values of two successive iterates is achieved,
    \item a relative tolerance on the objective values of two successive iterates is achieved,
    \item an absolute tolerance on two successive iterates in~$\ell_2$-norm is achieved, or
    \item a relative tolerance on two successive iterates in~$\ell_2$-norm is achieved.
\end{enumerate}
The only remaining stopping criterion is when the computations must stop due to computer rounding error.
In practice, this happens if the denominator of the updating formula discussed in \cref{ch:cobyqa-introduction} is zero.

\begin{table}[ht]
    \caption{Exit statuses of \gls{cobyqa}}
    \label{tab:exit-statuses}
    \centering
    \begin{tabularx}{\textwidth}{cX}
        \toprule
        $0$     & The lower bound for the trust-region radius has been reached.\\
        \midrule
        $1$     & A feasible iterate reached the target value on the objective values.\\
        \midrule
        $2$     & The desired absolute tolerance on the objective values of two successive iterates is reached.\\
        \midrule
        $3$     & The desired relative tolerance on the objective values of two successive iterates is reached.\\
        \midrule
        $4$     & The desired absolute tolerance on two successive iterates in~$\ell_2$-norm is reached.\\
        \midrule
        $5$     & The desired relative tolerance on two successive iterates in~$\ell_2$-norm is reached.\\
        \midrule
        $6$     & The maximum number of function evaluations has been exceeded.\\
        \midrule
        $7$     & The maximum number of iterations has been exceeded.\\
        \midrule
        $8$     & The denominator of the updating formula is zero.\\
        \midrule
        $9$     & All variables are fixed by the constraints.\\
        \midrule
        $-1$    & The bound constraints are infeasible.\\
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Retention of the history}

It is important to remark that~$\iter[k]$ may not be the best point ever encountered by \gls{cobyqa}.
Indeed, it is possible that a point close from feasibility (or even feasible) has been discarded because it had a large objective function value and only low penalty parameters were encountered.
Therefore, \gls{cobyqa} can store the history of all the points visited, together with the corresponding objective and constraint values.
Since this mechanism is expensive in terms of memory, it is executed only if the user desires it.

Amoung all the iterates, the best one is finally selected as follows.
First of all, only the points whose constraint violation is at most twice as large as the least one are considered.
Hence, if a feasible point is encountered, a feasible point will be returned by \gls{cobyqa}.
Among all these points, \gls{cobyqa} selects the one with the least merit value, using the last penalty parameter.
If several minimizers exist, we select the one with the least constraint violation and if several minimizers still exist, we select one with the least objective value.

Since this implementation is very expensive in terms of memory, the following technique will be implemented in the future.
The user will provide a maximum number~$N$ of iterate to retain, and only the last~$N$ iterates will be kept in memory, together with the interpolation set.
Finally, the procedure described in the previous paragraph will be applied.

\section{Description of the Python implementation}
\label{sec:python-implementation}

In this section, we provide details on the Python implementation of \gls{cobyqa}, its testing, and its distribution.\todo[color=OliveGreen!50]{Add project's links}
The source code of \gls{cobyqa} is available at the following url:
\begin{center}
    \url{https://github.com/ragonneau/cobyqa}
\end{center}
A complete user's guide for \gls{cobyqa} is available at the following url:
\begin{center}
    \url{https://www.cobyqa.com/}
\end{center}

\subsection{Choice of programming languages}

We chose to develop \gls{cobyqa} in Python because it is a simple open-source language, perfectly adapted to experimentation.
Although the current version of \gls{cobyqa} is mostly written in Python, the author plans to develop a Fortran version in the future.
This would have the ability to be interfaced with several other languages, such a Python, MATLAB, or Julia.
However, developing software in Fortran is time-consuming, and hence, this work will be carried out in the future.

The initial version of \gls{cobyqa} was entirely written in pure Python.
However, its execution time was prohibitively long on problems of dimensions~$n = 50$ and above.
Since most of the work was done in the subproblem solvers, the author decided to implement them in Cython~\cite{Behnel_Etal_2011}.
It is a compiled language that aims at improving the performance of Python code.
All the functions implemented in the module \texttt{cobyqa.linalg} are implemented in Cython.

Other motivativations for the author to develop the initial version of \gls{cobyqa} in Python are the extensive librairies that are available.
\Gls{cobyqa} does not only rely on Cython, but also on NumPy~\cite{Harris_Etal_2020} and Scipy~\cite{Virtanen_Etal_2020}.
These packages provides array structures, together with basic mathematical operations on them, and basic scientific computing tools.
In particular, the subproblem solvers call BLAS~\cite{Blackford_Etal_2002} for making matrices and vectors operations, and the implementation of \gls{nnls} relies on the DGELSY subroutine of LAPACK~\cite{Anderson_Etal_1999} to solve its unconstrained linear least-squares subproblem.

\subsection{Examples of usage}

The package \gls{cobyqa} provides a function \texttt{minimize}, which takes as parameters
\begin{enumerate}
    \item the objective function to be minimized,
    \item the initial guess,
    \item bound constraints (optional),
    \item linear constriants (optional),
    \item nonlinear constraints (optional), and
    \item a dictionary of options (each being optional).
\end{enumerate}

We provide an list of the available options in \cref{tab:cobyqa-options}.

\begin{table}[ht]
    \caption{Options of \gls{cobyqa}}
    \label{tab:cobyqa-options}
    \centering
    \begin{tabularx}{\textwidth}{cX}
        \toprule
        \texttt{rhobeg}     & Initial trust-region radius.\\
        \midrule
        \texttt{rhoend}     & Final trust-region radius.\\
        \midrule
        \texttt{npt}        & Number of interpolation points.\\
        \midrule
        \texttt{maxfev}     & Maximum number of objective and constraint function evaluations.\\
        \midrule
        \texttt{maxiter}    & Maximum number of iterations.\\
        \midrule
        \texttt{target}     & Target value on the objective function.\\
        \midrule
        \texttt{ftol\_abs}  & Absolute tolerance on the objective function.\\
        \midrule
        \texttt{ftol\_rel}  & Relative tolerance on the objective function.\\
        \midrule
        \texttt{xtol\_abs}  & Absolute tolerance on the decision variables.\\
        \midrule
        \texttt{xtol\_rel}  & Relative tolerance on the decision variables.\\
        \midrule
        \texttt{disp}       & Whether to print pieces of information on the execution of the solver.\\
        \midrule
        \texttt{debug}      & Whether to make debugging tests during the execution.\\
        \bottomrule
    \end{tabularx}
\end{table}

\subsubsection{Examples of usage}

For example, consider the unconstrained minimization of the chained Rosenbrock function
\begin{equation*}
    \obj(x) = \sum_{i = 1}^{n - 1} 100(x_{i + 1} - x_i^2)^2 + (1 - x_i)^2, \quad \text{for~$x \in \R^n$.}
\end{equation*}
As those afterwards, it is clearly unreasonable to employ a \gls{dfo} method to solve such a problem and it only serves as an example.
\Cref{lst:cobyqa-rosenbrock} shows how to solve such a problem with~$n = 5$ using \gls{cobyqa}.

\begin{lstpython}[%
    caption=Solving the Rosenbrock problem using \gls{cobyqa},
    label=lst:cobyqa-rosenbrock,
]
    >>> from scipy.optimize import rosen
    >>> from cobyqa import minimize
    >>>
    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]  # or anything else
    >>> res = minimize(rosen, x0)
    >>> res.x
    array([1., 1., 1., 1., 1.]) 
\end{lstpython}

To see how to supply to \gls{cobyqa} bound and linear constraints, consider the Example 16.4 of~\cite{Nocedal_Wright_2006}, defined as
\begin{align*}
    \min_{\iter \in \R^2}   & \quad (\iter_1 - 1)^2 + (\iter_2 - 2.5)^2\\
    \text{s.t.}             & \quad -\iter_1 + 2 \iter_2 \le 2,\\
                            & \quad \iter_1 + 2 \iter_2 \le 6,\\
                            & \quad \iter_1 - 2 \iter_2 \le 2,\\
                            & \quad \iter_1 \ge 0,\\
                            & \quad \iter_2 \ge 0.
\end{align*}
\Cref{lst:cobyqa-bound-linear} shows how to solve this problem using \gls{cobyqa}.

\begin{lstpython}[%
    caption=An example of \gls{cobyqa} with linear constraints,
    label=lst:cobyqa-bound-linear,
]
    >>> from cobyqa import minimize
    >>>
    >>> def obj(x):
    ...     return (x[0] - 1.0) ** 2.0 + (x[1] - 2.5) ** 2.0
    >>>
    >>> x0 = [2.0, 0.0]
    >>> xl = [0.0, 0.0]
    >>> Aub = [[-1.0, 2.0], [1.0, 2.0], [1.0, -2.0]]
    >>> bub = [2.0, 6.0, 2.0]
    >>> res = minimize(quadratic, x0, xl=xl, Aub=Aub, bub=bub)
    >>> res.x
    array([1.4, 1.7])
\end{lstpython}

Thus, to see how to supply nonlinear constraints to \gls{cobyqa}, consider the Problem G of~\cite{Powell_1994}, defined as
\begin{align*}
    \min_{\iter \in \R^3}   & \quad \iter_3\\
    \text{s.t.}             & \quad -5 \iter_1 + \iter_2 - \iter_3 \le 0,\\
                            & \quad 5 \iter_1 + \iter_2 - \iter_3 \le 0,\\
                            & \quad \iter_1^2 + \iter_2^2 + 4 \iter_2 - \iter_3 \le 0.
\end{align*}
\Cref{lst:cobyqa-problem-g} shows how to solve such a problem using \gls{cobyqa}, starting from the initial guess~$\iter[0] = [1, 1, 1]^{\T}$.

\begin{lstpython}[%
    caption=An example of \gls{cobyqa} with nonlinear constraints,
    label=lst:cobyqa-problem-g,
]
    >>> from cobyqa import minimize
    >>>
    >>> def obj(x):
    ...     return x[2]
    >>>
    >>> def cub(x):
    ...     return x[0] ** 2.0 + x[1] ** 2.0 + 4.0 * x[1] - x[2] 
    >>>
    >>> x0 = [1.0, 1.0, 1.0]
    >>> Aub = [[-5.0, 1.0, -1.0], [5.0, 1.0, -1.0]]
    >>> bub = [0.0, 0.0]
    >>> res = minimize(obj, x0, Aub=Aub, bub=bub, cub=cub)
    >>> res.x
    array([ 0., -3., -3.])   
\end{lstpython}

\subsubsection{Details on the returned structure}

The function \texttt{minimize} returns a structure (that we named \texttt{res} in the examples above) whose attributes are details in \cref{tab:optimize-result}.

\begin{table}[ht]
    \caption{Structure for the result of the optimization algorithm}
    \label{tab:optimize-result}
    \centering
    \begin{tabularx}{\textwidth}{cX}
        \toprule
        \texttt{x}          & Solution point provided by the optimization solver.\\
        \midrule
        \texttt{success}    & Flag indicating whether the optimization solver terminated successfully.\\
        \midrule
        \texttt{status}     & Termination status of the optimization solver.\\
        \midrule
        \texttt{message}    & Description of the termination status of the optimization solver.\\
        \midrule
        \texttt{fun}        & Value of the objective function at the solution point provided by the optimization solver.\\
        \midrule
        \texttt{jac}        & Approximation of the gradient of the objective function at the solution point provided by the optimization solver. If the value of a component is unknown, it is replaced by NaN.\\
        \midrule
        \texttt{nfev}       & Number of objective and constraint function evaluations.\\
        \midrule
        \texttt{nit}        & Number of iterations performed by the optimization solver.\\
        \midrule
        \texttt{maxcv}      & Maximum constraint violation at the solution point provided by the optimization solver. It is set only if the problem is not declared unconstrained by the optimization solver.\\
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Distribution of the package}

The subproblem solvers of \gls{cobyqa} are written in Cython, which necessitate a C/C++ compiler to work.
However, we made wheel distributions available for
\begin{enumerate}
    \item Windows, for both 32-bit and 64-bit operating systems,
    \item macOS, for both x64 and ARM64 architectures, and
    \item Linux, for both x64 and ARM64 architectures.
\end{enumerate}
The current version of \gls{cobyqa} is available for Python 3.7, 3.8, 3.9, and 3.10 (i.e., all the Python version available as of today).
The compilation is made using GitHub Actions, and relies on the Python package cibuildwheel\footnote{See \url{https://cibuildwheel.readthedocs.io/}.}.

\section{Some numerical experiments}
\label{sec:cobyqa-experiments}

To conclude this chapter, we present several numerical results for \gls{cobyqa}.
The performance and data profiles presented in this section have been obtained on the CUTEst library~\cite{Gould_Orban_Toint_2015}.
The experiments select all the available problems with a dimension at most \num{50} and a number of constraints at most \num[group-minimum-digits=4]{1000}.
See \cref{sec:benchmarking-tools} for the definition of the performance and data profiles in a \gls{dfo} context.
The merit function we consider in the following is defined as follows.
Let~$v_{\infty}(\iter) \in \R$ be the~$\ell_{\infty}$-constraint violation at~$\iter \in \R^n$, and define the merit function~$\varphi$ employed by the performance profiles by
\begin{empheq}[left={\varphi(\iter) = \empheqlbrace}]{alignat*=2}
    & \obj(\iter),                              && \quad \text{if~$v_{\infty}(\iter) \le \nu_1$,}\\
    & \infty,                                   && \quad \text{if~$v_{\infty}(\iter) \ge \nu_2$,}\\
    & \obj(\iter) + \gamma v_{\infty}(\iter),   && \quad \text{otherwise,}
\end{empheq}
for~$\iter \in \R^n$.
We choose in the experiments~$\nu_1 = 10^{-10}$,~$\nu_2 = 10^{-5}$, and~$\gamma = 10^5$.
Note that~$v_{\infty}$ is always zero for unconstrained problems.

\subsection{Performance of the \glsfmtshort{sqp} framework}

We observe in what follows the effects of the \gls{sqp} framework employed by \gls{cobyqa}.
We then compare the \gls{cobyqa} with a modification that replaces the term~$\nabla_{x, x}^2 \lagm[k](\iter[k], \lm[k])$ in its trust-region \gls{sqp} subproblem~\cref{eq:derivative-free-trust-region-sqp-subproblem} with~$\nabla^2 \objm[k](\iter[k])$.
Although it is known that the resulting algorithm is invalid, making the comparison between the two methods is interesting because it reveals the importance of the strategy.

Therefore, we make the following numerical experiments.
We consider the nonlinearly-constrained problems of the CUTEst library~\cite{Gould_Orban_Toint_2015} whose dimensions are at most \num{50} with at most \num[group-minimum-digits=4]{1000} constraints.
\Cref{sec:list-nonlinearly-constrained-problems} provides a detailed list of these problems.
We then plot the performance and data profiles for the two methods in \cref{fig:perf-wrong-hessian,fig:data-wrong-hessian}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-perf-cobyqa-wrong-hessian-qo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-perf-cobyqa-wrong-hessian-qo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-perf-cobyqa-wrong-hessian-qo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-perf-cobyqa-wrong-hessian-qo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles for the \gls{sqp} framework}
    \label{fig:perf-wrong-hessian}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-data-cobyqa-wrong-hessian-qo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-data-cobyqa-wrong-hessian-qo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-data-cobyqa-wrong-hessian-qo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-data-cobyqa-wrong-hessian-qo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles for the \gls{sqp} framework}
    \label{fig:data-wrong-hessian}
\end{figure}

We observe that \gls{cobyqa} performs much better than the modification for all tolerances considered.
It solves more problems, and uses less function evaluations than the modified algorithms on most of the problems.
Note that this observation is perfectly excepted.
We presented an example in \cref{subsec:sqp-simple-example} on which the modified \gls{sqp} framework does not work, while the true framework does.
Moreover, all the interpretations we presented in \cref{subsec:sqp-interpretation} indicate that it is necessary to employ~$\nabla_{x, x}^2 \lagm[k](\iter[k], \lm[k])$ in the second-order term of the objective function of the \gls{sqp} subproblem.

We can also observe on these plots that the estimated Lagrange multipliers we chose for \gls{cobyqa} are effective.
In the future, we may also attempt different strategy to estimate the Lagrange multipliers.

\subsection{Performance of the new Byrd-Omojokun approach}
\label{subsec:perf-byrd-omojokun}

We now compare the performance of \gls{cobyqa} with another modification.
More specifically, we replace our Byrd-Omojokun approach with the traditional Byrd-Omojokun approach, presented in~\cite[\S~15.4.4]{Conn_Gould_Toint_2000}.
The objective of this experiment is to show that our approach provides a much greater performance for \gls{cobyqa}.

We make the following numerical experiments.
We consider the linearly- and nonlinearly-constrained problems of the CUTEst library~\cite{Gould_Orban_Toint_2015} whose dimensions are at most \num{50} with at most \num[group-minimum-digits=4]{1000} constraints.
\Cref{sec:list-linearly-constrained-problems,sec:list-nonlinearly-constrained-problems} provide detailed lists of these problems.
We plot the performance and data profiles for the two methods in \cref{fig:perf-byrd-omojokun,fig:data-byrd-omojokun}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-perf-cobyqa-byrd-omojokun-nlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-perf-cobyqa-byrd-omojokun-nlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-perf-cobyqa-byrd-omojokun-nlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-perf-cobyqa-byrd-omojokun-nlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles for the Byrd-Omojokun approaches}
    \label{fig:perf-byrd-omojokun}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-data-cobyqa-byrd-omojokun-nlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-data-cobyqa-byrd-omojokun-nlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-data-cobyqa-byrd-omojokun-nlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-data-cobyqa-byrd-omojokun-nlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles for the Byrd-Omojokun approaches}
    \label{fig:data-byrd-omojokun}
\end{figure}

The results are inequivocal; our Byrd-Omojokun approach unquestionably overcomes the traditional Byrd-Omojokun approach in \gls{cobyqa}, for all displayed tolerances.
This can be explain by the fact that our Byrd-Omojokun approach provides a larger feasible region to the tangential steps, without worsening their feasibility.
Hence, a larger reduction in the objective function is usually obtained at each iteration.
It is interesting to observe that the difference is even greater than in the previous experiment.

\subsection{Performance of the quadratic models}
\label{subsec:alternative-models}

We now compare the performance of \gls{cobyqa} with two modifications.
Recall that \gls{cobyqa} employs quadratic models based on the derivative-free symmetric Broyden update (see \cref{subsec:symmetric-broyden-updates}).
Morever, recall that it includes a strategy to replace these models with the minimum Frobenius norm models if the method considers that the current models are inefficient (this replacement being exceptional).
The two modifications of \gls{cobyqa} we make are as follows.
The first one, referred to as \gls{cobyqa}~PSB, uses only the models based on the derivative-free symmetric Broyden update, without any replacement.
The second one, referred to as \gls{cobyqa}~MNH, uses only minimum Frobenius norm models.

We make the following numerical experiments.
We consider all the problems of the CUTEst library~\cite{Gould_Orban_Toint_2015} whose dimensions are at most \num{50} with at most \num[group-minimum-digits=4]{1000} constraints.
\Cref{sec:list-unconstrained-problems,sec:list-bound-constrained-problems,sec:list-linearly-constrained-problems,sec:list-nonlinearly-constrained-problems} provide detailed lists of these problems.
We plot the performance and data profiles for the three methods in \cref{fig:perf-models,fig:data-models}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-perf-cobyqa-models-ubnlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-perf-cobyqa-models-ubnlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-perf-cobyqa-models-ubnlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-perf-cobyqa-models-ubnlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles for the different models}
    \label{fig:perf-models}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-data-cobyqa-models-ubnlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-data-cobyqa-models-ubnlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-data-cobyqa-models-ubnlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-data-cobyqa-models-ubnlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles for the different models}
    \label{fig:data-models}
\end{figure}

We can draw the same conclusions for all considered tolerances, namely that \gls{cobyqa} performs better than \gls{cobyqa}~PSB, which itself performs better than \gls{cobyqa}~MNH.
More precisely, we can observe that \gls{cobyqa} always outperforms the two modifications for each precision at each iteration.
These results comfort us in the choice of the quadratic models.

Powell proposed a sophisticated mechanism to replace the quadratic models obtained by the derivative-free symmetric Broyden update of \gls{lincoa} with the minimum Frobenius norm ones.
We plan in the future to always try this mechanism, as it may provide even better performance for \gls{cobyqa}.

\subsection{Comparison on unconstrained problems}

In the remaining of this chapter, we compare the performance of \gls{cobyqa} with the Powell's \gls{dfo} methods that are available through \gls{pdfo}.
In all the comparisons, we include both \gls{cobyqa} and \gls{cobyla}, because both are capable of handling all types of problems, and because \gls{cobyqa} is expected to be a successor of \gls{cobyla}.

In this section, we first compare the performances of \gls{cobyqa}, \gls{newuoa}, and \gls{cobyla} on unconstrained problems.
We make the following numerical experiments.
We consider all the unconstrained problems of the CUTEst library~\cite{Gould_Orban_Toint_2015} whose dimensions are at most \num{50}.
\Cref{sec:list-unconstrained-problems} provides a detailed list of these problems.
We plot the performance and data profiles for the three methods in \cref{fig:perf-unconstrained-problems,fig:data-unconstrained-problems}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-newuoa-u.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-newuoa-u.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-newuoa-u.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-newuoa-u.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles on unconstrained problems}
    \label{fig:perf-unconstrained-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-newuoa-u.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-newuoa-u.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-newuoa-u.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-newuoa-u.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles on unconstrained problems}
    \label{fig:data-unconstrained-problems}
\end{figure}

We first observe that \gls{cobyqa} outperforms \gls{cobyla} in general.
For the highest tolerance (i.e.,~$\tau = 10^{-1}$), it necessitates the smallest number of function evaluations to converge on most of the problems.
Specifically, it needed the smallest number of functions evaluations to converge on \SI{45}{\percent} of the problems, while the percentage for \gls{cobyqa} is \SI{25}{\percent}.
Moreover, without any restriction on the number of function evaluations, they both converge for more than \SI{90}{\percent} of the problems.
This behavior is perfectly expected, because \gls{cobyla} necessitates the least amount of function evaluations to build the initial models.
As expected, this performance drops drastically when the precision decreases.

We also observe that \gls{cobyqa} and \gls{newuoa} have comparable performance for high and moderate tolerance.
For the lowest tolerance, \gls{cobyqa} is slightly outperformed by \gls{newuoa}, although the difference is not severe.
More specifically, for the tolerance~$\tau \ge 10^{-5}$ presented here, the difference observed between \gls{cobyqa} and \gls{newuoa} is insignificant, and we can conclude that both methods performed the same.
When~$\tau = 10^{-7}$, \gls{newuoa} converged faster than \gls{cobyqa} on \SI{55}{\percent} of the problems, while the percentage for \gls{cobyqa} is \SI{40}{\percent}.
\Gls{newuoa} finally solved \SI{88}{\percent} of the problems, while the percentage for \gls{cobyqa} is \SI{80}{\percent}.
In short, we can conclude that \gls{cobyqa} does not perform significantly worse than \gls{newuoa}.
This behavior is expected because both methods share very similar algorithms.
The difference between both method is likely to come from minor difference in the algorithms, in their implementations, and from computer rounding errors.

\subsection{Comparison on bound-constrained problems}

We now compare the performance of \gls{cobyqa} with \gls{bobyqa} and \gls{cobyla} on bound-constrained problems.
We make the following numerical experiments.
We consider all the bound-constrained problems of the CUTEst library~\cite{Gould_Orban_Toint_2015} whose dimensions are at most \num{50} and whose solutions are not fixed by the bounds.
\Cref{sec:list-bound-constrained-problems} provides a detailed list of these problems.
We plot the performance and data profiles for the three methods in \cref{fig:perf-bound-constrained-problems,fig:data-bound-constrained-problems}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-perf-bobyqa-cobyla-cobyqa-b.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-perf-bobyqa-cobyla-cobyqa-b.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-perf-bobyqa-cobyla-cobyqa-b.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-perf-bobyqa-cobyla-cobyqa-b.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles on bound-constrained problems}
    \label{fig:perf-bound-constrained-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-data-bobyqa-cobyla-cobyqa-b.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-data-bobyqa-cobyla-cobyqa-b.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-data-bobyqa-cobyla-cobyqa-b.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-data-bobyqa-cobyla-cobyqa-b.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles on bound-constrained problems}
    \label{fig:data-bound-constrained-problems}
\end{figure}

The comparison of \gls{cobyqa} with \gls{cobyla} is exactly the same than the one in the unconstrained case.
In other words, \gls{cobyqa} always outperforms \gls{cobyla}, except that for the highest tolerance, \gls{cobyla} converges faster on more problems than \gls{cobyqa}, the percentages being \SI{45}{\percent} and \SI{30}{\percent}.

Moreover, the comparison of \gls{cobyqa} with \gls{bobyqa} on bound-constrained problems is similar to the one between \gls{cobyqa} and \gls{newuoa} on unconstrained problems.
More specifically, they are very similar performance for high and moderate tolerance, and \gls{bobyqa} is slightly better when the tolerance is low.
However, for the tolerance~$\tau \le 10^{-3}$, \gls{cobyqa} always converges faster on more problems than \gls{bobyqa}, the difference being between \SI{10}{\percent} and \SI{20}{\percent}.
All in all, we can conclude that \gls{cobyqa} and \gls{bobyqa} share very similar performance.
This is also perfectly expected, because both solvers use similar methods.

\subsection{Comparison on linearly-constrained problems}
\label{subsec:comparison-cobyqa-lincoa}

We now compare the performance of \gls{cobyqa} with \gls{lincoa} and \gls{cobyla} on linearly-constrained problems.
We make the following numerical experiments.
We consider all the linearly-constrained problems of the CUTEst library~\cite{Gould_Orban_Toint_2015} whose dimensions are at most \num{50} with at most \num[group-minimum-digits=4]{1000} constraints.
\Cref{sec:list-linearly-constrained-problems} provides a detailed list of these problems.
We plot the performance and data profiles for the three methods in \cref{fig:perf-linearly-constrained-problems,fig:data-linearly-constrained-problems}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles on linearly-constrained problems}
    \label{fig:perf-linearly-constrained-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles on linearly-constrained problems}
    \label{fig:data-linearly-constrained-problems}
\end{figure}

Once again, in this experiment, \gls{cobyqa} outperforms \gls{cobyla}, particularly with low precision.
For high tolerances, \gls{cobyla} again necessitates the smallest number of function evaluations to converge on most of the problems.

However, \gls{cobyqa} is always outperformed by \gls{lincoa} to some degree.
More precisely, for the highest tolerance, the performance are very comparable, but lower tolerances, \gls{lincoa} converges faster than \gls{cobyqa} on \SI{60}{\percent} of the problems, the percentage for \gls{cobyqa} being around \SI{20}{\percent}.
Finally, when \gls{lincoa} converges on more than \SI{90}{\percent} of the problems, \gls{cobyqa} converges on \SI{85}{\percent}.
The difference between the two solvers is however not alarming.
It is likely that some strategy that \gls{lincoa} employs, such as that for replacing the models with the minimum Frobenius norm ones, performs better on linearly-constrained problems.
Moreover, after investigation, the author observed that the implementations of all the solvers are extremely sensible to the way it is coded.
For example, changing the order of the summation in an inner product may change the performance of an algorithm.
We will study this phenomenon carefully in the future.

We now consider another experiments, showing the advantages of \gls{cobyqa} over \gls{lincoa} and \gls{cobyla} is applications for which \gls{cobyqa} is designed.
We consider all the linearly-constrained problems of the CUTEst library~\cite{Gould_Orban_Toint_2015} whose dimensions are at most \num{50} with at most \num[group-minimum-digits=4]{1000} constraints, which admit at least one bound constraint.
Further, we set the objective function of the problem to~$\infty$ at the points at which the bound constraints are not satisfied.
This experiment reflects more closely the behavior faced by the optimization in engineering and industrial applications.
We plot the performance and data profiles for the three methods in \cref{fig:perf-linearly-constrained-problems-bounds,fig:data-linearly-constrained-problems-bounds}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles on linearly-constrained problems with bounds}
    \label{fig:perf-linearly-constrained-problems-bounds}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles on linearly-constrained problems with bounds}
    \label{fig:data-linearly-constrained-problems-bounds}
\end{figure}

On this experiment, \gls{cobyqa} overwhelmingly outperforms \gls{lincoa} and \gls{cobyqa}, particularly for low tolerances.
\gls{cobyqa} finally solves more problems, and uses the least amount of function evaluations on most of them.
This is important, because \gls{cobyqa} is designed to tackle problems for which the bound constraints cannot be violated.

\subsection{Comparison on nonlinearly-constrained problems}

Finally, we compare \gls{cobyqa} with \gls{cobyla} on nonlinearly-constrained problems.
We make the following numerical experiments.
We consider all the nonlinearly-constrained problems of the CUTEst library~\cite{Gould_Orban_Toint_2015} whose dimensions are at most \num{50} with at most \num[group-minimum-digits=4]{1000} constraints.
\Cref{sec:list-nonlinearly-constrained-problems} provides a detailed list of these problems.
We plot the performance and data profiles for the two methods in \cref{fig:perf-nonlinearly-constrained-problems,fig:data-nonlinearly-constrained-problems}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles on nonlinearly-constrained problems}
    \label{fig:perf-nonlinearly-constrained-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles on nonlinearly-constrained problems}
    \label{fig:data-nonlinearly-constrained-problems}
\end{figure}

We observe that \gls{cobyqa} provides better performance than \gls{cobyla} for all considered tolerances.
More specifically, \gls{cobyqa} converges faster than \gls{cobyla} on \SI{60}{\percent} of the problems, the percentage being between \SI{30}{\percent} and \SI{40}{\percent} for \gls{cobyla}, depending on the tolerance.
Finally, they both solve the same amount of problem, namely aroung \SI{80}{\percent}.
To achieve the same performance than \gls{cobyqa}, \gls{cobyla} needs in average~$2^3 = 8$ times more function evaluations than \gls{cobyqa}.
This is very satisfactory, since \gls{cobyqa} is designed as a successor of \gls{cobyla}.

To observe the general performance of \gls{cobyqa} over \gls{cobyla}, we plot in \cref{fig:perf-all-problems,fig:data-all-problems} the performance and data profiles on all the CUTEst problems of dimension at most \num{50}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-ubnlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-ubnlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-ubnlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-ubnlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles on all problems}
    \label{fig:perf-all-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-ubnlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-ubnlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-ubnlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-ubnlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles on all problems}
    \label{fig:data-all-problems}
\end{figure}

The results are unequivocal; \gls{cobyqa} outperforms \gls{cobyla} with a significant margin.
This is explain by the fact that \gls{cobyla} uses linear models of the objective and constraint functions, while \gls{cobyqa} employs quadratic models.
These results are hence not surprising.

We also compare the performance of \gls{cobyqa} with \gls{cobyla} on the nonlinearly-constrained problems that admit at least one bound constraint, setting the objective functions of the problems to be~$\infty$ at the points where the bounds are violated.
The performance and data profiles are provided in \cref{fig:perf-nonlinearly-constrained-problems-bounds,fig:data-nonlinearly-constrained-problems-bounds}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles on nonlinearly-constrained problems with bounds}
    \label{fig:perf-nonlinearly-constrained-problems-bounds}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles on nonlinearly-constrained problems with bounds}
    \label{fig:data-nonlinearly-constrained-problems-bounds}
\end{figure}

The results are similar to the ones in \cref{fig:perf-nonlinearly-constrained-problems,fig:data-nonlinearly-constrained-problems}, altough the performance of \gls{cobyqa} compared to that of \gls{cobyla} increases even more.
This is also perfectly expected, because \gls{cobyla} is not designed to always satisfy the bound constraints.

To conclude this section, we make the numerical experiment that reflects the most the reality for which \gls{cobyqa} is designed.
We compare \gls{cobyqa} with \gls{cobyla} on all the problems that admit at least one bound constraint, setting their objective functions to~$\infty$ outside of the bounds.
The performance and data profiles are provided in \cref{fig:perf-all-problems-bounds,fig:data-all-problems-bounds}, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-bnlqo-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-bnlqo-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-bnlqo-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-bnlqo-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Performance profiles on all problems with bounds}
    \label{fig:perf-all-problems-bounds}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-bnlqo-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-bnlqo-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-bnlqo-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-bnlqo-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption{Data profiles on all problems with bounds}
    \label{fig:data-all-problems-bounds}
\end{figure}

All in all, the performance of \gls{cobyqa} are compellingly better than that of \gls{cobyla}, particularly for moderate and high tolerances.
Therefore, \gls{cobyqa} is a great successor for \gls{cobyla}.

\section{Summary and remarks}

In this chapter, we presented the Python implementation of \gls{cobyqa}, and several numerical experiments.
In particular, we saw that the implementation avoids as much as possible to raise exceptions if the inputs do not satisfies the preconditions of the framework described in \cref{ch:cobyqa-introduction}.
The stopping criteria employed by \gls{cobyqa} are usual for a \gls{dfo} method.
Normally, the computations of \gls{cobyqa} stop whenever the indicator of the resolution of the algorithm reached a final value.
The implementation of \gls{cobyqa} has been highly challenging, because the nature of \gls{dfo} necessitate highly dedicated pieces of code.

Moreover, we strove to make the user's interface as simple as possible, to make its use easy.
A complete user's guide for using \gls{cobyqa} is available on the website of the solver\footnote{Available at \url{https://www.cobyqa.com/}.}.
We implemented the subproblem solvers in Cython, a programming language that blends the advantages of both Python and C.
Cython is a compiled language and is, therefore, much faster than Python alone.
The compilation of \gls{cobyqa} requires several dependencies (depending on the operating system), but compiled distributions are provided to ease the installation.

Finally, the numerical results obtained on \gls{cobyqa} are very encouraging.
They show that the method is working on most CUTEst problems, although a theoretical analysis of its convergence analysis needs to be done in the future.
The performance of \gls{cobyqa} is comparable to those of \gls{newuoa} and \gls{bobyqa} on unconstrained and bound-constrained problems, respectively, while it is able to tackle more general problems.
Moreover, compared to \gls{lincoa} and \gls{cobyla}, a strength of \gls{cobyqa} is that is always respect bound constraints.
On linearly-constrained problems, \gls{cobyqa} outperforms \gls{lincoa} on problems that contain inviolable bound constraints.
Most importantly, \gls{cobyqa} has much better performance than \gls{cobyla} on all types of problems, no matter whether bound constraints (if any) can be violated or not.
Ultimately, \gls{cobyqa} is a great successor of \gls{cobyla}.
