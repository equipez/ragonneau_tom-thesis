%% contents/cobyqa-implementation.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{\glsfmttext{cobyqa} \textemdash\ implementation and experiments}
\label{ch:cobyqa-implementation}

In this chapter, we provide details on the implementation of \gls{cobyqa}.
% This implementation is a very technical work.
In particular, \cref{sec:implementation-details} presents in detail the management of the user's inputs, describes every stopping criterion, and exhibits a mechanism for returning the best iterate, which is not necessarily the last one.
\Cref{sec:python-implementation} introduces the Python implementation, which is open-source and publicly available, and it also provides some examples of use.
Finally, we present in \cref{sec:cobyqa-experiments} some numerical experiments, comparing in different scenarios \gls{cobyqa} with \gls{newuoa}, \gls{bobyqa}, \gls{lincoa}, and \gls{cobyla}.
These numerical experiments confirm the good performance of \gls{cobyqa}.
In particular, it evidently outperforms \gls{cobyla} on all experiments, often by a significant margin.
% These numerical experiments confirm that \gls{cobyqa} perform compatively with \gls{newuoa}, \gls{bobyqa}, while being able to solve more general problems; moreover, it performs much better than \gls{lincoa} on problems with inviolable bounds.
% More importantly, it outperforms \gls{cobyla} on all types of problems.

% We decided to implement the first version of \gls{cobyqa} in Python for its simplicity and its large amount of users.
% However, Python has the computational flows of an interpreted language, making it slow compared to compiled languages.
% Therefore, we implement the subproblem solvers of \gls{cobyqa} in Cython, a programming language that blends the advantages of both Python and C.
% Cython is a compiled language and is, therefore, much faster than Python alone.

\section{Additional algorithmic details}
\label{sec:implementation-details}

\subsection{Preprocessing of the inputs}

When presenting the framework of \gls{cobyqa} in \cref{ch:cobyqa-introduction}, we assumed that the bound constraints satisfy~$\xl < \xu$, and that the initial trust-region radius satisfies
\begin{equation}
    \label{eq:initial-trust-region-radius-condition}
    \rad[0] \le \frac{1}{2} \min_{1 \le i \le n} @@ (\xu_i - \xl_i).
\end{equation}
However, the inputs received from the user may not fulfill these conditions.
In the implementation of \gls{cobyqa}, we preprocess the inputs to satisfy them as long as possible.

First of all, if~$\xl_i > \xu_i$ for some~$i \in \set{1, 2, \dots, n}$, then no computation is attempted since the problem is infeasible.
If there exist bound constraints such that~$\xl_i$ and~$\xu_i$ takes the same value, then we fix the corresponding component of~$x$ at this value and perform the computations on the problem restricted to the remaining components of~$x$.
After this restriction, if~\cref{eq:initial-trust-region-radius-condition} does not hold, then the value of~$\rad[0]$ is reduced to the right-hand side of~\cref{eq:initial-trust-region-radius-condition}.
% If the initial does not satisfy the bound constraints we project it to the box defined by these constraints.
% Note that this projection is already described in \cref{subsec:interpolation-based-quadratic-models}, because the initial is modified so that each component is either on a bound or keeps a distance of at least~$\rad[0]$ from the corresponding bounds.

% Recall from \cref{subsec:pdfo-preprocessing} that the package \gls{pdfo} removes the linear equality constraints by defining a subspace of~$\R^n$ of lower dimension, and ensures that each iterate lies in this subspace.
% This is not necessary for \gls{cobyqa}, which handles equality constraints directly.
% Moreover, the author plans to include \gls{cobyqa} into \gls{pdfo} in the future.
% Hence, this feature will become available.

\subsection{Additional stopping criteria}

The only stopping criterion presented so far is that we terminate the algorithm when the lower-bound~$\radlb[k]$ needs to be reduced to a value below a threshold~$\radlb[\infty]$.
However, in the implementation, we also stop the computations if
\begin{enumerate}
    \item the number of function evaluations reaches a maximal value,
    \item the number of iterations reaches a maximal value,
    \item a target value on the objective function is reached by a feasible iterate,
\end{enumerate}
The only remaining stopping criterion is when the computations must stop due to computer rounding errors.
In practice, this happens if the denominator of the updating formula described in \cref{ch:cobyqa-introduction} becomes zero, which should not happen in exact arithmetic.

% \begin{table}[ht]
%     \caption{Exit statuses of \gls{cobyqa}}
%     \label{tab:exit-statuses}
%     \centering
%     \begin{tabularx}{\textwidth}{cX}
%         \toprule
%         $0$     & The lower bound for the trust-region radius has been reached.\\
%         \midrule
%         $1$     & A feasible iterate reached the target value on the objective values.\\
%         \midrule
%         % $2$     & The desired absolute tolerance on the objective values of two successive iterates is reached.\\
%         % \midrule
%         % $3$     & The desired relative tolerance on the objective values of two successive iterates is reached.\\
%         % \midrule
%         % $4$     & The desired absolute tolerance on two successive iterates in~$\ell_2$-norm is reached.\\
%         % \midrule
%         % $5$     & The desired relative tolerance on two successive iterates in~$\ell_2$-norm is reached.\\
%         % \midrule
%         $2$     & The maximum number of function evaluations has been exceeded.\\
%         \midrule
%         $3$     & The maximum number of iterations has been exceeded.\\
%         \midrule
%         $4$     & The denominator of the updating formula is zero.\\
%         \midrule
%         $5$     & All variables are fixed by the constraints.\\
%         \midrule
%         $-1$    & The bound constraints are infeasible.\\
%         \bottomrule
%     \end{tabularx}
% \end{table}

\subsection{Returning the best iterate}

It is important to remark that the current iterate may not be the best point encountered so far by \gls{cobyqa}.
This depends on the penalty parameters in the merit function that we use to assess the quality of the iterates.
Indeed, a point that is removed from a previous interpolation set may be better than the current iterate according to the current merit function.
% Therefore, \gls{cobyqa} can store the history of all the points visited, together with the corresponding objective and constraint values.
% Since this mechanism is expensive in terms of memory, it is executed only if the user desires it.
Therefore, it is not always reasonable to return the last iterate as the approximate solution when \gls{cobyqa} terminates.

Instead, \gls{cobyqa} returns a point that is selected in the following way from all the points visited upon the termination.
First of all, we consider only the points whose constraint violations are at most twice as large as the least one.
Hence, if a feasible point has been produced, only the feasible points will be under consideration.
Among all these points, \gls{cobyqa} selects the one that minimizes the merit function value, with the penalty parameter taking the value of the last iteration.
If several minimizers exist, we choose the one with the least constraint violation; 
if multiple choices are still possible, we select one with the least objective value.

In order to perform the selection specified above, the current version of \gls{cobyqa} stores all the iterates, together with the corresponding objective function values and constraint violations.
This is expensive in terms of memory.
Indeed, it is not necessary to save a point if both its objective function value and its constraint violation are larger than those of another point visited by \gls{cobyqa}.
We will improve the implementation of \gls{cobyqa} in this way in future versions.

\section{Description of the Python implementation}
\label{sec:python-implementation}

In this section, we provide details on the Python implementation of \gls{cobyqa} and the way we distribute it.

\subsection{Choice of programming languages}

We chose to develop \gls{cobyqa} in Python because it is a simple open-source language, widely used by both researchers and industrial practitioners.
% Although the current version of \gls{cobyqa} is mostly written in Python, the author plans to develop a Fortran version in the future.
% This would have the ability to be interfaced with several other languages, such a Python, MATLAB, or Julia.
% However, developing software in Fortran is time-consuming, and hence, this work will be carried out in the future.
However, the subproblem solvers described in \cref{ch:cobyqa-subproblems} are not written in pure Python, but in Cython~\cite{Behnel_Etal_2011}, which is a compiled language that aims at improving the performance of Python code.
The motivation is that these solvers are the bottleneck for the execution time of the linear algebra calculations made by \gls{cobyqa}.
Indeed, the initial version of \gls{cobyqa} was entirely written in Python, but the execution time was prohibitively long on problems of dimensions~$n = 20$ and above.

Another motivation for us to develop the \gls{cobyqa} in Python is the extensive availability of libraries.
\Gls{cobyqa} does not only rely on Cython, but also on NumPy~\cite{Harris_Etal_2020} and SciPy~\cite{Virtanen_Etal_2020}.
These packages provide array structures, together with basic mathematical operations and basic scientific computing tools.
In particular, SciPy provides interfaces for BLAS~\cite{Blackford_Etal_2002} and LAPACK~\cite{Anderson_Etal_1999} subroutines in Cython, which are intensively used by the subproblem solvers.

\subsection{Distribution of the package}

The Python implementation of \gls{cobyqa} is open-source and distributed under the BSD-3-Clause license.
The source code of \gls{cobyqa} is available at
\begin{center}
    \url{https://github.com/ragonneau/cobyqa}.
\end{center}
A complete documentation for \gls{cobyqa} is available at
\begin{center}
    \url{https://www.cobyqa.com/}.
\end{center}

A C/C++ compiler is needed to build the subproblem solvers of \gls{cobyqa} written in Cython.
To ease the efforts of users, we made wheel distributions available for
\begin{enumerate}
    \item Windows, both 32-bit and 64-bit operating systems,
    \item macOS, both x64 and ARM64 architectures, and
    \item Linux, both x64 and ARM64 architectures.
\end{enumerate}
The current version of \gls{cobyqa} is available for Python 3.7, 3.8, 3.9, and 3.10, i.e., all the Python version supported as of August 2022.
% The Python wheels are compiled using GitHub Actions, and relies on the Python package cibuildwheel\footnote{See \url{https://cibuildwheel.readthedocs.io/}.}.

\subsection{Illustrations of usage}

The package \gls{cobyqa} provides a function \texttt{minimize}, which takes the following arguments.
\begin{enumerate}
    \item The objective function to be minimized,
    \item the initial guess,
    \item bound constraints (optional),
    \item linear constraints (optional),
    \item nonlinear constraints (optional), and
    \item a dictionary of options (listed in \cref{tab:cobyqa-options}, each being optional).
\end{enumerate}

\begin{table}[ht]
    \caption{Options of \gls{cobyqa}}
    \label{tab:cobyqa-options}
    \centering
    \begin{tabularx}{\textwidth}{cX}
        \toprule
        \texttt{rhobeg}     & Initial trust-region radius\\
        \midrule
        \texttt{rhoend}     & Final trust-region radius\\
        \midrule
        \texttt{npt}        & Number of interpolation points\\
        \midrule
        \texttt{maxfev}     & Maximum number of objective and constraint function evaluations\\
        \midrule
        \texttt{maxiter}    & Maximum number of iterations\\
        \midrule
        \texttt{target}     & Target value on the objective function\\
        % \midrule
        % \texttt{ftol\_abs}  & Absolute tolerance on the objective function.\\
        % \midrule
        % \texttt{ftol\_rel}  & Relative tolerance on the objective function.\\
        % \midrule
        % \texttt{xtol\_abs}  & Absolute tolerance on the decision variables.\\
        % \midrule
        % \texttt{xtol\_rel}  & Relative tolerance on the decision variables.\\
        \midrule
        \texttt{disp}       & Whether to print pieces of information on the execution of the solver\\
        \midrule
        \texttt{debug}      & Whether to make debugging tests during the execution\\
        \bottomrule
    \end{tabularx}
\end{table}

In the following, we will provide example to illustrate how to specify the aforementioned arguments.

\subsubsection{Examples of usage}

For example, consider the unconstrained minimization of the chained Rosenbrock function
\begin{equation*}
    \obj(x) = \sum_{i = 1}^{n - 1} 100(x_{i + 1} - x_i^2)^2 + (1 - x_i)^2, \quad \text{for~$x \in \R^n$.}
\end{equation*}
It only serves as an illustration.
In practice, it is clearly unreasonable to employ a \gls{dfo} method to solve such a problem.
\Cref{lst:cobyqa-rosenbrock} shows how to solve such a problem using \gls{cobyqa} with~$n = 5$, starting from the initial guess~$\iter[0] = [1.3, 0.7, 0.8, 1.9, 1.2]^{\T}$.

\begin{lstpython}[%
    caption=Solving the Rosenbrock problem using \gls{cobyqa},
    label=lst:cobyqa-rosenbrock,
]
    >>> from scipy.optimize import rosen
    >>> from cobyqa import minimize
    >>>
    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]  # starting point
    >>> res = minimize(rosen, x0)
    >>> res.x
    array([1., 1., 1., 1., 1.]) 
\end{lstpython}

To see how to supply to \gls{cobyqa} bound and linear constraints, consider the Example 16.4 of~\cite{Nocedal_Wright_2006}, defined as
\begin{align*}
    \min_{\iter \in \R^2}   & \quad (\iter_1 - 1)^2 + (\iter_2 - 2.5)^2\\
    \text{s.t.}             & \quad -\iter_1 + 2 \iter_2 \le 2,\\
                            & \quad \iter_1 + 2 \iter_2 \le 6,\\
                            & \quad \iter_1 - 2 \iter_2 \le 2,\\
                            & \quad \iter_1 \ge 0,\\
                            & \quad \iter_2 \ge 0.
\end{align*}
\Cref{lst:cobyqa-bound-linear} shows how to solve this problem using \gls{cobyqa} starting from the initial guess~$\iter[0] = [2, 0]^{\T}$.

\begin{lstpython}[%
    caption=An example of \gls{cobyqa} with linear constraints,
    label=lst:cobyqa-bound-linear,
]
    >>> from cobyqa import minimize
    >>>
    >>> def obj(x):
    ...     return (x[0] - 1.0) ** 2.0 + (x[1] - 2.5) ** 2.0
    >>>
    >>> x0 = [2.0, 0.0]
    >>> xl = [0.0, 0.0]
    >>> Aub = [[-1.0, 2.0], [1.0, 2.0], [1.0, -2.0]]
    >>> bub = [2.0, 6.0, 2.0]
    >>> res = minimize(quadratic, x0, xl=xl, Aub=Aub, bub=bub)
    >>> res.x
    array([1.4, 1.7])
\end{lstpython}

Finally, to see how to supply nonlinear constraints to \gls{cobyqa}, consider the Problem G of~\cite{Powell_1994}, defined by
\begin{align*}
    \min_{\iter \in \R^3}   & \quad \iter_3\\
    \text{s.t.}             & \quad -5 \iter_1 + \iter_2 - \iter_3 \le 0,\\
                            & \quad 5 \iter_1 + \iter_2 - \iter_3 \le 0,\\
                            & \quad \iter_1^2 + \iter_2^2 + 4 \iter_2 - \iter_3 \le 0.
\end{align*}
\Cref{lst:cobyqa-problem-g} shows how to solve such a problem using \gls{cobyqa}, starting from the initial guess~$\iter[0] = [1, 1, 1]^{\T}$.
Note that we can supply the linear constraints are nonlinear constraints.
However, in doing so, the code cannot detect that the constraint is linear, and hence, will build quadratic models of these constraints.

\pagebreak

\begin{lstpython}[%
    caption=An example of \gls{cobyqa} with nonlinear constraints,
    label=lst:cobyqa-problem-g,
]
    >>> from cobyqa import minimize
    >>>
    >>> def obj(x):
    ...     return x[2]
    >>>
    >>> def cub(x):
    ...     return x[0] ** 2.0 + x[1] ** 2.0 + 4.0 * x[1] - x[2] 
    >>>
    >>> x0 = [1.0, 1.0, 1.0]
    >>> Aub = [[-5.0, 1.0, -1.0], [5.0, 1.0, -1.0]]
    >>> bub = [0.0, 0.0]
    >>> res = minimize(obj, x0, Aub=Aub, bub=bub, cub=cub)
    >>> res.x
    array([ 0., -3., -3.])   
\end{lstpython}

\subsubsection{Details of the returned structure}

The function \texttt{minimize} returns a structure (that we named \texttt{res} in the examples above) whose attributes are detailed in \cref{tab:optimize-result}.

\begin{table}[ht]
    \caption{Structure for the result of the optimization algorithm}
    \label{tab:optimize-result}
    \centering
    \begin{tabularx}{\textwidth}{cX}
        \toprule
        \texttt{x}          & Solution point\\
        % \midrule
        % \texttt{success}    & Flag indicating whether \gls{cobyqa} terminated successfully\\
        \midrule
        \texttt{status}     & Termination status of the optimization solver reflecting which stopping criterion has been reached\\
        \midrule
        \texttt{message}    & Description of the termination status\\
        \midrule
        \texttt{fun}        & Value of the objective function at the solution point\\
        \midrule
        \texttt{jac}        & Approximate gradient of the objective function at the solution point\\
        \midrule
        \texttt{nfev}       & Number of objective and constraint function evaluations\\
        \midrule
        \texttt{nit}        & Number of iterations\\
        \midrule
        \texttt{maxcv}      & $\ell_{\infty}$-constraint violation at the solution point, if applicable\\
        \bottomrule
    \end{tabularx}
\end{table}

\section{Numerical experiments}
\label{sec:cobyqa-experiments}

To conclude this chapter, we present extensive numerical experiments for \gls{cobyqa}.
We will make comparisons using performance and data profiles obtained on CUTEst problems~\cite{Gould_Orban_Toint_2015}, with at most \num{50} variables and at most \num[group-minimum-digits=4]{1000} constraints, excluding bound constraints.
These problems are listed in \cref{sec:list-unconstrained-problems,sec:list-bound-constrained-problems,sec:list-linearly-constrained-problems,sec:list-nonlinearly-constrained-problems}, according to the types of their constraints.

As introduced in \cref{sec:benchmarking-tools}, the performance and data profiles require us to define a merit function.
% The merit function we consider is as follows.
Let~$v_{\infty}(\iter) \in \R$ be the~$\ell_{\infty}$-constraint violation at~$\iter \in \R^n$.
The merit function~$\varphi$ is defined by
\begin{empheq}[left={\varphi(\iter) = \empheqlbrace}]{alignat*=2}
    & \obj(\iter),                              && \quad \text{if~$v_{\infty}(\iter) \le \nu_1$,}\\
    & \infty,                                   && \quad \text{if~$v_{\infty}(\iter) \ge \nu_2$,}\\
    & \obj(\iter) + \gamma v_{\infty}(\iter),   && \quad \text{otherwise,}
\end{empheq}
for~$\iter \in \R^n$.
We choose in the experiments~$\nu_1 = 10^{-10}$,~$\nu_2 = 10^{-5}$, and~$\gamma = 10^5$.
Note that~$v_{\infty}$ is always zero for unconstrained problems.

For all the experiments, we present the performance and data profiles corresponding to~$\tau \in \set{10^{-1}, 10^{-3}, 10^{-5}, 10^{-7}}$ in the convergence test~\cref{eq:convergence-test-profiles}.
The starting points of the problems are set to the default ones provided in CUTEst.
The initial trust-region radius~$\rad[0]$ is set to one, and the final value~$\radlb[\infty]$ for the lower bound on the trust-region radius is set to~$10^{-6}$.
The maximal number of function evaluations is~$500n$, where~$n$ denotes the dimension of the problem being solved.
For the methods that employ underdetermined quadratic interpolation models, the number of interpolation points is set to~$2n + 1$.

\subsection{Testing the Hessian of the \glsfmtshort{sqp} subproblem}

Recall that the objective function of the \gls{sqp} subproblem takes a Hessian matrix that approximates~$\nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.
In \gls{cobyqa}, this Hessian is set to~$\nabla_{x, x}^2 \lagm[k](\iter[k], \lm[k])$, defined in \cref{subsec:cobyqa-framework}.
It is erroneous to set this Hessian to~$\nabla^2 \objm[k](\iter[k])$, unless there is no nonlinear constraints (see \cref{sec:sqp-method}).
To demonstrate this fact, we compare \gls{cobyqa} with a modification that sets this Hessian to~$\nabla^2 \objm[k](\iter[k])$.
% Although it is known that the resulting algorithm is invalid, making the comparison between the two methods is interesting because it reveals the importance of the strategy.

\Cref{fig:perf-wrong-hessian,fig:data-wrong-hessian} provide the performance and data profiles for these two methods, based on the nonlinearly constrained problems listed in \cref{sec:list-nonlinearly-constrained-problems}.
According to these profiles, \gls{cobyqa} performs much better than the modified method, which is expected.
It solves more problems, and uses less function evaluations than the modified method on most of the problems.
These results demonstrate the effectiveness of the \gls{sqp} methodology of \gls{cobyqa}.
It also shows that the estimated Lagrange multipliers we chose in \cref{subsec:least-squares-lagrange-multipliers} are reasonable.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-perf-cobyqa-wrong-hessian-qo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-perf-cobyqa-wrong-hessian-qo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-perf-cobyqa-wrong-hessian-qo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-perf-cobyqa-wrong-hessian-qo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles with erroneous Hessian]{Performance profiles of \gls{cobyqa} and the modified version with erroneous Hessian on nonlinearly constrained problems}
    \label{fig:perf-wrong-hessian}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-data-cobyqa-wrong-hessian-qo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-data-cobyqa-wrong-hessian-qo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-data-cobyqa-wrong-hessian-qo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","Erroneous-COBYQA"}}{plain-1-50-data-cobyqa-wrong-hessian-qo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles with erroneous Hessian]{Data profiles of \gls{cobyqa} and the modified version with erroneous Hessian on nonlinearly constrained problems}
    \label{fig:data-wrong-hessian}
\end{figure}

\subsection{Testing our Byrd-Omojokun approach for inequality constraints}
\label{subsec:perf-byrd-omojokun}

To handle inequality constraints, \gls{cobyqa} takes an extension of the Byrd-Omojokun that we devised in \cref{subsec:composite-step-inequality}.
This extension is different from the one presented in~\cite[\S~15.4.4]{Conn_Gould_Toint_2000}.
To demonstrate the advantage of our extension, we compare \gls{cobyqa} with a modified version that employs the Byrd-Omojokun approach in~\cite[\S~15.4.4]{Conn_Gould_Toint_2000}.

In this comparison, we consider only linearly and nonlinearly constrained problems with at least one inequality constraint.
The problems are taken from \cref{sec:list-linearly-constrained-problems,sec:list-nonlinearly-constrained-problems} accordingly.

\Cref{fig:perf-byrd-omojokun,fig:data-byrd-omojokun} presents the corresponding performance and data profiles of \gls{cobyqa} and the modified version specified above.
According to these profiles, our extension to the Byrd-Omojokun approach evidently outperforms the one presented in~\cite[\S~15.4.4]{Conn_Gould_Toint_2000}.
As mentioned in \cref{subsec:composite-step-inequality}, this can be explained by the fact that our Byrd-Omojokun approach provides a larger feasible region to the tangential steps.
Hence, a larger reduction in the objective function of the \gls{sqp} subproblem can be expected.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-perf-cobyqa-byrd-omojokun-nlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-perf-cobyqa-byrd-omojokun-nlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-perf-cobyqa-byrd-omojokun-nlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-perf-cobyqa-byrd-omojokun-nlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles of our new Byrd-Omojokun approach]{Performance profiles of \gls{cobyqa} and a modified version on  and nonlinearly constrained problems with at least one inequality constraint}
    \label{fig:perf-byrd-omojokun}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-data-cobyqa-byrd-omojokun-nlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-data-cobyqa-byrd-omojokun-nlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-data-cobyqa-byrd-omojokun-nlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","Modified-COBYQA"}}{plain-1-50-data-cobyqa-byrd-omojokun-nlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles of our new Byrd-Omojokun approach]{Data profiles of \gls{cobyqa} and a modified version on linearly and nonlinearly constrained problems with at least one inequality constraint}
    \label{fig:data-byrd-omojokun}
\end{figure}

\subsection{Performance of the derivative-free symmetric Broyden update}
\label{subsec:alternative-models}

Recall that \gls{cobyqa} employs quadratic models based on the derivative-free symmetric Broyden update (see \cref{subsec:symmetric-broyden-updates}).
Moreover, it includes a strategy to replace these models with the minimum Frobenius norm models when the method considers that the current models are inefficient, as detailed at the end of \cref{subsec:geometry-improvement}.

To justify our choice for the models, we now compare the performance of \gls{cobyqa} with two modifications.
The first one, referred to as \gls{cobyqa}~PSB, always uses the models based on the derivative-free symmetric Broyden update.
The second one, referred to as \gls{cobyqa}~MNH, always uses the minimum Frobenius norm models.

\Cref{fig:perf-models,fig:data-models} provide the performance and data profiles for these three methods, based on all the problems listed in \cref{sec:list-unconstrained-problems,sec:list-bound-constrained-problems,sec:list-linearly-constrained-problems,sec:list-nonlinearly-constrained-problems}.
These profiles show that \gls{cobyqa} outperforms \gls{cobyqa}~PSB, which in turn outperforms \gls{cobyqa}~MNH.
These results confirm that our choice for the quadratic models is reasonable.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-perf-cobyqa-models-ubnlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-perf-cobyqa-models-ubnlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-perf-cobyqa-models-ubnlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-perf-cobyqa-models-ubnlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles with different models]{Performance profiles of \gls{cobyqa}, \gls{cobyqa}~PSB, and \gls{cobyqa}~MNH on all problems}
    \label{fig:perf-models}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-data-cobyqa-models-ubnlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-data-cobyqa-models-ubnlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-data-cobyqa-models-ubnlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYQA-PSB","COBYQA-MNH"}}{plain-1-50-data-cobyqa-models-ubnlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles with different models]{Data profiles of \gls{cobyqa}, \gls{cobyqa}~PSB, and \gls{cobyqa}~MNH on all problems}
    \label{fig:data-models}
\end{figure}

% Powell proposed a sophisticated mechanism to replace the quadratic models obtained by the derivative-free symmetric Broyden update of \gls{lincoa} with the minimum Frobenius norm ones.
% We plan in the future to always try this mechanism, as it may provide even better performance for \gls{cobyqa}.

\subsection{Performance of \glsfmttext{cobyqa} on unconstrained problems}
\label{subsec:perf-cobyqa-unconstrained-problems}

In the remaining of this chapter, we compare the performance of \gls{cobyqa} with the Powell's \gls{dfo} methods that are available through \gls{pdfo}.
In all the comparisons, we include both \gls{cobyqa} and \gls{cobyla}, because both are capable of handling all types of problems, and because \gls{cobyqa} is expected to be a successor to \gls{cobyla}.

We start with the comparison of \gls{cobyqa}, \gls{newuoa}, and \gls{cobyla} on unconstrained problems.
\Cref{fig:perf-unconstrained-problems,fig:data-unconstrained-problems} present the performance and data profiles of these solvers, based on all the problems listed in \cref{sec:list-unconstrained-problems}.
These are all the unconstrained problems of the CUTEst library with at most \num{50} variables.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-newuoa-u.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-newuoa-u.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-newuoa-u.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-newuoa-u.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles on unconstrained problems]{Performance profiles of \gls{cobyqa}, \gls{newuoa}, and \gls{cobyla} on unconstrained problems}
    \label{fig:perf-unconstrained-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-newuoa-u.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-newuoa-u.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-newuoa-u.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","NEWUOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-newuoa-u.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles on unconstrained problems]{Data profiles of \gls{cobyqa}, \gls{newuoa}, and \gls{cobyla} on unconstrained problems}
    \label{fig:data-unconstrained-problems}
\end{figure}

We first observe that \gls{cobyqa} outperforms \gls{cobyla} in general, the margin being large for the tolerance~$\tau$ is low.
According to the performance profiles in \cref{fig:perf-unconstrained-problems}, with~$\tau = 10^{-1}$,~$10^{-3}$,~$10^{-5}$, and~$10^{-7}$, \gls{cobyqa} solves about \SI{98}{\percent}, \SI{90}{\percent}, \SI{85}{\percent}, and \SI{79}{\percent} of the problems, respectively, whereas the percentages for \gls{cobyla} are about \SI{89}{\percent}, \SI{67}{\percent}, \SI{50}{\percent}, and \SI{39}{\percent}.
Moreover, for~$\tau \in \set{10^{-3}, 10^{-5}, 10^{-7}}$, \gls{cobyqa} takes the least number of function evaluations to converge on about \SI{35}{\percent} of the problems, while the percentages for \gls{cobyla} are between \SI{15}{\percent} and \SI{30}{\percent}. 

However, for~$\tau = 10^{-1}$, \gls{cobyla} uses the smallest number of function evaluations to converge on about \SI{45}{\percent} of problems, while the percentage for \gls{cobyqa} is lower, being about \SI{25}{\percent}.
This is expected, because \gls{cobyla} necessitates~$n + 1$ function evaluations to build the initial model for an~$n$-dimensional problem, while \gls{cobyqa} requires~$2n + 1$.
Consequently, \gls{cobyla} takes less function evaluations to make progress at the early stage and hence, performs better for a low-precision comparison.

We also observe that \gls{cobyqa} and \gls{newuoa} perform comparably in this experiment, which is expected because they share very similar algorithms.
\Gls{newuoa} slightly outperforms \gls{cobyqa}, but the difference is marginal.
This is likely due to minor differences in the algorithms, in their implementations, and computer rounding errors.
In future releases of \gls{cobyqa}, we will try to close the tiny gap between \gls{newuoa} and \gls{cobyqa} by refining the implementation.

\subsection{Performance of \glsfmttext{cobyqa} on bound-constrained problems}

We now compare the performance of \gls{cobyqa}, \gls{bobyqa}, and \gls{cobyla} on bound-constrained problems.
\Cref{fig:perf-bound-constrained-problems,fig:data-bound-constrained-problems} present the performance and data profiles of these solvers, based on all the problems listed in \cref{sec:list-bound-constrained-problems}.
These are all the bound-constrained problems of the CUTEst library with at most \num{50} variables.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-perf-bobyqa-cobyla-cobyqa-b.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-perf-bobyqa-cobyla-cobyqa-b.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-perf-bobyqa-cobyla-cobyqa-b.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-perf-bobyqa-cobyla-cobyqa-b.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles on bound-constrained problems]{Performance profiles of \gls{cobyqa}, \gls{bobyqa}, and \gls{cobyla} on bound-constrained problems}
    \label{fig:perf-bound-constrained-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-data-bobyqa-cobyla-cobyqa-b.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-data-bobyqa-cobyla-cobyqa-b.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-data-bobyqa-cobyla-cobyqa-b.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","BOBYQA","COBYLA"}}{plain-1-50-data-bobyqa-cobyla-cobyqa-b.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles on bound-constrained problems]{Data profiles of \gls{cobyqa}, \gls{bobyqa}, and \gls{cobyla} on bound-constrained problems}
    \label{fig:data-bound-constrained-problems}
\end{figure}

The comparison of \gls{cobyqa} with \gls{cobyla} is quite similar to the one in the unconstrained case.
More specifically, for~$\tau \in \set{10^{-3}, 10^{-5}, 10^{-7}}$, \gls{cobyqa} outperforms \gls{cobyla} by a large margin.
For the tolerance~$\tau = 10^{-1}$, \gls{cobyqa} solves more problems than \gls{cobyla}, the percentages being about \SI{98}{\percent} and \SI{91}{\percent}.
However, for this low-precision comparison, according to the performance profiles in \cref{fig:perf-bound-constrained-problems}, \gls{cobyla} solves more problems than \gls{cobyqa} using the least number of function evaluations, the percentages being about \SI{53}{\percent} and \SI{35}{\percent}.
This is expected for the same reason as the unconstrained case.

Moreover, we observe that \gls{cobyqa} outperforms \gls{bobyqa} in general.
For the high-precision test with~$\tau = 10^{-7}$, the advantage of \gls{cobyqa} over \gls{bobyqa} is considerable.
When~$\tau = 10^{-3}$ and~$10^{-5}$, \gls{cobyqa} slightly outperforms \gls{bobyqa}.
Finally, for the tolerance~$\tau = 10^{-1}$, the two methods perform similarly.
It is interesting that \gls{cobyqa} outperforms \gls{bobyqa} on bound-constrained problems, because \gls{bobyqa} is particularly designed for this type of problems, while \gls{cobyqa} is a general-purpose solver.

\subsection{Performance of \glsfmttext{cobyqa} on linearly constrained problems}
\label{subsec:comparison-cobyqa-lincoa}

We now compare the performance of \gls{cobyqa}, \gls{lincoa}, and \gls{cobyla} on linearly constrained problems.
Recall that one of the critical features of \gls{cobyqa} is that is always respect bound constraints, if any.
This is important in many applications, because the objective function may be undefined when bounds are violated (see \cref{sec:simple-constraints}).
However, \gls{lincoa} and \gls{cobyla} do not always respect bound constraints.
To observe different behaviors of these methods with respect to bounds we consider two scenarios in the experiments.
In the first one, we allow the solvers to violate bound constraints, if any; the objective functions of the problems are well-defined even if bound constraints are violated.
In the second one, we assume that the bounds are inviolable, and we set the objective function values to~$\infty$ if any bound constraint is violated.
As will be shown, \gls{cobyqa} outperforms both \gls{lincoa} and \gls{cobyla} in the second scenario with large margins.

\subsubsection{Comparison when the bounds are violable}

\Cref{fig:perf-linearly-constrained-problems,fig:data-linearly-constrained-problems} present the performance and data profiles of these solvers, based on all the problems listed in \cref{sec:list-linearly-constrained-problems}.
These are all the linearly constrained problems of the CUTEst library with at most \num{50} variables and at most \num[group-minimum-digits=4]{1000} linear constraints.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles on linearly constrained problems]{Performance profiles of \gls{cobyqa}, \gls{lincoa}, and \gls{cobyla} on linearly constrained problems}
    \label{fig:perf-linearly-constrained-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles on linearly constrained problems]{Data profiles of \gls{cobyqa}, \gls{lincoa}, and \gls{cobyla} on linearly constrained problems}
    \label{fig:data-linearly-constrained-problems}
\end{figure}

\gls{cobyqa} outperforms \gls{cobyla} when~$\tau = 10^{-5}$ and~$10^{-7}$ with a fair margin.
They perform similarly when~$\tau = 10^{-3}$.
However, when~$\tau = 10^{-1}$, \gls{cobyla} solves more problems than both \gls{cobyqa} and \gls{lincoa} using the least number of function evaluations.
The advantage of \gls{cobyla} on this low-precision test is expected, as explained in \cref{subsec:perf-cobyqa-unconstrained-problems}.

However, \gls{cobyqa} is always outperformed by \gls{lincoa} for all values of~$\tau$.
Note that \gls{lincoa} is specifically designed for linearly constrained problems, while \gls{cobyqa} is a general-purpose solver.
On of the possible reasons for the advantage of \gls{lincoa} over \gls{cobyqa} is that it uses a more careful strategy to decide when to replace the default models with the minimum Frobenius norm ones.
In further releases of \gls{cobyqa}, we will investigate the differences between \gls{cobyqa} and \gls{lincoa}, and try to improve the performance of \gls{cobyqa} on linearly constrained problems.

\subsubsection{Comparison when the bounds are inviolable}

We now consider the experiment when the bound constraints are inviolable.
Among all the linearly constrained problems of the CUTEst library with at most \num{50} variables and at most \num[group-minimum-digits=4]{1000} linear constraints listed in \cref{sec:list-linearly-constrained-problems}, we test only those with at least one bound constraint.
% This experiment reflects more closely the behavior faced by the optimization solvers in many engineering and industrial applications.
\Cref{fig:perf-linearly-constrained-problems-bounds,fig:data-linearly-constrained-problems-bounds} present the performance and data profiles of \gls{cobyqa}, \gls{lincoa}, and \gls{cobyla} based on these problems.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-lincoa-nl-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles on linearly constrained problems with bounds]{Performance profiles of \gls{cobyqa}, \gls{lincoa}, and \gls{cobyla} on linearly constrained problems with inviolable bounds}
    \label{fig:perf-linearly-constrained-problems-bounds}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","LINCOA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-lincoa-nl-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles on linearly constrained problems with bounds]{Data profiles of \gls{cobyqa}, \gls{lincoa}, and \gls{cobyla} on linearly constrained problems with inviolable bounds}
    \label{fig:data-linearly-constrained-problems-bounds}
\end{figure}

On this experiment, \gls{cobyqa} outperforms \gls{lincoa} and \gls{cobyqa}, the margins being significant in the high-precision tests with~$\tau = 10^{-5}$ and~$10^{-7}$.
Generally speaking, \gls{cobyqa} solves more problems, and uses the least amount of function evaluations on most of them.
% This is important, because \gls{cobyqa} is designed to tackle problems for which the bound constraints cannot be violated.

It is noteworthy that \gls{cobyqa} performs much better than \gls{lincoa} on these linearly constrained problems with inviolable bounds, whereas the comparison is the opposite when the bounds are violable.
Consequently, we expect that \gls{cobyqa} is a better solver than \gls{lincoa} for applications that contain inviolable bound constraints.

\subsection{Performance of \glsfmttext{cobyqa} compared with \glsfmttext{cobyla}}

We now compare the performances of \gls{cobyqa} and \gls{cobyla}.
We conduct three experiments.
The first two experiments compare the methods on nonlinearly constrained problems, with violable and inviolable bounds, respectively.
In the second experiment, if a bound is violated, we set the objective function value to~$\infty$, leaving the constraint values untouched.
The last experiment compares \gls{cobyqa} and \gls{cobyla} on all types of problems, constrained and unconstrained.
As will be shown, \gls{cobyqa} outperforms \gls{cobyla} in all scenarios.

\subsubsection{Comparison on nonlinearly constrained problem when the bounds are violable}

\Cref{fig:perf-nonlinearly-constrained-problems,fig:data-nonlinearly-constrained-problems} present the performance and data profiles of these solvers, based on all the problems listed in \cref{sec:list-nonlinearly-constrained-problems}.
These are all the nonlinearly constrained problems of the CUTEst library with at most \num{50} variables and at most \num[group-minimum-digits=4]{1000} constraints, excluding bound constraints.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles on nonlinearly constrained problems]{Performance profiles of \gls{cobyqa} and \gls{cobyla} on nonlinearly constrained problems}
    \label{fig:perf-nonlinearly-constrained-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles on nonlinearly constrained problems]{Data profiles of \gls{cobyqa} and \gls{cobyla} on nonlinearly constrained problems}
    \label{fig:data-nonlinearly-constrained-problems}
\end{figure}

We observe that \gls{cobyqa} provides better performance than \gls{cobyla} for all considered tolerances.
More specifically, \gls{cobyqa} uses no more function evaluations than \gls{cobyla} to converge on about \SI{60}{\percent} of the problems.
% the percentages being between \SI{30}{\percent} and \SI{40}{\percent} for \gls{cobyla}.
They eventually solve a similar amount of problems, namely around \SI{80}{\percent}.
% However, to achieve the same performance than \gls{cobyqa}, \gls{cobyla} needs in average~$2^3 = 8$ times more function evaluations than \gls{cobyqa}.
% This is very satisfactory, since \gls{cobyqa} is designed as a successor to \gls{cobyla}.

\subsubsection{Comparison on nonlinearly constrained problem when the bounds are inviolable}

We now consider the experiment when the bound constraints are inviolable.
Among all the nonlinearly constrained problems of the CUTEst library with at most \num{50} variables and at most \num[group-minimum-digits=4]{1000} constraints, excluding bound constraints, listed in \cref{sec:list-nonlinearly-constrained-problems}, we test only those with at least one bound constraint.
% This experiment reflects more closely the behavior faced by the optimization solvers in many engineering and industrial applications.
\Cref{fig:perf-nonlinearly-constrained-problems-bounds,fig:data-nonlinearly-constrained-problems-bounds} present the performance and data profiles of \gls{cobyqa} and \gls{cobyla} based on these problems.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-qo-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles on nonlinearly constrained problems with bounds]{Performance profiles of \gls{cobyqa} and \gls{cobyla} on nonlinearly constrained problems with inviolable bounds}
    \label{fig:perf-nonlinearly-constrained-problems-bounds}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo-bounds.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo-bounds.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo-bounds.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-qo-bounds.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles on nonlinearly constrained problems with bounds]{Data profiles of \gls{cobyqa} and \gls{cobyla} on nonlinearly constrained problems with inviolable bounds}
    \label{fig:data-nonlinearly-constrained-problems-bounds}
\end{figure}

The results are similar to the ones in \cref{fig:perf-nonlinearly-constrained-problems,fig:data-nonlinearly-constrained-problems}, although the advantage of \gls{cobyqa} over \gls{cobyla} becomes even more visible.
This is also expected, because \gls{cobyqa} is designed to always respect the bound constraints, while \gls{cobyla} is not.

\subsubsection{Comparison on all types of problems}

One of the motivations for designing \gls{cobyqa} is to provide a successor for \gls{cobyla}.
To demonstrate that it is reasonable to replace \gls{cobyla} with \gls{cobyqa} in general, we compare their performances on all types of problems, constrained and unconstrained.
\Cref{fig:perf-all-problems,fig:data-all-problems} provide the performance and data profiles on all the CUTEst problems of dimension at most \num{50} listed in \cref{sec:list-unconstrained-problems,sec:list-bound-constrained-problems,sec:list-linearly-constrained-problems,sec:list-nonlinearly-constrained-problems}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-ubnlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-ubnlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-ubnlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-ubnlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Performance profiles on all problems]{Performance profiles of \gls{cobyqa} and \gls{cobyla} on all types of problems}
    \label{fig:perf-all-problems}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-ubnlqo.csv}{1}
        \caption{Tolerance~$\tau = 10^{-1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-ubnlqo.csv}{3}
        \caption{Tolerance~$\tau = 10^{-3}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-ubnlqo.csv}{5}
        \caption{Tolerance~$\tau = 10^{-5}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-ubnlqo.csv}{7}
        \caption{Tolerance~$\tau = 10^{-7}$}
    \end{subfigure}
    \caption[Data profiles on all problems]{Data profiles of \gls{cobyqa} and \gls{cobyla} on all types of problems}
    \label{fig:data-all-problems}
\end{figure}

The superiority of \gls{cobyqa} over \gls{cobyla} is demonstrated evidently by these profiles.
\Gls{cobyqa} outperforms \gls{cobyla} with a substantial margin, and the margin is larger when the precision of the comparison is higher.
When~$\tau = 10^{-3}$,~$10^{-5}$, and~$10^{-7}$, \gls{cobyqa} solves much more problems than \gls{cobyla}, while taking less function evaluations on most of the problems solved.
These results are not surprising, because \gls{cobyla} uses linear models of the objective and constraint functions, whereas \gls{cobyqa} uses quadratic models, which approximate these functions better in general.

We also conducted a similar experiment when the bound constraints are inviolable.
The results are similar to those presented in \cref{fig:perf-all-problems,fig:data-all-problems} and hence omitted, although the margins are even larger, as expected.

To conclude, \gls{cobyqa} performs convincingly better than \gls{cobyla}, and hence, is a great successor for \gls{cobyla} as a general-purpose solver.

% We now make the numerical experiment that reflects the most the reality for which \gls{cobyqa} is designed.
% We compare \gls{cobyqa} with \gls{cobyla} on all the problems that admit at least one bound constraint, setting their objective functions to~$\infty$ outside of the bounds.
% The performance and data profiles are provided in \cref{fig:perf-all-problems-bounds,fig:data-all-problems-bounds}, respectively.

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-bnlqo-bounds.csv}{1}
%         \caption{Tolerance~$\tau = 10^{-1}$}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-bnlqo-bounds.csv}{3}
%         \caption{Tolerance~$\tau = 10^{-3}$}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-bnlqo-bounds.csv}{5}
%         \caption{Tolerance~$\tau = 10^{-5}$}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawperformanceprofiles{{"COBYQA","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-bnlqo-bounds.csv}{7}
%         \caption{Tolerance~$\tau = 10^{-7}$}
%     \end{subfigure}
%     \caption{Performance profiles on all problems with bounds}
%     \label{fig:perf-all-problems-bounds}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-bnlqo-bounds.csv}{1}
%         \caption{Tolerance~$\tau = 10^{-1}$}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-bnlqo-bounds.csv}{3}
%         \caption{Tolerance~$\tau = 10^{-3}$}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-bnlqo-bounds.csv}{5}
%         \caption{Tolerance~$\tau = 10^{-5}$}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawdataprofiles{{"COBYQA","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-bnlqo-bounds.csv}{7}
%         \caption{Tolerance~$\tau = 10^{-7}$}
%     \end{subfigure}
%     \caption{Data profiles on all problems with bounds}
%     \label{fig:data-all-problems-bounds}
% \end{figure}

% Therefore, \gls{cobyqa} is a great successor for \gls{cobyla}.

% \subsection{Advantage of quadratic models over linear models}

% To conclude this section, we make a last numerical experiments that compares the performance of \gls{cobyqa} for different size of the interpolation set.
% More particularly, we compare \gls{cobyqa} using different size of the interpolation, the default~$m = 2n + 1$ and the least possible one~$m = n + 2$.
% We also compare these two variations with \gls{cobyla}.
% When using~$m = n + 2$, the initial models are almost linear, and only one coefficient of the Hessian matrix may be nonzero.
% The experiment select all the available problems with a dimension at most \num{50} and a number of constraints at most \num[group-minimum-digits=4]{1000}.
% The performance and data profiles are provided in \cref{fig:perf-interpolation-sets,fig:data-interpolation-sets}, respectively.

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawperformanceprofiles{{"COBYQA-$m=2n+1$","COBYQA-$m=n+2$","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-np2-ubnlqo.csv}{1}
%         \caption{Tolerance~$\tau = 10^{-1}$}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawperformanceprofiles{{"COBYQA-$m=2n+1$","COBYQA-$m=n+2$","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-np2-ubnlqo.csv}{3}
%         \caption{Tolerance~$\tau = 10^{-3}$}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawperformanceprofiles{{"COBYQA-$m=2n+1$","COBYQA-$m=n+2$","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-np2-ubnlqo.csv}{5}
%         \caption{Tolerance~$\tau = 10^{-5}$}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawperformanceprofiles{{"COBYQA-$m=2n+1$","COBYQA-$m=n+2$","COBYLA"}}{plain-1-50-perf-cobyla-cobyqa-np2-ubnlqo.csv}{7}
%         \caption{Tolerance~$\tau = 10^{-7}$}
%     \end{subfigure}
%     \caption{Performance profiles on different interpolation sets}
%     \label{fig:perf-interpolation-sets}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawdataprofiles{{"COBYQA-$m=2n+1$","COBYQA-$m=n+2$","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-np2-ubnlqo.csv}{1}
%         \caption{Tolerance~$\tau = 10^{-1}$}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawdataprofiles{{"COBYQA-$m=2n+1$","COBYQA-$m=n+2$","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-np2-ubnlqo.csv}{3}
%         \caption{Tolerance~$\tau = 10^{-3}$}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawdataprofiles{{"COBYQA-$m=2n+1$","COBYQA-$m=n+2$","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-np2-ubnlqo.csv}{5}
%         \caption{Tolerance~$\tau = 10^{-5}$}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \drawdataprofiles{{"COBYQA-$m=2n+1$","COBYQA-$m=n+2$","COBYLA"}}{plain-1-50-data-cobyla-cobyqa-np2-ubnlqo.csv}{7}
%         \caption{Tolerance~$\tau = 10^{-7}$}
%     \end{subfigure}
%     \caption{Data profiles on different interpolation sets}
%     \label{fig:data-interpolation-sets}
% \end{figure}

% We observe a perfectly excepted result.
% For the highest tolerance, \gls{cobyla} and the version of \gls{cobyqa} with~$m = n + 2$ solve more problems using the least amount of function evaluations than \gls{cobyqa} with~$m = 2n + 1$.
% This is because they require the least number of evaluations for building their initial models.
% However, for lower tolerance, \gls{cobyqa} with~$m = 2n + 1$ always outperforms \gls{cobyqa} with~$m = n + 2$, which itself outperforms \gls{cobyla}.
% Recall that \gls{cobyla} necessitate~$m = n + 1$ to build its initial interpolation models.
% Therefore, \gls{cobyqa} with~$m = n + 2$ only has a single interpolation point more than \gls{cobyla}, but has significantly better performance.

\section{Summary and remarks}

In this chapter, we presented the Python implementation of \gls{cobyqa}, and extensive numerical experiments.

We first provided additional algorithmic details, which were omitted in the general presentation of \gls{cobyqa} in \cref{ch:cobyqa-introduction,ch:cobyqa-subproblems}.

We then detailed the Python implementation of \gls{cobyqa}, which is available at \url{https://www.cobyqa.com/}.
We briefly introduced some programming aspects of the implementation, provided an overview of the signature of the main Python function, and illustrated the usage by several simple examples.

Finally, we presented extensive numerical experiments on \gls{cobyqa}.
First, we demonstrated the effectiveness of several critical strategies of \gls{cobyqa}, including the \gls{sqp} methodology, our extension to the Byrd-Omojokun approach in the inequality case, and the derivative-free symmetric Broyden update of Powell.
Second, we tested the performance of \gls{cobyqa} compared with Powell's \gls{dfo} solvers, which are standard benchmarks for the assessment of \gls{dfo} methods.
The performance of \gls{cobyqa} is comparable to \gls{newuoa} on unconstrained problems, and it outperforms \gls{bobyqa} on bound-constrained ones, while also being able to tackle more general problems.
In contrast to \gls{lincoa} and \gls{cobyla}, a strength of \gls{cobyqa} is that it always respects bound constraints.
On linearly constrained problems, \gls{cobyqa} clearly outperforms \gls{lincoa} if the problems contain inviolable bound constraints.
Most importantly, \gls{cobyqa} has much better performance than \gls{cobyla} on all types of problems, constrained and unconstrained, no matter whether bound constraints (if any) can be violated or not.

The implementation of \gls{cobyqa} has been highly challenging, as is the case for all model-based \gls{dfo} solvers\footnote{Recall that \citeauthor{Powell_2006}~\cite{Powell_2006} mentioned that the development of \gls{newuoa} was time-consuming, and \enquote{was very frustrating,} so was the implementation of \gls{cobyqa}.}.
We endeavored to develop \gls{cobyqa} in order to provide a successor to \gls{cobyla} as a general-purpose \gls{dfo} solver.
Finally, the numerical experiments have demonstrated that our goal is achieved.
