%% contents/cobyqa-subproblems.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{\glsfmttext{cobyqa} \textemdash\ solving the subproblems}
\label{ch:cobyqa-subproblems}

This chapter introduces the methods employed by \gls{cobyqa} to solve its various subproblems.
We first present the classical Steihaug-Toint \glsxtrfull{tcg} method in \cref{sec:tcg}.
We then introduce in \cref{sec:cobyqa-tangential,sec:cobyqa-normal} active-set variations of the \gls{tcg} method to solve the tangential subproblem~\cref{eq:cobyqa-tangential} and the normal subproblem~\cref{eq:cobyqa-normal}, respectively.
\Cref{sec:cobyqa-lagrange-multipliers} adapts the \gls{nnls} algorithm~\cite[Alg.~23.10]{Lawson_Hanson_1987} to solve~\cref{eq:least-squares-lagrange-multipliers-cobyqa} for estimating the Lagrange multiplier in \gls{cobyqa}.
Finally, \cref{sec:cobyqa-geometry-improving} details the procedure that \gls{cobyqa} employs to solve the geometry-improving subproblem~\cref{eq:geometry-subproblem}.

\section{The \glsfmtlong{tcg} (\glsfmtshort{tcg}) method}
\label{sec:tcg}

The classical trust-region subproblem for unconstrained optimization is of the form
\begin{subequations}
    \label{eq:problem-tcg}
    \begin{align}
        \min_{\sstep \in \R^n}  & \quad Q(\sstep)\\
        \text{s.t.}             & \quad \norm{\sstep} \le \rad,
    \end{align}
\end{subequations}
with~$Q \in \qpoly$ being the trust-region model and~$\rad > 0$ being the trust-region radius.
The usual convergence results for trust-region methods do not require us to solve~\cref{eq:problem-tcg} exactly.
Rather, we only need to find an inexact solution~$\sstep[\ast]$ that satisfies
\begin{equation*}
    Q(0) - Q(\sstep[\ast]) \ge c \norm{\nabla Q(0)} \min @@ \set[\bigg]{\frac{\norm{\nabla Q(0)}}{\norm{\nabla^2 Q}}, \rad},
\end{equation*}
for some~$c > 0$, where we assume that~$\norm{\nabla Q(0)} / \norm{\nabla^2 Q} = \infty$ if~$\nabla^2 Q = 0$.
This is achieved for example by the Cauchy step, i.e., the step that minimizes~$Q$ along~$-\nabla Q(0)$, subject to the trust-region constraint~\cite[Thm.~4]{Powell_1975b}.

% Therefore, in trust-region method, we solve~\cref{eq:problem-tcg} approximately.
\index{TCG@\glsfmtshort{tcg}|(}A well-known method for finding such an inexact solution to~\cref{eq:problem-tcg} is the Steihaug-Toint \gls{tcg} method~\cite{Steihaug_1983,Toint_1981}, presented in \cref{alg:tcg}.
Briefly speaking, this algorithm entertains conjugate gradients iterations, and stops the computations if a step reaches the boundary of the trust region.

\begin{algorithm}
    \caption{Steihaug-Toint \glsfmtshort{tcg} method}
    \label{alg:tcg}
    \DontPrintSemicolon
    \onehalfspacing
    \KwData{Quadratic function~$Q$ and trust-region radius~$\rad > 0$.}
    \KwResult{An approximate solution to~\cref{eq:problem-tcg}.}
    Set~$\sstep[0] \gets 0$\;
    Set the search direction~$\pstep[0] \gets -\nabla Q(\sstep[0])$\;
    \For{$k = 0, 1, \dots$ until~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$}{ \nllabel{alg:tcg-stop}
        % Set
        % \begin{algoempheq}[left={\alpha_Q^k \gets \empheqlbrace}]{alignat*=2}
        %     & \frac{- \nabla Q(\sstep[k])^{\T} \pstep[k]}{(\pstep[k])^{\T} (\nabla^2 Q) \pstep[k]}  && \quad \text{if~$(\pstep[k])^{\T} (\nabla^2 Q) \pstep[k] > 0$,}\\
        %     & \infty                                                                                && \quad \text{otherwise}
        % \end{algoempheq} \nllabel{alg:tcg-alphaq}
        Set~$\alpha_Q^k \gets \argmin @@ \set{Q(\sstep[k] + \alpha \pstep[k]) : \alpha \ge 0}$\; \nllabel{alg:tcg-alphaq}
        Set~$\alpha_{\rad}^k \gets \argmax @@ \set{\alpha \ge 0 : \norm{\sstep[k] + \alpha \pstep[k]} \le \rad}$\nomenclature[Op]{$\argmax$}{Global maximizer operator}\; \nllabel{alg:tcg-alphad}
        Set the step size~$\alpha^k \gets \min @@ \set{\alpha_Q^k, \alpha_{\rad}^k}$\;
        Update the iterate~$\sstep[k + 1] \gets \sstep[k] + \alpha^k \pstep[k]$\;
        \eIf{$\alpha^k < \alpha_{\rad}^k$}{
            Set~$\beta^k \gets \norm{\nabla Q(\sstep[k + 1])}^2 / \norm{\nabla Q(\sstep[k])}^2$\;
            Update~$\pstep[k + 1] \gets -\nabla Q(\sstep[k + 1]) + \beta^k \pstep[k]$\;
        }{
            Break\; \nllabel{alg:tcg-break}
        }
    }
\end{algorithm}

In practice,~$\alpha_{\rad}^k$ in \cref{alg:tcg-alphad} of \cref{alg:tcg} is obtained by solving the quadratic equation
\begin{equation*}
    \norm{\sstep[k] + \alpha \pstep[k]}^2 = \rad^2,
\end{equation*}
which provides
\begin{equation}
    \label{eq:tcg-alphad}
    \alpha_{\rad}^k =\frac{\sqrt{\big[ (\sstep[k])^{\T}\pstep[k]]^2 + \norm{\pstep[k]}^2(\rad^2 - \norm{\sstep[k]}^2)} - (\sstep[k])^\T\pstep[k]}{\norm{\pstep[k]}^2}.
\end{equation}
For~$\alpha_Q^k$ in \cref{alg:tcg-alphaq}, we have the explicit formula
\begin{subequations}
    \label{eq:tcg-alphaq}
    \begin{empheq}[left={\alpha_Q^k = \empheqlbrace}]{alignat=2}
        & \frac{- \nabla Q(\sstep[k])^{\T} \pstep[k]}{(\pstep[k])^{\T} (\nabla^2 Q) \pstep[k]}, && \quad \text{if~$(\pstep[k])^{\T} (\nabla^2 Q) \pstep[k] > 0$,} \label{eq:tcg-alphaq-1}\\
        & \infty,                                                                               && \quad \text{otherwise,}
    \end{empheq}
\end{subequations}
which is valid due to the stopping criterion~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$ in \cref{alg:tcg-stop} of the algorithm.
As detailed in \cref{prop:tcg}, this stopping criterion is indeed equivalent to~$\norm{\pstep[k]} = 0$, which is usually used when presenting the \gls{tcg} algorithm.

\begin{proposition}
    \label{prop:tcg}
    In \cref{alg:tcg-stop} of \cref{alg:tcg}, the stopping criterion~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$ is satisfied if and only if~$\norm{\pstep[k]} = 0$.
\end{proposition}

\begin{proof}
    It is obvious that~$\norm{\pstep[k]} = 0$ implies~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$.
    For the converse, we assume without loss of generality that~$k \ge 1$.
    When the stopping criterion in \cref{alg:tcg-stop} is tested, we must have~$\alpha^{k - 1} = \alpha_Q^{k - 1}$, and~$\alpha_Q^{k - 1}$ is evaluated by~\cref{eq:tcg-alphaq-1}; otherwise,~$\alpha^{k - 1}$ would equal~$\alpha_{\rad}^{k - 1}$, and the algorithm would have stopped in \cref{alg:tcg-break}.
    By~\cref{eq:tcg-alphaq-1} and the fact that~$\sstep[k] = \sstep[k - 1] + \alpha_Q^{k - 1} \pstep[k - 1]$, we then have~$\nabla Q(\sstep[k])^{\T} \pstep[k - 1] = 0$.
    Therefore, due to the definition of~$\pstep[k]$, we see that~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$ implies~$\nabla Q(\sstep[k]) = 0$, which further leads to~$\beta^{k - 1} = 0$, and hence~$\norm{\pstep[k]} = 0$.
\end{proof}

The \gls{tcg} algorithm enjoys many good properties.
In particular, \citeauthor{Yuan_2000}~\cite{Yuan_2000} showed that the \gls{tcg} step provides at least half the reduction provided by the exact solution to~\cref{eq:problem-tcg} if~$\nabla^2 Q$ is positive definite.
The tangential and normal subproblem solvers employed by \gls{cobyqa} are constrained variations of the \gls{tcg} method,  which will be presented in \cref{sec:cobyqa-tangential,sec:cobyqa-normal}.\index{TCG@\glsfmtshort{tcg}|)}

\section{Solving the tangential subproblem}
\label{sec:cobyqa-tangential}

We now present the constrained variations of the \gls{tcg} method we use in \gls{cobyqa} to solve the tangential subproblem~\cref{eq:cobyqa-tangential}.
We introduce two variations, one for the bound-constrained case (when~$\iub \cup \ieq = \emptyset$), and the other for the linearly constrained case (when~$\iub \cup \ieq \neq \emptyset$).

\subsection{Bound-constrained case}

\index{TCG@\glsfmtshort{tcg}!bound-constrained|(}We introduce here the bound-constrained variation of the \gls{tcg} method designed by \citeauthor{Powell_2009} for his solver \gls{bobyqa}~\cite{Powell_2009}.
The method is described in~\cite[\S~3]{Powell_2009}, and we present it in \cref{alg:bvtcg} for ease of reference.
It is an active-set variation of the \gls{tcg} method in \cref{alg:tcg}.

This method is employed by \gls{cobyqa} to solve the tangential subproblem~\cref{eq:cobyqa-tangential} when only bound constraints exist.
In such a case,~\cref{eq:cobyqa-tangential} is of the form
\begin{subequations}
    \label{eq:problem-bvtcg}
    \begin{align}
        \min_{\sstep \in \R^n}  & \quad Q(\sstep) \label{eq:problem-bvtcg-obj}\\
        \text{s.t.}             & \quad \xl \le \sstep \le \xu,\\
                                & \quad \norm{\sstep} \le \rad,
    \end{align}
\end{subequations}
where the objective function~$Q$ is quadratic, the lower bound~$\xl \in (\R \cup \set{-\infty})^n$ and the upper bound~$\xu \in (\R \cup \set{\infty})^n$ satisfy~$\xl \le 0$,~$\xu \ge 0$, and~$\xl < \xu$.
Note that the lower bound~$\xl$ in~\cref{eq:problem-bvtcg} indeed corresponds to the bound~$\xl - \iter[k]$ in~\cref{eq:cobyqa-tangential}, but we abuse the notation~$\xl$ for simplicity.
The situation for~$\xu$ is similar.

\subsubsection{Description of the working set}

The method maintains a working set of active bounds.
At the~$k$th iteration, if a bound constraint is in the working set, then the corresponding coordinate of~$\sstep[k]$ is fixed at this bound, and it will not be changed in the subsequent iterations.
In the subspace of the coordinates that are not fixed, a \gls{tcg} step is taken.
If a new bound is hit by the \gls{tcg} step, the bound is added to the working set, and the procedure is restarted.
The working set is only enlarged through the iterations, and hence, the restarting happens only finitely many times.

The initial working set is a subset of the active bounds at the starting point~$\sstep[0] = 0$, which is the trust-region center.
More specifically, the initial working set is
\begin{equation}
    \label{eq:bounds-initial-working-set}
    \mathcal{W}^0 \eqdef \set[\bigg]{i \in \set{1, \dots, n} : \xl_i = \sstep[0]_i ~ \text{and} ~ \frac{\partial Q}{\partial \sstep_i}(\sstep[0]) \ge 0, ~ \text{or} ~ \xu_i = \sstep[0]_i
~ \text{and} ~ \frac{\partial Q}{\partial \sstep_i}(\sstep[0]) \le 0},
\end{equation}
which excludes the active bounds such that a step along~$-\nabla Q(\sstep[0])$ would depart from the bounds.

\subsubsection{Description of the \glsfmtshort{tcg} method}

\Cref{alg:bvtcg} details the \gls{tcg} method.
Given a vector~$z \in \R^n$, we use~$\Pi^k(z) \in \R^n$ to denote the vector whose~$i$th component is~$z_i$ if~$i \notin \mathcal{W}^k$, and zero otherwise.

\begin{algorithm}
    \caption[Bound-constrained \glsfmtshort{tcg} method]{Bound-constrained \glsfmtshort{tcg} method~\cite[\S~3]{Powell_2009}}
    \label{alg:bvtcg}
    \DontPrintSemicolon
    \onehalfspacing
    \KwData{Quadratic function~$Q$, bounds~$\xl < \xu$, and trust-region radius~$\rad > 0$.}
    \KwResult{An approximate solution to~\cref{eq:problem-bvtcg}.}
    Set~$\sstep[0] \gets 0$ and~$\mathcal{W}^0$ according to~\cref{eq:bounds-initial-working-set}\;
    Set the search direction~$\pstep[0] \gets -\Pi^0(\nabla Q(\sstep[0]))$\;
    \For{$k = 0, 1, \dots$ until~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$}{ \nllabel{alg:bvtcg-stop}
        Set~$\alpha_Q^k \gets \argmin @@ \set{Q(\sstep[k] + \alpha \pstep[k]) : \alpha \ge 0}$ using~\cref{eq:tcg-alphaq}\;
        % Set
        % \begin{algoempheq}[left={\alpha_Q^k \gets \empheqlbrace}]{alignat*=2}
        %     & \frac{- \nabla Q(\sstep[k])^{\T} \pstep[k]}{(\pstep[k])^{\T} (\nabla^2 Q) \pstep[k]}  && \quad \text{if~$(\pstep[k])^{\T} (\nabla^2 Q) \pstep[k] > 0$,}\\
        %     & \infty                                                                                && \quad \text{otherwise}
        % \end{algoempheq}
        Set~$\alpha_{\rad}^k \gets \argmax @@ \set{\alpha \ge 0 : \norm{\sstep[k] + \alpha \pstep[k]} \le \rad}$ using~\cref{eq:tcg-alphad}\;
        Set~$\alpha_B^k \gets \argmax @@ \set{\alpha \ge 0 : \xl \le \sstep[k] + \alpha \pstep[k] \le \xu}$\; \nllabel{alg:bvtcg-alphab}
        Set the step size~$\alpha^k \gets \min @@ \set{\alpha_Q^k, \alpha_{\rad}^k, \alpha_B^k}$\;
        Update the iterate~$\sstep[k + 1] \gets \sstep[k] + \alpha^k \pstep[k]$\;
        % \uIf{$\alpha^k = \alpha_{\rad}^k$}{
        %     Break\;
        % }
        % \uElseIf{$\alpha^k = \alpha_B^k$}{
        %     Add the first bound hit by~$\sstep[k + 1]$ to~$\mathcal{W}^k$, obtaining~$\mathcal{W}^{k + 1}$\;
        %     Set~$\pstep[k + 1] \gets -\Pi^{k + 1}(\nabla Q(\sstep[k + 1]))$\;
        % }
        % \Else{
        %     Set~$\mathcal{W}^{k + 1} \gets \mathcal{W}^k$\;
        %     Set~$\beta^k \gets \norm{\Pi^{k + 1}(\nabla Q(\sstep[k + 1]))}^2 / \norm{\Pi^{k + 1}(\nabla Q(\sstep[k]))}^2$\;
        %     Update~$\pstep[k + 1] \gets -\Pi^{k + 1}(\nabla Q(\sstep[k])) + \beta^k \pstep[k]$\;
        % }
        \uIf{$\alpha^k < \min \set {\alpha_{\rad}^k, \alpha_B^k}$}{
            Set~$\mathcal{W}^{k + 1} \gets \mathcal{W}^k$\;
            Set~$\beta^k \gets \norm{\Pi^{k + 1}(\nabla Q(\sstep[k + 1]))}^2 / \norm{\Pi^{k + 1}(\nabla Q(\sstep[k]))}^2$\;
            Update~$\pstep[k + 1] \gets -\Pi^{k + 1}(\nabla Q(\sstep[k + 1])) + \beta^k \pstep[k]$\; \nllabel{alg:bvtcg-tcg-step}
        }
        \uElseIf{$\alpha^k < \alpha_{\rad}^k$}{
            Add the first bound hit by~$\sstep[k + 1]$ to~$\mathcal{W}^k$, obtaining~$\mathcal{W}^{k + 1}$\;
            Set~$\pstep[k + 1] \gets -\Pi^{k + 1}(\nabla Q(\sstep[k + 1]))$\; \nllabel{alg:bvtcg-restart}
        }
        \Else{
            Break\;
        }
    }
\end{algorithm}

Note that~$\alpha_B^k$ in \cref{alg:bvtcg-alphab} of \cref{alg:bvtcg} is~$\infty$ if all points in~$\set{\sstep[k] + \alpha \pstep[k] : \alpha \ge 0}$ lie between the bounds~$\xl$ and~$\xu$.

\subsubsection{Stopping criteria}

Similar to \cref{prop:tcg}, we observe the following for the stopping criterion in \cref{alg:bvtcg-stop} of \cref{alg:bvtcg}.

\begin{proposition}
    \label{prop:bvtcg}
    In \cref{alg:bvtcg-stop} of~\cref{alg:bvtcg}, the stopping criterion~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$ is satisfied if and only if~$\norm{\pstep[k]} = 0$.
\end{proposition}

\begin{proof}
    It is obvious that~$\norm{\pstep[k]} = 0$ implies~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$.
    Suppose that~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$ in \cref{alg:bvtcg-stop} of~\cref{alg:bvtcg}, and assume without loss of generality that~$k \ge 1$.
    If \cref{alg:bvtcg-stop} is reached from \cref{alg:bvtcg-restart}, then
    \begin{equation*}
        \norm{\pstep[k]}^2 = \norm{\Pi^k(\nabla Q(\sstep[k]))}^2 =  Q (\sstep[k])^{\T}  \Pi^k(\nabla Q(\sstep[k])) = - Q (\sstep[k])^{\T}  \pstep[k] \le 0,
    \end{equation*}
    and hence~$\norm{\pstep[k]} = 0$.
    If \cref{alg:bvtcg-stop} is reached from \cref{alg:bvtcg-tcg-step}, then we can establish~$\norm{\pstep[k]} = 0$ by an argument similar to the proof of \cref{prop:tcg}.
\end{proof}

\Citeauthor{Powell_2009}~\cite[\S~3]{Powell_2009} proposed two additional stopping criteria for the
\gls{tcg} algorithm, in order to avoid unworthy computations.
\Cref{alg:bvtcg} omits them for simplicity, and we specify them now.
First, the algorithm is stopped if the move along~$\pstep[k]$ is expected to give a relatively small reduction in~$Q$, namely if
\begin{equation}
    \label{eq:bvtcg-stop1}
    \norm[\big]{\Pi^k(\nabla Q(\sstep[k]))} \rad \le \nu [Q(\sstep[0]) - Q(\sstep[k])]
\end{equation}
for some constant~$\nu > 0$.
The second additional stopping criterion is not mentioned in~\cite{Powell_2009} but implemented in
\gls{bobyqa}, where the algorithm is also terminated if the reduction in~$Q$ provided by the recent iteration is tiny compared to the reduction so far, that is if
\begin{equation}
    \label{eq:bvtcg-stop2}
    Q(\sstep[k]) - Q(\sstep[k + 1]) \le \nu [Q(\sstep[0]) - Q(\sstep[k + 1])].
\end{equation}
\Gls{cobyqa} includes both of the stopping criteria with~$\nu = 0.01$ when implementing~\cref{alg:bvtcg}.\index{TCG@\glsfmtshort{tcg}!bound-constrained|)}

% The linearly constrained \gls{tcg} procedure with these additional stopping criteria is implemented in \gls{cobyqa} under the function \texttt{cobyqa.linalg.lctcg}.

\subsubsection{Moving round the trust-region boundary}

\Gls{cobyqa} employs an adapted version of \cref{alg:bvtcg} for solving its tangential subproblem~\cref{eq:cobyqa-tangential} if~$\iub \cup \ieq = \emptyset$.
The adaptation, also proposed by \citeauthor{Powell_2009}~\cite[\S~3]{Powell_2009} and implemented in \gls{bobyqa}, is based on the following observation.
Let~$\sstep[\ast]$ be the point returned by \cref{alg:bvtcg}.
If~$\sstep[\ast]$ satisfies~$\norm{\sstep[\ast]} = \rad$, it is likely that the objective function in~\cref{eq:problem-bvtcg-obj} can be further decreased by moving this point round the trust-region boundary.
% In fact, the global solution of~\cref{eq:problem-bvtcg} is on the trust-region boundary in such a case.
The method employed by \gls{cobyqa} then attempts to further reduce the objective function by refining~$\sstep[\ast]$ with a step that approximately solves
\begin{subequations}
    \label{eq:cobyqa-tangential-bounds-alternative}
    \begin{align}
        \min_{\sstep \in \R^n}  & \quad Q(\sstep[\ast] + \sstep)\\
        \text{s.t.}             & \quad \xl \le \sstep[\ast] + \sstep \le \xu,\\
                                & \quad \norm{\sstep[\ast] + \sstep} = \rad,\\
                                & \quad \sstep \in \vspan @@ \set{\Pi^{\ast}(\sstep[\ast]), \Pi^{\ast}(\nabla Q(\sstep[\ast]))},
    \end{align}
\end{subequations}
where~$\Pi^{\ast}$ corresponds initially to the operator~$\Pi^k$ at the last iteration of \cref{alg:bvtcg}.
Note that a substantial reduction in~$Q$ is unlikely if either~$\Pi^{\ast}(\nabla Q(\sstep[\ast]))$ or the angle between~$\Pi^{\ast}(\sstep[\ast])$ and~$-\Pi^{\ast}(\nabla Q(\sstep[\ast]))$ is tiny.
Therefore, the method designed by \citeauthor{Powell_2009} attempts to refine~$\sstep[\ast]$ round the trust-region boundary only if
\begin{equation*}
    \norm{\Pi^{\ast}(\sstep[\ast])}^2 \norm{\Pi^{\ast}(\nabla Q(\sstep[\ast]))}^2 - [\Pi^{\ast}(\sstep[\ast])^{\T} \Pi^{\ast}(\nabla Q(\sstep[\ast]))]^2 > \xi [Q(\sstep[0]) - Q(\sstep[\ast])]^2,
\end{equation*}
for some~$\xi \in (0, 1)$.
In \gls{cobyqa}, we choose~$\xi = 10^{-4}$.
When this refinement is entertained, the method builds an orthogonal basis~$\set{\Pi^{\ast}(\sstep[\ast]), w}$ of~$\vspan \set{\Pi^{\ast}(\sstep[\ast]), \Pi^{\ast}(\nabla Q(\sstep[\ast]))}$ by computing the vector~$w \in \R^n$ that satisfies
\begin{equation*}
    w^{\T} \Pi^{\ast}(\sstep[\ast]) = 0, \quad w^{\T} \Pi^{\ast}(\nabla Q(\sstep[\ast])) < 0, \quad \text{and} \quad \norm{w} = \norm{\Pi^{\ast}(\sstep[\ast])}.
\end{equation*}
Further, the method considers the function
\begin{equation*}
    \sstep(\theta) \eqdef [\cos(\theta) - 1] \Pi^{\ast}(\sstep[\ast]) + \sin(\theta) w, \quad \text{for~$0 \le \theta \le \pi / 2$,}
\end{equation*}
finds an approximate solution~$\theta^{\ast}$ to the univariate problem
\begin{subequations}
    \begin{align}
        \min_{\theta \in \R}    & \quad Q(\sstep[\ast] + \sstep(\theta))\\
        \text{s.t.}             & \quad \xl \le \sstep[\ast] + \sstep(\theta) \le \xu, \label{eq:cobyqa-tangential-bounds-alternative-2-bds}\\
                                & \quad 0 \le \theta \le \pi / 2,
    \end{align}
\end{subequations}
and uses~$\sstep(\theta^{\ast})$ as an approximate solution to~\cref{eq:cobyqa-tangential-bounds-alternative}.
Note that the choice of~$w$ ensures that~$\norm{\sstep[\ast] + \sstep(\theta^{\ast})} = \norm{\sstep[\ast]} = \rad$.

% To solve approximately such a problem, the solver seeks for the greatest reduction in the objective function for a range of equally spaced values of~$\theta$, chosen to ensure the feasibility of the iterates.
If the value of~$\theta^{\ast}$ is restricted by~\cref{eq:cobyqa-tangential-bounds-alternative-2-bds}, then the first active bound is added to the working set~$\mathcal{W}^{\ast}$, and the refining procedure specified above is invoked again after~$\sstep[\ast]$ is updated to~$\sstep[\ast] + \sstep(\theta^{\ast})$, and~$\Pi^{\ast}$ is updated according to~$\mathcal{W}^{\ast}$.
Since the working set is only enlarged, this procedure is invoked at most~$n - \card(\mathcal{W}^{\ast})$ times, where~$\mathcal{W}^{\ast}$ denotes the working set when \cref{alg:bvtcg} terminates.

% The bound-constrained \gls{tcg} procedure together with the improving mechanism is implemented in \gls{cobyqa} under the function \texttt{cobyqa.linalg.bvtcg}.\index{TCG@\glsfmtshort{tcg}!bound-constrained|)}

\subsection{Linearly constrained case}
\label{subsec:lctcg}

\index{TCG@\glsfmtshort{tcg}!linearly constrained|(}We now introduce the linearly constrained variation of the \gls{tcg} method designed by \citeauthor{Powell_2015}~\cite{Powell_2015} for his solver \gls{lincoa}.
This algorithm is described in~\cite[\S~3,\S~5]{Powell_2015}.
For ease of reference, we present it in \cref{alg:lctcg} and briefly explain the idea behind the algorithm in the sequel.
The original method in~\cite{Powell_2015} is devised for linear inequality constraints only, and \cref{alg:lctcg} adapts it to handle linear equality constraints as well.

This method is employed by \gls{cobyqa} when~$\iub \cup \ieq \neq \emptyset$.
In such a case, the tangential subproblem~\cref{eq:cobyqa-tangential} is of the form
\begin{subequations}
    \label{eq:problem-lctcg}
    \begin{align}
        \min_{\sstep \in \R^n}  & \quad Q(\sstep)\\
        \text{s.t.}             & \quad A \sstep \le b, \label{eq:problem-lctcg-ub}\\
                                & \quad C \sstep = 0,\\
                                & \quad \norm{\sstep} \le \rad,
    \end{align}
\end{subequations}
where the objective function~$Q$ is quadratic,~$A \in \R^{m_1 \times n}$,~$C \in \R^{m_2 \times n}$, and~$b \in \R^{m_1}$ with~$b \ge 0$.
In this form, we include the bound constraints in~\cref{eq:problem-lctcg-ub}.
Since the method is a feasible method, the bounds will be respected.
In what follows, we denote the~$j$th column of~$A^{\T}$ by~$a_j$, and that of~$C^{\T}$ by~$c_j$.

The method uses an active-set strategy.
It maintains a working set of the linear \emph{inequality} constraints.
% In addition, it maintains the QR factorization of a matrix whose columns form a basis of the space spanned by the gradients of the constraints in this working set.
As in \cref{alg:bvtcg}, if the working set is modified, then the \gls{tcg} procedure is restarted.
However, this method allows constraints to be removed from the working set, while \cref{alg:bvtcg} does not.

\subsubsection{Description of the working set}

The working set is not directly the set of the active inequality constraints at the current iterate, for the following reason.
Assume that~$b_j$ is positive and tiny for a certain~$j$, so that the~$j$th constraint is almost active at~$\sstep[0] = 0$.
If~$j$ does not belong to the initial working set and if we have~$a_j^{\T} \nabla Q(\sstep[0]) < 0$, then it is likely that the first step~$\sstep[1]$ generated by the \gls{tcg} method has a small norm, because a tiny step along the search direction~$-\nabla Q(\sstep[0])$ may reach the boundary of the feasible set.
Moreover, this tiny step would lead to a change of the working set, and as observed by \citeauthor{Powell_2015}~\cite[\S~3]{Powell_2015}, small steps that cause changes in the working set are numerically
expensive, and hence, should be avoided.
Therefore, we must consider constraints with small residuals when defining the working set.

The method simultaneously defines the initial search direction and the initial working set.
More precisely, for any~$\sstep \in \R^n$ that is feasible for~\cref{eq:problem-lctcg}, we set
\begin{equation*}
    \mathcal{J}(\sstep) \eqdef \set[\big]{j \in \set{1, \dots, m_1} : b_j - a_j^{\T} \sstep \le \sigma \rad \norm{a_j}},
\end{equation*}
for some~$\sigma \in (0, 1)$.
We take~$\sigma = 0.2$ in \gls{cobyqa}.
The method defines the initial search direction~$\pstep[0]$ as the solution to
\begin{subequations}
    \label{eq:tcg-linear-working-set}
    \begin{align}
        \min_{\sstep \in \R^n}  & \quad \norm{\sstep + \nabla Q(\sstep[0])}\\
        \text{s.t.}             & \quad a_j^{\T} \sstep \le 0, ~ j \in \mathcal{J}(\sstep[0]),\\
                                & \quad C \sstep = 0.
    \end{align}
\end{subequations}
In other words,~$\pstep[0]$ is the closest direction from~$-\nabla Q(\sstep[0])$ such that any step from~$\sstep[0]$ along~$\pstep[0]$ moves no closer to the boundary of the constraints in~$\mathcal{J}(\sstep[0])$.
The initial working set is chosen to be
\begin{equation}
    \label{eq:tcg-linear-working-set-initial}
    \mathcal{W}^0 = \set{j \in \mathcal{J}(\sstep[0]) : a_j^{\T} \pstep[0] = 0}.
\end{equation}
It excludes the inequality constraints with~$j \in \mathcal{J}(\sstep[0])$ and~$a_j^{\T} \pstep[0] < 0$, because any step from~$\sstep[0]$ along~$\pstep[0]$ moves further away from these constraints.

After defining~$\pstep[0]$ and~$\mathcal{W}^0$, the method then considers the restriction of~\cref{eq:problem-lctcg} to the orthogonal complement\footnote{The orthogonal complement of~\cref{eq:cg-space-init} is the null space of the corresponding constraints.} of
\begin{equation}
    \label{eq:cg-space-init}
    \set[\big]{a_j : j \in \mathcal{W}^0} \cup \set[\big]{c_j : 1 \le j \le m_2},
\end{equation}
and entertains CG iterations in this space, using~$\pstep[0]$ as the first search direction.
Note that~$\pstep[0]$ is the projection of~$-\nabla Q(\sstep[0])$ onto this space (see~\cref{lem:lctcg}).
When a linear inequality constraint or the trust-region constraint is hit by a CG iterate, the CG
step is truncated and the CG iterations are terminated.
However, upon this termination, if the current iterate~$\sstep[k + 1]$ is not on the trust-region boundary, then the procedure is restarted, changing the next search direction~$\pstep[k + 1]$ to the unique solution to
\begin{subequations}
    \label{eq:tcg-linear-working-set-after}
    \begin{align}
        \min_{\sstep \in \R^n}  & \quad \norm{\sstep + \nabla Q(\sstep[k + 1])}\\
        \text{s.t.}             & \quad a_j^{\T} \sstep \le 0, ~ j \in \mathcal{J}(\sstep[k + 1]),\\
                                & \quad C \sstep = 0.
    \end{align}
\end{subequations}
Further, the working set is modified to be
\begin{equation}
    \label{eq:tcg-linear-working-set-restart}
    \mathcal{W}^{k + 1} = \set{j \in \mathcal{J}(\sstep[k + 1]) : a_j^{\T} \pstep[k + 1] = 0}.
\end{equation}
% The method terminates if a \gls{tcg} iterate either reaches the boundary of the trust region or hits a new linear inequality constraint while being close to the trust-region boundary.

% Using QR factorizations, it builds a linearly independent subset of the working set.
% Therefore, after executing the \citeauthor{Goldfarb_Idnani_1983} method, we have a solution to~\cref{eq:tcg-linear-working-set} together with a linearly independent subset of the working set.
% In fact, denote~$\hat{Q}R$ the QR factorization of the matrix whose columns are the gradients of the active constraints at the solution, and let~$\check{Q}$ be a matrix such that~$[\hat{Q}, \check{Q}]$ is orthogonal.
% It turns out that the solution to~\cref{eq:tcg-linear-working-set} is~$\check{Q} \check{Q}^{\T} v$ (see~\cite[Eq.~(3.7)]{Powell_2015}).

\subsubsection{Description of the \glsfmtshort{tcg} method}

\Cref{alg:lctcg} details the linearly constrained \gls{tcg} method designed by \citeauthor{Powell_2015}~\cite{Powell_2015}.
In the algorithm, we denote by~$\Pi^k$ the projection onto the orthogonal complement of
\begin{equation*}
    \set[\big]{a_j : j \in \mathcal{W}^k} \cup \set[\big]{c_j : 1 \le j \le m_2}.
\end{equation*}

\begin{algorithm}
    \caption[Linearly constrained \glsfmtshort{tcg} method]{Linearly constrained \glsfmtshort{tcg} method~\cite[\S~3, \S~5]{Powell_2015}}
    \label{alg:lctcg}
    \DontPrintSemicolon
    \onehalfspacing
    \KwData{Quadratic function~$Q$, matrices~$A$ and~$C$, right-hand side~$b \ge 0$, trust-region radius~$\rad > 0$, and constant~$\sigma \in (0, 1)$.}
    \KwResult{An approximate solution to~\cref{eq:problem-lctcg}.}
    Set~$\sstep[0] \gets 0$\;
    Set~$\pstep[0]$ to the solution to~\cref{eq:tcg-linear-working-set}\; \nllabel{alg:lctcg-sd-init}
    Set~$\mathcal{W}^0$ as described in~\cref{eq:tcg-linear-working-set-initial}\; \nllabel{alg:lctcg-set-init}
    \For{$k = 0, 1, \dots$ until~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$}{ \nllabel{alg:lctcg-stop}
        Set~$\alpha_Q^k \gets \argmin @@ \set{Q(\sstep[k] + \alpha \pstep[k]) : \alpha \ge 0}$ using~\cref{eq:tcg-alphaq}\;
        % Set
        % \begin{algoempheq}[left={\alpha_Q^k \gets \empheqlbrace}]{alignat*=2}
        %     & \frac{- \nabla Q(\sstep[k])^{\T} \pstep[k]}{(\pstep[k])^{\T} (\nabla^2 Q) \pstep[k]}  && \quad \text{if~$(\pstep[k])^{\T} (\nabla^2 Q) \pstep[k] > 0$,}\\
        %     & \infty                                                                                && \quad \text{otherwise}
        % \end{algoempheq}
        Set~$\alpha_{\rad}^k \gets \argmax @@ \set{\alpha \ge 0 : \norm{\sstep[k] + \alpha \pstep[k]} \le \rad}$ using~\cref{eq:tcg-alphad} \nllabel{alg:lctcg-alpharad}\;
        Set~$\alpha_L^k \gets \argmax @@ \set{\alpha \ge 0 : A(\sstep[k] + \alpha \pstep[k]) \le b}$ \nllabel{alg:lctcg-alphal}\;
        Set the step size~$\alpha^k \gets \min @@ \set{\alpha_Q^k, \alpha_{\rad}^k, \alpha_L^k}$\;
        Update the iterate~$\sstep[k + 1] \gets \sstep[k] + \alpha^k \pstep[k]$\;
        \uIf{$\alpha^k < \min \set{\alpha_{\rad}^k, \alpha_L^k}$}{
            Set~$\mathcal{W}^{k + 1} \gets \mathcal{W}^k$\;
            Set~$\beta^k \gets \norm{\Pi^{k + 1}(\nabla Q(\sstep[k + 1]))}^2 / \norm{\Pi^{k + 1}(\nabla Q(\sstep[k]))}^2$\;
            % Set
            % \begin{algomathdisplay}
            %     \beta^k \gets \frac{\Pi^{k + 1} \big( \nabla Q(\sstep[k + 1]) \big)^{\T} (\nabla^2 Q) \pstep[k]}{(\pstep[k])^{\T} (\nabla^2 Q) \pstep[k]}
            % \end{algomathdisplay}
            Update~$\pstep[k + 1] \gets -\Pi^{k + 1} \big( \nabla Q(\sstep[k + 1]) \big) + \beta^k \pstep[k]$\; \nllabel{alg:lctcg-tcg-step}
        }
        \uElseIf{$\alpha^k < \alpha_{\rad}^k$}{ \nllabel{alg:lctcg-restart}
            Set~$\pstep[k + 1]$ to the solution to~\cref{eq:tcg-linear-working-set-after}\; \nllabel{alg:lctcg-sd-restart}
            Set~$\mathcal{W}^{k + 1}$ as described in~\cref{eq:tcg-linear-working-set-restart}\; \nllabel{alg:lctcg-set-restart}
        }
        \Else{
            Break\;
        }
    }
\end{algorithm}

\Cref{alg:lctcg} takes almost the same form as~\cref{alg:bvtcg}.
The role of~$\alpha_L^k$ in \cref{alg:lctcg} is identical to that of~$\alpha_B^k$ in~\cref{alg:bvtcg}.
The only essential difference between the two algorithms is how they define the working set and how
they define the searching direction when the working set is updated.

\Cref{alg:lctcg-restart} of \cref{alg:lctcg} differs from Powell's algorithm described in~\cite[\S~3,\S~5]{Powell_2015}.
Powell's algorithm replaces~$\alpha^k < \alpha_{\rad}^k$ in \cref{alg:lctcg-restart} with~$\norm{\sstep[k]} \le (1-\sigma)\rad$ (see the end of~\cite[\S~3]{Powell_2015}),
which is a stronger condition because~$\norm{\sstep[k]} < \rad$ implies~$\alpha^k < \alpha_{\rad}^k$.
In our numerical experiments with \gls{cobyqa}, the condition~$\alpha^k < \alpha_{\rad}^k$ works slightly better than~$\norm{\sstep[k]} \le (1-\sigma)\rad$.

Problems~\cref{eq:tcg-linear-working-set,eq:tcg-linear-working-set-after} are solved using the Goldfarb-Idnani method for quadratic programming~\cite{Goldfarb_Idnani_1983}.
It is an active-set method designed for minimizing positive definite quadratic function subject to linear constraints.

\subsubsection{Stopping criteria}

Similar to \cref{prop:tcg,prop:bvtcg}, we will show that the stopping criterion
in \cref{alg:lctcg-stop} of \cref{alg:lctcg} is also equivalent to~$\norm{\pstep[k]} = 0$.
To this end, we need the following simple observation taken from~\cite[\S~3]{Powell_2015}.
Its proof is \emph{trivial}, but we include it for completeness.

\begin{lemma}
    \label{lem:lctcg}
    For~$g \in \R^n$ and~$\mathcal{J} \subseteq \set{1, \dots, m_1}$, let~$\tilde{\pstep}$ be the unique solution to
    \begin{subequations}
        \label{eq:lctcg-lemma}
        \begin{align}
            \min_{\sstep \in \R^n}  & \quad \norm{\sstep - g}\\
            \text{s.t.}             & \quad a_j^{\T} \sstep \le 0, ~ j \in \mathcal{J},\\
                                    & \quad C \sstep = 0.
        \end{align}
    \end{subequations}
    Then~$\tilde{\pstep}$ is the projection of~$g$ onto the orthogonal complement of
    \begin{equation*}
        \set[\big]{a_j : j \in \mathcal{J}, ~ a_j^{\T} \tilde{\pstep} = 0} \cup \set[\big]{c_j : 1 \le j \le m_2}.
    \end{equation*}
\end{lemma}

\begin{proof}
    Define~$\tilde{\mathcal{J}} = \set{j \in \mathcal{J} : a_j^{\T} \tilde{\pstep} = 0}$.
    Then it suffices to prove that~$\tilde{\pstep}$ solves
    \begin{subequations}
        \label{eq:lctcg-lemma-proof-1}
        \begin{align}
            \min_{\sstep \in \R^n}  & \quad \norm{\sstep - g}\\
            \text{s.t.}             & \quad a_j^{\T} \sstep = 0, ~ j \in \tilde{\mathcal{J}},\\
                                    & \quad C \sstep = 0,
        \end{align}
    \end{subequations}
    whose solution is unique.
    By the definitions of~$\mathcal{J}$ and~$\tilde{\mathcal{J}}$, there exists a neighborhood~$\mathcal{N}$ of~$\tilde{\pstep}$ such that
    \begin{equation}
        \label{eq:lctcg-lemma-proof-2}
        \mathcal{N} \cap \set{\sstep \in \R^n : a_j^{\T} \sstep = 0, ~ j \in \tilde{\mathcal{J}}, ~ C \sstep = 0} \subseteq \set{\sstep \in \R^n : a_j^{\T} \sstep \le 0, ~ j \in \mathcal{J}, ~ C \sstep = 0}.
    \end{equation}
    Since~$\tilde{\pstep}$ is a global solution to~\cref{eq:lctcg-lemma}, we obtain from~\cref{eq:lctcg-lemma-proof-2} that~$\tilde{\pstep}$ is a local solution to~\cref{eq:lctcg-lemma-proof-1}.
    This finishes the proof because~\cref{eq:lctcg-lemma-proof-1} is convex.
\end{proof}

% We observe the following, which is useful for a robust implementation.

\begin{proposition}
    \label{prop:lctcg}
    In \cref{alg:lctcg-stop} of~\cref{alg:lctcg}, the stopping criterion~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$ is satisfied if and only if~$\norm{\pstep[k]} = 0$.
\end{proposition}

\begin{proof}
    It is obvious that~$\norm{\pstep[k]} = 0$ implies~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$.
    Suppose that~$\nabla Q (\sstep[k])^{\T} \pstep[k] \ge 0$ in \cref{alg:lctcg-stop} of~\cref{alg:lctcg}, and assume without loss of generality that~$k \ge 1$.
    If \cref{alg:lctcg-stop} is reached from \cref{alg:lctcg-set-restart}, then we have~$\pstep[k] = -\Pi^k(\nabla Q(\sstep[k]))$ according to \cref{lem:lctcg}.
    Thus,
    \begin{equation*}
        \norm{\pstep[k]}^2 = \norm{\Pi^k(\nabla Q(\sstep[k]))}^2 = Q (\sstep[k])^{\T}  \Pi^k(\nabla Q(\sstep[k])) = - Q (\sstep[k])^{\T}  \pstep[k] \le 0,
    \end{equation*}
    and hence~$\norm{\pstep[k]} = 0$.
    If \cref{alg:lctcg-stop} is reached from \cref{alg:lctcg-tcg-step}, then we can establish~$\norm{\pstep[k]} = 0$ by an argument similar to the proof of \cref{prop:tcg}.
\end{proof}

Similar to the bound-constrained case, \Citeauthor{Powell_2015}~\cite[\S~2]{Powell_2015} proposed two additional stopping criteria for the \gls{tcg} algorithm, in order to avoid unworthy computations.
\Cref{alg:lctcg} omits them for simplicity, and we specify them now.
First of all, the algorithm is stopped if the move along~$\pstep[k]$ is expected to give a relatively small reduction in~$Q$, namely if
\begin{equation}
    \label{eq:lctcg-stop1}
    \alpha_{\rad}^k \abs[\big]{\nabla Q(\sstep[k])^{\T} \pstep[k]} \le \nu [Q(\sstep[0]) - Q(\sstep[k])]
\end{equation}
for some constant~$\nu > 0$.
The condition~\cref{eq:lctcg-stop1} can be regarded as a refined and weakened version of~\cref{eq:bvtcg-stop1}.
Moreover, the algorithm also tests~\cref{eq:bvtcg-stop2} and stop when this condition holds.
%, which means that the reduction in~$Q$ provided by the recent iteration is tiny compared to the reduction so far.
\index{TCG@\glsfmtshort{tcg}!linearly constrained|)}
As demonstrated by~\citeauthor{Powell_2015}~\cite[\S~5]{Powell_2015}, these two stopping criteria guarantee that the algorithm terminates after a finite number of iterations.
\Gls{cobyqa} includes both of the stopping criteria with~$\nu = 0.01$ when implementing~\cref{alg:lctcg}.

% The linearly constrained \gls{tcg} procedure with these additional stopping criteria is implemented in \gls{cobyqa} under the function \texttt{cobyqa.linalg.lctcg}.

% \todo{Tom: Your description of the ``modification'' (moving towards boundaries of the active
% constraints) is wrong. Check
% \url{https://github.com/zequipe/pdfo_ref/blob/master/fsrc/lincoa/trustregion.f90}. Is your
% implementation correct? It is important for the performance of LINCOA according to my test. In the
% thesis, however, I suggest we omit this part. I will explain to you why. }
%\subsubsection{Moving towards the boundaries of the constraints in the working set}

%Now we describe a modification that \citeauthor{Powell_2015} proposed to \cref{alg:lctcg}.
%Note that this modification is not mentioned in in the paper~\cite{Powell_2015}, but is implemented
%in \citeauthor{Powell_2015}'s code of \gls{lincoa}. \gls{cobyqa} also implements it.

%This modification is applied before initiating the CG iterations when a new working set is defined.
%More specifically, it is applied to \cref{alg:lctcg-stop} of \cref{alg:lctcg}, before the stopping
%criteria~$\nabla Q(\sstep[k])^\T \pstep[k] \ge 0$ is tested, and it is applied if and only if this
%line is reached from either \cref{alg:lctcg-set-init} or \cref{alg:lctcg-set-restart}.
%In these cases, the algorithm tests whether
%\begin{equation*}
%    b_j - a_j^{\T} \sstep[k] \ge \xi \rad \norm{a_j}
%\end{equation*}
%for some~$j \in \mathcal{W}^k$. [Zaikun: This is wrong! It tests the linear independent subset
%of~$\mathcal{W}^k$.]

%%After calculating the search direction at \cref{alg:lctcg-sd-init} of \cref{alg:lctcg}, we might have
%%\begin{equation*}
%%    b_j - a_j^{\T} (\sstep[0] + \pstep[0]) \ge \xi \rad \norm{a_j}
%%\end{equation*}
%%for some~$\xi \in (0, \sigma)$ and some~$j$ in the working set.
%%In such a case, the method first moves~$\sstep[0]$ towards the boundaries of the constraints in the working set.
%%This is because the constraints in the working set are presumed active at the solution,
%%In \gls{cobyqa}, we choose~$\xi = 10^{-4}$.
%%The same applies to \cref{alg:lctcg-sd-restart} of \cref{alg:lctcg}.

\section{Solving the normal subproblem}
\label{sec:cobyqa-normal}

In this section, we present the method employed by \gls{cobyqa} to approximately solve its normal subproblem~\cref{eq:cobyqa-normal}.
Unlike for the tangential subproblem, the objective function of the normal subproblem is not quadratic, but piecewise quadratic.
More specifically, problem~\cref{eq:cobyqa-normal} is of the form
\begin{subequations}
    \label{eq:problem-cpqp}
    \begin{align}
        \min_{z \in \R^n}   & \quad \norm{\posp{\hat{A} z - \hat{b}}}^2 + \norm{\check{A} z - \check{b}}^2\\
        \text{s.t.}         & \quad \xl \le z \le \xu,\\
                            & \quad \norm{z} \le \rad,
    \end{align}
\end{subequations}
where~$\hat{A} \in \R^{m_1 \times n}$,~$\check{A} \in \R^{m_2 \times n}$,~$\hat{b} \in \R^{m_1}$,~$\check{b} \in \R^{m_2}$,~$\xl \in (\R \cup \set{-\infty})^n$, and~$\xu \in (\R \cup \set{\infty})^n$.
We assume that the bounds~$\xl$ and~$\xu$ satisfy~$\xl \le 0$,~$\xu \ge 0$, and~$\xl < \xu$.
Note that these bounds are not the same as in~\cref{eq:problem-cobyqa}.
In~\cref{eq:problem-cpqp},~$\posp{\cdot}$ takes the positive part of a number.

\subsubsection{Reformulation of the problem}

The main difficulty in solving~\cref{eq:problem-cpqp}, even approximately, is the piecewise quadratic term~$\norm{\posp{\hat{A} z - \hat{b}}}^2$ in its objective function.
To handle this term, we introduce a new variable~$v \in \R^{m_1}$ and reformulate~\cref{eq:problem-cpqp} as
\begin{align*}
    \min_{(z, v) \in \R^n \times \R^{m_1}}  & \quad \norm{\check{A} z - \check{b}}^2 + \norm{v}^2\\
    \text{s.t.}                             & \quad \hat{A} z - v \le \hat{b},\\
                                            & \quad v \ge 0,\\
                                            & \quad \xl \le z \le \xu,\\
                                            & \quad \norm{z} \le \rad.
\end{align*}
This problem takes the form
\begin{subequations}
    \label{eq:problem-cpqp-reformulated}
    \begin{align}
        \min_{\sstep \in \R^{n + m_1}}  & \quad Q(\sstep)\\
        \text{s.t.}                     & \quad A \sstep \le b,\\
                                        & \quad \norm{\sstep}_{D} \le \rad, \label{eq:problem-cpqp-reformulated-tr}
    \end{align}
\end{subequations}
with~$s$ being the vector that concatenates~$z$ and~$v$, and
\begin{equation*}
    Q(\sstep) \eqdef - 2 \begin{bmatrix} \check{b}^{\T} \check{A} & 0 \end{bmatrix} s + s^{\T} \begin{bmatrix} \check{A}^{\T} \check{A} & 0\\ 0 & I_{m_1} \end{bmatrix} s, \quad 
    A \eqdef
    \begin{bmatrix}
        \hat{A} & -I_{m_1}\\
        0       & -I_{m_1}\\
        I_n     & 0\\
        -I_n    & 0
    \end{bmatrix}, \quad \text{and} \quad
    b \eqdef
    \begin{bmatrix}
        \hat{b}\\
        0\\
        \xu\\
        -\xl
    \end{bmatrix}.
\end{equation*}
In~\cref{eq:problem-cpqp-reformulated},~$\norm{\cdot}_D$ denotes the seminorm defined by
\begin{equation*}
    \norm{\sstep}_D \eqdef \sqrt{\sstep^{\T} D \sstep},
\end{equation*}
with~$D$ being the diagonal matrix whose first~$n$ diagonal elements are one, and the remaining~$m_1$ components are zero.

Compared with the original problem~\cref{eq:problem-cpqp}, problem~\cref{eq:problem-cpqp-reformulated} increases the dimension of the problem, and introduce additional linear inequality constraints.
However, it has the advantage that its objective function is quadratic.
In addition, this reformulation enables us to apply a \gls{tcg} method, as specified below.
% From a geometrical point of view, the feasible set engendered by the only nonlinear constraint is a ball for the problems tackled by \cref{alg:lctcg}, while it is a cylinder in~\cref{eq:problem-cpqp-reformulated}.
% To solve problem~\cref{eq:problem-cpqp-reformulated}, and hence~\cref{eq:problem-cpqp}, we design below a variation of \cref{alg:lctcg}.

\subsubsection{Description of the \glsfmtshort{tcg} method}

We observe that problem~\cref{eq:problem-cpqp-reformulated} takes almost the same form as~\cref{eq:problem-lctcg} with~$C = 0$, the only difference being that the trust-region constraint~\cref{eq:problem-cpqp-reformulated-tr} is defined using the seminorm~$\norm{\cdot}_D$ instead of the~$\ell_2$-norm.
Based on this observation, we propose to approximately solve~\cref{eq:problem-cpqp-reformulated} using~\cref{alg:cpqp}, which is a straightforward adaptation of \cref{alg:lctcg} that replaces~$\norm{\cdot}$ with~$\norm{\cdot}_D$ in the trust region.

\begin{algorithm}
    \caption{\Glsfmtshort{tcg} method for approximately solving~\cref{eq:problem-cpqp-reformulated}}
    \label{alg:cpqp}
    \DontPrintSemicolon
    \onehalfspacing
    \KwData{Quadratic function~$Q$, matrices~$A$ and~$D$, right-hand side~$b$, trust-region radius~$\rad > 0$, and constant~$\sigma \in (0, 1)$.}
    \KwResult{An approximate solution to~\cref{eq:problem-cpqp-reformulated}.}
    \nonl Identical to \cref{alg:lctcg} with~$C = 0$, except that the initial guess~$\sstep[0]$ is not~$0$ but\;
    \nonl a feasible point, and the formula in \cref{alg:lctcg-alpharad} is replaced with
    \begin{algomathdisplay}
        \alpha_{\rad}^k \gets \argmax @@ \set{\alpha \ge 0 : \norm{\sstep[k] + \alpha \pstep[k]}_D \le \rad}.
    \end{algomathdisplay}
\end{algorithm}

Note that \cref{alg:cpqp} requires the initial guess to be feasible; otherwise,~$\alpha_{\rad}^0$ or~$\alpha_L^0$ may not exist (see \cref{alg:lctcg-alpharad,alg:lctcg-alphal} of \cref{alg:lctcg}).
In practice, we take
\begin{equation*}
    \sstep[0] =
    \begin{bmatrix}
        0\\
        \posp{-\hat{b}}
    \end{bmatrix}.
\end{equation*}

% It is important to remark that in \cref{alg:lctcg}, if~$\alpha^k = \alpha_{\rad}^k$, then the algorithm stops.
% Otherwise, we necessarily have~$\alpha^k > \alpha_{\rad}^k$.

% \subsubsection{Description of the working set}

% Just as in \cref{subsec:lctcg}, we define
% \begin{equation*}
%     \mathcal{J}(\sstep, v) \eqdef \set[\Big]{j \le 2n + 2m_1 : h_j - e_j^{\T}(E \sstep + F v) \le \sigma \rad \sqrt{\norm{e_j^{\T} E}^2 + \norm{e_j^{T} F}^2}},
% \end{equation*}
% for some~$\sigma \in (0, 1)$, set to~$\sigma = 0.2$ in \gls{cobyqa}.
% The initial iterates, that must be feasible, are~$\sstep[0] = 0$ and~$v^0 = \posp{-b}$
% Further, we let the initial search directions~$(\pstep[0]_{\sstep}, \pstep[0]_v)$ solve
% \begin{subequations}
%     \label{eq:cpqp-working-set}
%     \begin{align}
%         \min_{(\sstep, v) \in \R^n \times \R^{m_1}} & \quad \frac{1}{2} [ \norm{\sstep + \nabla_{\sstep} Q(\sstep[k], v^k)}^2 + \norm{v + \nabla_v Q(\sstep[k], v^k)}^2 ]\\
%         \text{s.t.}                                 & \quad e_j^{\T} (E \sstep + F v) \le 0, ~ j \in \mathcal{J}(\sstep[k], v^k).
%     \end{align}
% \end{subequations}
% for~$k = 0$.
% The initial working set is then
% \begin{equation}
%     \label{eq:cpqp-working-set-initial}
%     \mathcal{W}^0 = \set{j \in \mathcal{J}(\sstep[0], v^0) : e_j^{\T}(E \pstep[0]_{\sstep} + F \pstep[0]_v) = 0}.
% \end{equation}

% As in \cref{alg:lctcg}, when a new constraint is hit by a \gls{tcg} step, then the method is restarted.
% In such a case, the next search directions~$(\pstep[k + 1]_{\sstep}, \pstep[k + 1]_v)$ solve~\cref{eq:cpqp-working-set} after incrementing~$k$, and the working set is modified to be
% \begin{equation}
%     \label{eq:cpqp-working-set-restart}
%     \mathcal{W}^{k + 1} = \set{j \in \mathcal{J}(\sstep[k + 1], v^{k + 1}) : e_j^{\T}(E \pstep[k + 1]_{\sstep} + F \pstep[k + 1]_v) = 0}.
% \end{equation}
% We denote for convenience by~$(\Pi_{\sstep}^k(x, z), \Pi_v^k(x, z))$ be the unique solution to
% \begin{align*}
%     \min_{(\sstep, v) \in \R^n \times \R^{m_1}} & \quad \frac{1}{2} [ \norm{\sstep - x}^2 + \norm{v - z}^2 ]\\
%     \text{s.t.}                                 & \quad e_j^{\T} (E \sstep + F v) = 0, ~ j \in \mathcal{W}^k,
% \end{align*}
% for~$(x, z) \in \R^n \times \R^{m_1}$, and we remark that
% \begin{empheq}[left=\empheqlbrace]{alignat*=1}
%     & \pstep[0]_{\sstep} = \Pi_{\sstep}^0(-\nabla_{\sstep} Q(\sstep[0], v^0), -\nabla_v Q(\sstep[0], v^0)) = -\Pi_{\sstep}^0(\nabla_{\sstep} Q(\sstep[0], v^0), \nabla_v Q(\sstep[0], v^0)),\\
%     & \pstep[0]_v = \Pi_v^0(-\nabla_{\sstep} Q(\sstep[0], v^0), -\nabla_v Q(\sstep[0], v^0)) = -\Pi_v^0(\nabla_{\sstep} Q(\sstep[0], v^0), \nabla_v Q(\sstep[0], v^0)).
% \end{empheq}
% Similar conclusion can be drawn for~$\pstep[k + 1]_{\sstep}$ and~$\pstep[k + 1]_v$ when the method is restarted.

% As for \cref{alg:lctcg}, the problem~\cref{eq:cpqp-working-set} is solved numerically using the \citeauthor{Goldfarb_Idnani_1983} method~\cite{Goldfarb_Idnani_1983}.

% \subsubsection{Description of the \glsfmtshort{tcg}-like method}

% We are now equipped to present the \gls{tcg}-like method for solving the problem~\cref{eq:problem-cpqp-reformulated}, and hence~\cref{eq:problem-cpqp}.
% The complete method is described in \cref{alg:cpqp}.

% \begin{algorithm}
%     \caption{\Glsfmtshort{tcg}-like method for convex piecewise quadratic programming}
%     \label{alg:cpqp}
%     \DontPrintSemicolon
%     \onehalfspacing
%     \KwData{Matrices~$C$,~$E$, and~$F$, vectors~$d$ and~$h$, trust-region radius~$\rad > 0$, and constant~$\sigma \in (0, 1)$.}
%     \KwResult{An approximate solution to~\cref{eq:problem-cpqp}.}
%     Set~$\sstep[0] \gets 0$,~$v^0 \gets \posp{-b}$, and~$g^0 \gets -C^{\T} C d$\;
%     Set~$(\pstep[0]_{\sstep}, \pstep[0]_v)$ to the solution to~\cref{eq:cpqp-working-set} with~$k = 0$\; \nllabel{alg:cpqp-sd-init}
%     Set~$\mathcal{W}^0$ as described in~\cref{eq:cpqp-working-set-initial}\;
%     \For{$k = 0, 1, \dots$ until~$\norm{\pstep[k]_{\sstep}}^2 + \norm{\pstep[k]_v}^2 = 0$}{
%         Set
%         \begin{algoempheq}[left={\alpha_Q^k \gets \empheqlbrace}]{alignat*=2}
%             & \frac{- (g^k)^{\T} \pstep[k]_{\sstep} - (v^k)^{\T} \pstep[k]_v}{\norm{C \pstep[k]_{\sstep}}^2 + \norm{\pstep[k]_v}^2} && \quad \text{if~$\norm{C \pstep[k]_{\sstep}}^2 + \norm{\pstep[k]_v}^2 > 0$,}\\
%             & \infty                                                                                                                && \quad \text{otherwise}
%         \end{algoempheq}
%         Set~$\alpha_{\rad}^k \gets \argmax @@ \set{\alpha \ge 0 : \norm{\sstep[k] + \alpha \pstep[k]_{\sstep}} \le \rad}$\;
%         Set~$\alpha_L^k \gets \argmax @@ \set{\alpha \ge 0 : E (\sstep[k] + \alpha \pstep[k]_{\sstep}) + F (v^k + \alpha \pstep[k]_v) \le h}$\;
%         Set the step size~$\alpha^k \gets \min @@ \set{\alpha_Q^k, \alpha_{\rad}^k, \alpha_L^k}$\;
%         Update the iterates~$\sstep[k + 1] \gets \sstep[k] + \alpha^k \pstep[k]_{\sstep}$ and~$v^{k + 1} \gets v^k + \alpha^k \pstep[k]_v$\;
%         Update the gradient~$g^{k + 1} \gets g^k + \alpha^k C^{\T} C \pstep[k]_{\sstep}$\;
%         \uIf{$\alpha^k = \alpha_{\rad}^k$}{
%             Break\;
%         }
%         \uElseIf{$\alpha^k = \alpha_L^k$}{
%             Set~$(\pstep[k + 1]_{\sstep}, \pstep[k + 1]_v)$ to the solution to~\cref{eq:cpqp-working-set} after incrementing~$k$\; \nllabel{alg:cpqp-sd-restart}
%             Set~$\mathcal{W}^{k + 1}$ as described in~\cref{eq:cpqp-working-set-restart}\;
%         }
%         \Else{
%             Set~$\mathcal{W}^{k + 1} \gets \mathcal{W}^k$\;
%             Set
%             \begin{algomathdisplay}
%                 \beta^k \gets \frac{\Pi_{\sstep}^{k + 1}(g^{k + 1}, v^{k + 1})^{\T} C^{\T} C \pstep[k]_{\sstep} + \Pi_v^{k + 1}(g^{k + 1}, v^{k + 1})^{\T} \pstep[k]_v}{\norm{C \pstep[k]_{\sstep}}^2 + \norm{\pstep[k]_v}^2}
%             \end{algomathdisplay}
%             Update~$\pstep[k + 1]_{\sstep} \gets -\Pi_{\sstep}^{k + 1}(g^{k + 1}, v^{k + 1}) + \beta^k \pstep[k]_{\sstep}$ and~$\pstep[k + 1]_v \gets -\Pi_v^{k + 1}(g^{k + 1}, v^{k + 1}) + \beta^k \pstep[k]_v$\;
%         }
%     }
% \end{algorithm}

% Similarly to \cref{alg:lctcg}, after calculating the search directions at \cref{alg:cpqp-sd-init,alg:cpqp-sd-restart}, the error term
% \begin{equation*}
%     h_j - e_j^{\T} E (\sstep[k] + \pstep[k]_{\sstep}) - e_j^{\T} F (v^k + \pstep[k]_v)
% \end{equation*}
% may be substantial for some~$j$ in the working set.
% In such a case, the method first moves~$(\sstep[k], v^k)$ towards the boundaries of the constraints in the working set.

% \subsubsection{Additional stopping criteria}

% As in \cref{alg:lctcg}, we also implement the following two stopping criteria.
% First of all, the computations are stopped if the decrease in the objective function is small along the current search direction, namely if
% \begin{equation*}
%     \alpha_{\rad}^k \abs[\big]{(g^k)^{\T} \pstep[k]_{\sstep} + (v^k)^{\T} \pstep[k]_v} \le \nu [Q(\sstep[0], v^0) - Q(\sstep[k], v^k)],
% \end{equation*}
% for some constant~$\nu > 0$, set to~$\nu = 0.01$ in \gls{cobyqa}.
% Moreover, the computations are also stopped if the reduction provided by the current search direction is tiny compared to the reduction so far, that is if
% \begin{equation*}
%     Q(\sstep[k], v^k) - Q(\sstep[k + 1], v^{k + 1}) \le \nu [Q(\sstep[0], v^0) - Q(\sstep[k + 1], v^{k + 1})].
% \end{equation*}
% These two stopping criteria are once again crucial to the termination of the algorithm.
% Indeed, remark that the trust-region constraint appears only as a stopping criteria in \cref{alg:cpqp}.
% Therefore, the termination proof made by \citeauthor{Powell_2009}~\cite[\S~3]{Powell_2009} for \cref{alg:lctcg} also holds for \cref{alg:cpqp}.

% The \gls{tcg}-like procedure for convex piecewise quadratic programming we presented is implemented in \gls{cobyqa} under the function \texttt{cobyqa.linalg.cpqp}.

\section{Evaluating the least-squares Lagrange multiplier}
\label{sec:cobyqa-lagrange-multipliers}

We present in this section the method employed by \gls{cobyqa} to solve the least-squares problem~\cref{eq:least-squares-lagrange-multipliers-cobyqa}.
It is adapted from the \gls{nnls} algorithm~\cite[Alg.~23.10]{Lawson_Hanson_1987} as follows.
% The update mechanism of the estimated Lagrange multipliers in \gls{cobyqa} is based on constrained least-squares problems, where some variables must remain nonnegative in order to satisfy some complementary slackness conditions.

Problem~\cref{eq:least-squares-lagrange-multipliers-cobyqa} takes the form
\begin{subequations}
    \label{eq:problem-nnls}
    \begin{align}
        \min_{\sstep \in \R^n}  & \quad \mu(\sstep) \eqdef \frac{1}{2} \norm{A \sstep - b}^2\\
        \text{s.t.}             & \quad \sstep_i \ge 0, ~ i = 1, 2, \dots, n_0,
    \end{align}
\end{subequations}
where~$A \in \R^{m \times n}$,~$b \in \R^m$, and~$n_0$ is a nonnegative integer with~$n_0 \le n$.
We observe that if~$n_0 = 0$, then~\cref{eq:problem-nnls} is a simple unconstrained least-squares problem, which can be solved using traditional methods.
Our major concern in this section is the case~$n_0 \ge 1$.

In order to solve problem~\cref{eq:problem-nnls} when~$n_0 \ge 1$, we construct an active-set method based on~\cite[Alg.~23.10]{Lawson_Hanson_1987}, that maintains a working set of the nonnegativity constraints.
%, referred to as \gls{nnls}.
It is a particular case of the \gls{bvls} algorithm~\cite{Stark_Parker_1995}.
The method is described in \cref{alg:nnls}.
% It uses the notation~$\varrho(\mathcal{W})$, defined as follows.
Given a working set~$\mathcal{W} \subseteq \set{1, 2, \dots, n_0}$, the algorithm uses~$\varrho(\mathcal{W})$ to denote the least-norm solution to
\begin{equation*}
    \min_{\sstep \in \R^n} \frac{1}{2} \norm{A_{\scriptscriptstyle \mathcal{W}} \sstep - b}^2
\end{equation*}
where~$A_{\scriptscriptstyle \mathcal{W}}$ is the matrix whose~$i$th column is that of~$A$ if~$i \notin \mathcal{W}$, and zero otherwise.

\begin{algorithm}
    \caption{\Glsdesc{nnls} method}
    \label{alg:nnls}
    \DontPrintSemicolon
    \onehalfspacing
    \KwData{Matrix~$A$, vector~$b$, and integer~$n_0 \ge 1$.}
    \KwResult{A solution to~\cref{eq:problem-nnls}.}
    Set~$\sstep[0] \gets 0$,~$\mathcal{W}^{-1} \gets \set{1, 2, \dots, n_0}$, and~$k \gets 0$\;
    \While{the \gls{kkt} conditions do not hold at~$\sstep[k]$}{ \nllabel{alg:nnls-kkt}
        Build~$\mathcal{W}^k$ by removing from~$\mathcal{W}^{k - 1}$ the smallest~$i$ that solves
        \begin{algomathdisplay}
            \min_{i \in \mathcal{W}^k} @@ \frac{\partial \mu}{\partial \sstep_i}(\sstep[k])
        \end{algomathdisplay} \nllabel{alg:nnls-remove}
        Set the trial point~$\pstep[k] \gets \varrho(\mathcal{W}^k)$\; \nllabel{alg:nnls-trial-1}
        \While{$\pstep[k]_i \le 0$ for some~$i \notin \mathcal{W}^k$ with~$i \le n_0$}{  \nllabel{alg:nnls-inner}
            Set the step size
            \begin{algomathdisplay}
                \alpha^k \gets \min @@ \set[\bigg]{\frac{\sstep[k]_i}{\sstep[k]_i - \pstep[k]_i} : \pstep[k]_i \le 0, ~ i \notin \mathcal{W}^k, ~ i \le n_0}
            \end{algomathdisplay}
            Update~$\sstep[k + 1] \gets \sstep[k] + \alpha^k (\pstep[k] - \sstep[k])$ and increment~$k$\;
            Expand the working set~$\mathcal{W}^k \gets \mathcal{W}^{k - 1} \cup \set{i \le n_0 : \sstep[k]_i = 0}$\;
            Set the trial point~$\pstep[k] \gets \varrho(\mathcal{W}^k)$\; \nllabel{alg:nnls-trial-2}
        }
        Update~$\sstep[k + 1] \gets \pstep[k]$ and increment~$k$\;
    }
\end{algorithm}

The \gls{kkt} conditions at \cref{alg:nnls-kkt} of \cref{alg:nnls} are easy to verify in practice.
A point~$\sstep[k] \in \R^n$ satisfies the \gls{kkt} conditions for~\cref{eq:problem-nnls} if and only if~$\nabla \mu(\sstep[k]) \ge 0$ and
\begin{equation*}
    \frac{\partial \mu}{\partial \sstep_i} (\sstep[k]) = 0 \quad \text{if~$i > n_0$ or~$\sstep[k]_i > 0$.}
\end{equation*}
% When the algorithm terminates, an exact solution to~\cref{eq:problem-nnls} is obtained.
For more discussion on this algorithm, see~\cite[\S~23.3]{Lawson_Hanson_1987} and~\cite{Stark_Parker_1995}.

% This modification of the \gls{nnls} algorithm is implemented in \gls{cobyqa} under the function \texttt{cobyqa.linalg.nnls}.

% \subsubsection{Convergence of the method}

% We sketch below the convergence properties of \cref{alg:nnls}.
% Since the termination criteria of the outer loop are the \gls{kkt} conditions for problem~\cref{eq:problem-nnls}, the termination of the algorithm implies its convergence because the problem is convex and the constraints are linear.

% \paragraph{Termination of the inner loop}

% At each inner loop iteration, the size of the working set~$\mathcal{W}^k$ is enlarged.
% If~$\mathcal{W}^k$ is maximal, that is~$\mathcal{W}^k = \set{1, 2, \dots, n_0}$, then the condition at \cref{alg:nnls-inner} of \cref{alg:nnls} is always false, and the loop terminates.

% \paragraph{Termination of the outer loop}

% Termination of the outer loop can be directly inferred by showing that the value of~$\mu$ strictly decreases at each outer loop iteration.
% Such a condition ensures that the working set~$\mathcal{W}^k$ at a given outer loop iteration is different from all its previous instances, as each iterate of the outer loop is feasible\todo{Understand why and explain in the thesis} (see~\cite[Lem.~23.17]{Lawson_Hanson_1987} and the discussion below).

% When an outer loop iteration finishes, the value of~$\sstep[k + 1]$ solves
% \begin{align*}
%     \min_{\sstep \in \R^n}  & \quad \mu(\sstep)\\
%     \text{s.t.}             & \quad \sstep_i \ge 0, ~ i \in \set{1, 2, \dots, n_0},\\
%                             & \quad \sstep_i = 0, ~ i \in \mathcal{W}^k,
% \end{align*}
% If the \gls{kkt} conditions for problem~\cref{eq:problem-nnls} hold at~$\sstep[k + 1]$, then termination occurs.
% Otherwise, after incrementing~$k$, the index yielding the least value of~$\nabla \mu(\sstep[k])$ is removed from the working set~$\mathcal{W}^k$, and the tentative solution vector~$\pstep[k]$ clearly provides~$\mu(\pstep[k]) < \mu(\sstep[k])$.
% The result is then proven if no inner loop is entertained (that is, if condition at \cref{alg:nnls-inner} of \cref{alg:nnls} does not hold).
% Otherwise, at the end of each inner loop, we have
% \begin{align*}
%     \norm{A \sstep[k + 1] - b}  & = \norm[\big]{A (\sstep[k] + \alpha^k (\pstep[k] - \sstep[k])) - b}\\
%                                 & = \norm[\big]{(1 - \alpha^k) (A \sstep[k] - b) + \alpha^k (A \pstep[k] - b)}\\
%                                 & < \norm{A \sstep[k] - b},
% \end{align*}
% since~$\alpha^k \in (0, 1)$.
% The termination of the outer loop is then proven, as well as the convergence of the method.

% \begin{itemize}
%     \item How is it related to the original \gls{nnls} by considering the positive and negative parts?
% \end{itemize}

\section{Solving the geometry-improving subproblem}
\label{sec:cobyqa-geometry-improving}

We now detail how \gls{cobyqa} approximately solves the geometry-improving subproblem~\cref{eq:geometry-subproblem}.
Such a subproblem is of the form
\begin{subequations}
    \label{eq:geometry-simple}
    \begin{align}
        \max_{\sstep \in \R^n}  & \quad \abs{Q(\sstep)}\\
        \text{s.t.}             & \quad \xl \le \sstep \le \xu,\\
                                & \quad \norm{\sstep} \le \rad,
    \end{align}
\end{subequations}
where~$Q$ a is quadratic function, the bounds~$\xl \in (\R \cup \set{-\infty})^n$ and~$\xu \in (\R \cup \set{\infty})^n$ satisfy~$\xl \le 0$,~$\xu \ge 0$, and~$\xl < \xu$.
Note that these bounds are not the same as in~\cref{eq:problem-cobyqa}.

The method employed by \gls{cobyqa} is adopted from \gls{bobyqa}~\cite{Powell_2009}.
It computes two alternative approximations, and select the one that provides the larger absolute value of the denominator of the updating formula (see~\cref{subsec:geometry-improvement}).

The first alternative is a constrained Cauchy step.
More specifically, the method evaluates two steps, one for the minimization of~$Q(\sstep)$, one for the minimization of~$-Q(\sstep)$, and selects the one that provides the largest value of~$\abs{Q(\sstep)}$.
For the minimization of~$Q$, we first define a direction~$\sstep[c]$ by solving
\begin{subequations}
    \label{eq:cobyqa-geometry-cauchy}
    \begin{align}
        \min_{\sstep \in \R^n}  & \quad Q(0) + \nabla Q(0)^{\T} \sstep\\
        \text{s.t.}             & \quad \xl \le \sstep \le \xu,\\
                                & \quad \norm{\sstep} \le \rad.
    \end{align}
\end{subequations}
The considered step is then obtained by minimizing~$Q$ along~$\sstep[c]$ subject to the bounds and the trust-region constraint.
For the minimization of~$-Q$, we define the direction by solving an analog of~\cref{eq:cobyqa-geometry-cauchy}, with~$Q$ changed to~$-Q$, and then obtain the step by minimizing~$-Q$ along this direction subject to the constraints.

The second alternative is as follows.
Let~$\xpt$ be the current interpolation set.
Without any loss of generality, assume that~$0 \in \xpt$ is the best point so far.
The method minimizes~$\abs{Q(\sstep)}$ along the straight lines through~$0$ and the other interpolation points.
In other words, it computes
\begin{align*}
    \max_{\sstep \in \R^n}  & \quad \abs{Q(\sstep)}\\
    \text{s.t.}             & \quad \xl \le \sstep \le \xu,\\
                            & \quad \norm{\sstep} \le \rad,\\
                            & \quad \sstep \in \set{\alpha y : \alpha \in \R, ~ y \in \xpt}.
\end{align*}
This subproblem can be reformulated as~$\card(\xpt) - 1$ maximization problems in the variable~$\alpha$, which can be directly solved.
% This mechanism is described in~\cite{Powell_2008} for the unconstrained case, stating that adding this new constraint not only simplifies the computations, but also reduces the number of function evaluations in several experiments.

Another method that the author tried to approximately solve~\cref{eq:geometry-simple} is to employ \cref{alg:lctcg} on both~$Q$ and~$-Q$, taking the solution that provides the larger absolute value of~$Q$.
However, in our numerical experiments on \gls{cobyqa}, this strategy performs much worse than the one explained above.
This concurs with the observations of \citeauthor{Powell_2008}~\cite{Powell_2008} made on \gls{bobyqa}.
Therefore, we decide not to use this strategy in the implementation of \gls{cobyqa}.
% This concludes our discussion of the methods employed by \gls{cobyqa} to solve its subproblem.

\section{Summary and remarks}

We presented in this chapter the detailed methods employed by \gls{cobyqa} to solve its various subproblems.

To solve the tangential subproblem~\cref{eq:cobyqa-tangential} and the normal subproblem~\cref{eq:cobyqa-normal}, we apply active-set variations of the \gls{tcg} algorithm.
In particular, the tangential subproblem~\cref{eq:cobyqa-tangential} is either solved using the subproblem solver of \gls{bobyqa}~\cite{Powell_2009} or a that of \gls{lincoa}~\cite{Powell_2015} with a slight modification, depending on whether it admits linear constraints or not.
Moreover, we adapted the subproblem solver of \gls{lincoa} to approximately solve a reformulation of the normal subproblem.

We then presented a method for solving the least-squares problem~\cref{eq:least-squares-lagrange-multipliers-cobyqa} that defines the least-squares Lagrange multiplier.
This solver is a straightforward adaptation of the \gls{nnls} algorithm~\cite[Alg.~23.10]{Lawson_Hanson_1987} for nonnegative least-squares problems.
% The solution returned by this algorithm is exact.

Finally, we introduced the method employed by \gls{cobyqa} for solving the geometry-improving subproblem~\cref{eq:geometry-subproblem}.
This method is adopted from \gls{bobyqa}~\cite{Powell_2009}.
Two approximate solutions are calculated, the better one (according to some criterion) being returned.
