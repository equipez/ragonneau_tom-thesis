%% contents/introduction.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Introduction}

\nomenclature[Fa]{$\obj$}{Real-valued objective function defined on~$\R^n$}%
\nomenclature[Fb]{$\con{i}$}{Real-valued constraint function defined on~$\R^n$, with~$i \in \iub \cup \ieq$}%
\nomenclature[Fc]{$\lag$}{Lagrangian function}%
\nomenclature[Na]{$\in$}{Set membership notation}%
\nomenclature[Nb]{$\subseteq$}{Set inclusion notation}%
\nomenclature[Nc]{$\qedsymbol$}{Halmos symbol}%
\nomenclature[Nd]{$A^{\T}$,~$v^{\T}$}{Transpose of a matrix or a vector}%
\nomenclature[Ne]{$I_n$}{Identity matrix on~$\R^{n \times n}$}%
\nomenclature[Nf]{$e_k$}{Standard coordinate vector of~$\R^n$ ($k$-th column of~$I_n$), with~$1 \le k \le n$}%
\nomenclature[Ng]{$\mathcal{O}(\cdot)$}{Big-O notation}%
\nomenclature[Nh]{$o(\cdot)$}{Little-O notation}%
\nomenclature[Oa]{$[\cdot]_{+}$}{Elementwise positive-part operator}%
\nomenclature[Ob]{$[\cdot]_{-}$}{Elementwise negative-part operator}%
\nomenclature[Oc]{$\abs{\cdot}$}{Elementwise modulus operator}%
\nomenclature[Od]{$\inner{\cdot, \cdot}$}{Inner-product operator (may be subscripted for sake of clarity)}%
\nomenclature[Oe]{$\norm{\cdot}$}{Norm of a vector or a matrix (may be subscripted for sake of clarity)}%
\nomenclature[Of]{$\nabla$}{Gradient operator (elements~$\partial / \partial x_i$, with~$i \in \set{1, 2, \dots, n}$)}%
\nomenclature[Og]{$\nabla^2$}{Hessian operator (elements~$\partial^2 / \partial x_i \partial x_j$, with~$i, j \in \set{1, 2, \dots, n}$)}%
\nomenclature[Oh]{$\land$}{Logic and operator}%
\nomenclature[Sa]{$\emptyset$}{Empty set}%
\nomenclature[Sb]{$[a, b]$}{Closed set~$\set{x \in \R : a \le x \le b}$ with~$a \le b$}%
\nomenclature[Sc]{$(a, b)$}{Open set~$\set{x \in \R : a < x < b}$ with~$a < b$}%
\nomenclature[Sd]{$[a, b)$}{Semi-open set~$\set{x \in \R : a \le x < b}$ with~$a < b$}%
\nomenclature[Se]{$(a, b]$}{Semi-open set~$\set{x \in \R : a < x \le b}$ with~$a < b$}%
\nomenclature[Sf]{$\R$}{Set of real numbers}%
\nomenclature[Sg]{$\R^n$}{Real coordinate space of dimension~$n$}%
\nomenclature[Sh]{$\R^{m \times n}$}{Real matrix space of dimension~$m \times n$}%
\nomenclature[Si]{$\Omega$}{Feasible set, included in~$\R^n$}%
\nomenclature[Sj]{$\ieq$}{Set of indices of the equality constraints}%
\nomenclature[Sk]{$\iub$}{Set of indices of the inequality constraints}%
\todo[noline]{Replace the nomenclature items}

\section{Overview of \glsfmtlong{dfo}}

Optimization is the study of extremal points and values of mathematical functions.
It aims at minimizing (or maximizing) a real-valued function~$\obj$, referred to as the \emph{objective function}, within a given set of points~$\Omega \subseteq \R^n$, referred to as the \emph{feasible set}.
It is well known that essential information to optimization is embraced in the (possibly generalized) derivatives of the functions involved.
However, in practice, evaluations of such derivatives may be unreliable, or prohibitively expensive, if not impossible.
It motivates the study of \gls{dfo}~\cite{Conn_Scheinberg_Vicente_2009b,Audet_Hare_2017,Custodio_Scheinberg_Vicente_2017,Larson_Menickelly_Wild_2019}, where problems are solved using only function values.
This thesis focuses on methods and software for \gls{dfo}.

\Gls{dfo} problems arise naturally when the objective function or the feasible set results from complex experiments or simulations.
Regarding these functions as black boxes, people often refer to those problems as \gls{bbo} problems~\cite{Audet_Hare_2017}, which constitute a major type of \gls{dfo} problems in practice.
Note that \gls{dfo} differs from nonsmooth optimization~\cite{Clark_1983,Cui_Pang_2021}, which studies problems that involve nonsmooth functions.
In \gls{dfo}, the major difficulty is not the possible nonsmoothness of the functions involved, but the lack of knowledge about the structures of the problems.
In theoretical analysis of \gls{dfo} methods, it is not uncommon to assume that the underlying functions enjoy some smoothness, although algorithms cannot retrieve their (classical or generalized) derivatives.
We emphasize that if any derivative information can be evaluated at an affordable cost or approximated well enough, \gls{dfo} methods are not recommended, as they are very unlikely to outperform methods that use derivatives.
Consider for example minimizing an objective function defined by a sophisticated simulation whose source code is available.
One may then attempt to evaluate derivatives using automatic differentiation tools~\cite{Griewank_2003,Griewank_Walther_2008} and apply derivative-based methods.

For \gls{dfo} methods, the leading complexity measure we consider is the number of function evaluations.
In practice, each function evaluation may require several minutes or even several days to complete~\cite[\S~1.4]{Audet_Hare_2017}.
For instance, a recent application of \gls{dfo} is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}, for which every objective function evaluation necessitates training a machine learning model (see \cref{subsec:machine-learning}).
Hence, in \gls{dfo} methods, the expense of numerical linear algebra is less of a concern, although we will maintain it acceptable.

In this introduction, we consider the nonlinearly-constrained problem
\begin{subequations}
    \label{eq:problem-introduction}
    \begin{align}
        \min        & \quad \obj(x)\\
        \text{s.t.} & \quad \con{i}(x) \le 0, ~ i \in \iub, \label{eq:problem-introduction-cub}\\
                    & \quad \con{i}(x) = 0, ~ i \in \ieq, \label{eq:problem-introduction-ceq}\\
                    & \quad x \in \R^n, \nonumber
    \end{align}
\end{subequations}
where the \emph{objective} and \emph{constraint functions}~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are real-valued functions on~$\R^n$, and where the sets of indices~$\iub$ and~$\ieq$ are finite (perhaps empty) and disjoint.
The feasible set of this problem is
\begin{equation*}
    \Omega \eqdef \set{x \in \R^n : \con{i}(x) \le 0 ~ \text{for~$i \in \iub$}, ~ \con{i}(x) = 0 ~ \text{for~$i \in \ieq$}}.
\end{equation*}

We emphasize that~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq} may include bound constraints.
We do not extract them explicitly in this chapter, but they may need to be handled differently from other constraints, because they often represent inalienable physical or theoretical restrictions.
We will later in this thesis introduce a new \gls{dfo} method, namely \gls{cobyqa}, which will take this into consideration (see \cref{ch:cobyqa-introduction}).

\section{Examples of applications}

\subsection{Automatic error analysis}

A typical example of \gls{dfo} applications is automatic error analysis~\cite{Higham_1993,Higham_2002}, which formulates numerical computation's accuracies and stabilities using optimization problems.
Consider, for instance, the Gaussian elimination with partial pivoting of a matrix~$A \in \R^{n \times n}$, given in \cref{alg:gaussian-elimination}, where the superscripts denote iteration numbers.

\begin{algorithm}[ht]
    \caption{Gaussian elimination with partial pivoting}
    \label{alg:gaussian-elimination}
    \DontPrintSemicolon
    \KwData{Matrix~$A \in \R^{n \times n}$.}
    Initialize $A^{(0)} \gets A$\;
    \For{$k = 1, 2, \dots, n - 1$}{
        Determine the pivot index~$j = \argmax \set[\big]{\abs[\big]{A_{i, k}^{(k - 1)}} : k \le i \le n}$\;
        \eIf{$A_{j, k}^{(k - 1)} \neq 0$}{
            Exchange the~$k$th and the~$j$th rows of~$A^{(k - 1)}$\;
            Evaluate the multiplier~$\tau^k \in \R^n$ with components
            \begin{algoempheq}[left={\tau_i^k = \empheqlbrace}]{alignat*=2}
                & A_{i, k}^{(k - 1)} / A_{k, k}^{(k - 1)}   && \quad \text{if~$i > k$, and}\\
                & 0                                         && \quad \text{otherwise}
            \end{algoempheq}
            Update~$A^{(k)} \gets (I_n - \tau^k e_k^{\T})A^{(k - 1)}$\;
        }{
            Set~$A^{(k)} \gets A^{(k - 1)}$\;
        }
    }
\end{algorithm}

\todo[noline]{Write down explicitly the conventions on the subscripts and superscripts.}

\Citeauthor{Wilkinson_1963}'s backward error analysis (see, e.g., equation~(25.14) of chapter~3 in~\cite{Wilkinson_1963}, where~$t$ is introduced at the beginning of paragraph~10, and~$g$ at the end of page~97) demonstrates that the growth factor of the Gaussian elimination, defined by
\begin{equation}
    \label{eq:gaussian-elimination-growth-factor}
    \rho_n(A) \eqdef \frac{\max_{0 \le k \le n - 1} \norm{A^{(k)}}_{\max}}{\norm{A}_{\max}},
\end{equation}
determinates the numerical stability of \cref{alg:gaussian-elimination}, where~$\norm{\cdot}_{\max}$ denotes the max norm of a matrix, i.e., the largest absolute value of the matrix's entries.
More specifically, the~$\ell_{\infty}$-norm of the backward error of the computed solution is bounded from above by a term proportional to~$\rho_n(A)$.
To study the worst-case scenario, we wish to determine how large~$\rho_n$ can be and hence, to solve
\begin{equation}
    \label{eq:gaussian-elimination-problem}
    \max_{A \in \R^{n \times n}} \rho_n(A).
\end{equation}
Note that~$\R^{n \times n}$ is isomorphic to~$\R^{n^2}$ and hence, problem~\cref{eq:gaussian-elimination-problem} can straightforwardly be formulated as problem~\cref{eq:problem-introduction}.
Besides, although the growth factor is defined everywhere, it may not be continuous at the points yielding a tie in the selection of the pivot element.
Moreover, it is not differentiable at the points yielding a tie in any maximum operator in equation~\cref{eq:gaussian-elimination-growth-factor}.
Hence, optimization methods based on derivative information cannot be used for this problem.
In such a case, \gls{dfo} methods can help solving problem~\cref{eq:gaussian-elimination-problem}.
Note that the optimal value and all optimal solutions to problem~\cref{eq:gaussian-elimination-problem} are known~\cite{Higham_Higham_1989}, but \gls{dfo} methods can be used to help the theoretical development~\cite{Higham_1993}.

\subsection{Tuning nonlinear optimization methods}

Another well-known example of \gls{dfo} applications is the parameter tuning of nonlinear optimization methods~\cite{Audet_Orban_2006}.
For example, consider \cref{alg:trust-region}, a basic trust-region method for solving problem~\cref{eq:problem-introduction} when~$\iub = \ieq = \emptyset$.

\begin{algorithm}[ht]
    \caption{Basic trust-region method for unconstrained optimization}
    \label{alg:trust-region}
    \DontPrintSemicolon
    \KwData{Objective function~$\obj$, initial guess~$x^0 \in \R^n$, initial trust-region radius~$\Delta_0 > 0$, and parameters~$0 < \eta_1 \le \eta_2 < 1$ and~$0 < \theta_1 < 1 < \theta_2$.}
    \For{$k = 0, 1, \dots$}{
        Define a simple function~$m_k$ such that~$m_k(d) \approx f(x^k + d)$ for~$\norm{d} \le \Delta_k$\;
        Set the trial step~$d^k$ to an approximate solution to
        \begin{algomathdisplay}
            \begin{aligned}
                \min        & \quad m_k(d)\\
                \text{s.t.} & \quad \norm{d} \le \Delta_k,\\
                            & \quad d \in \R^n
            \end{aligned}
        \end{algomathdisplay}
        Evaluate the trust-region ratio
        \begin{algomathdisplay}
            \rho_k \gets \frac{\obj(x^k) - \obj(x^k + d^k)}{m_k(0) - m_k(d^k)}
        \end{algomathdisplay}
        \eIf{$\rho_k \ge \eta_1$}{ \nllabel{alg:trust-region-success}
            Update the trial point~$x^{k + 1} \gets x^k + d^k$\;
        }{
            Retain the trial point~$x^{k + 1} \gets x^k$\;
        }
        Update the trust-region radius
        \begin{algoempheq}[left={\Delta_{k + 1} \gets \empheqlbrace}]{alignat*=2}
            & \theta_1 \Delta_k && \quad \text{if~$\rho_k \le \eta_1$,}\\
            & \Delta_k          && \quad \text{if~$\eta_1 < \rho_k \le \eta_2$, and}\\
            & \theta_2 \Delta_k && \quad \text{otherwise}
        \end{algoempheq}
    }
\end{algorithm}

The most important simplification in \cref{alg:trust-region} lies in \cref{alg:trust-region-success}.
A complete framework includes a parameter~$\eta_0 \ge 0$ satisfying~$\eta_0 \le \eta_1$, and the condition in \cref{alg:trust-region-success} is replaced by~$\rho_k \ge \eta_0$.
In practice, we usually have~$\eta_0 = 0$.
However, this parameter genuinely complexifies the theoretical analysis of the trust-region method, and hence, we omit it here for sake of simplicity.
We consider only the four parameters~$\eta_1$,~$\eta_2$,~$\theta_1$, and~$\theta_2$.
To choose those parameters, we minimize some measure of the method's expense (e.g., the \glsxtrshort{cpu} time to solve a given set of optimization problems),~$C$ say.
In other words, we wish to solve the optimization problem
\begin{subequations}
    \label{eq:tuning-algorithms-problem}
    \begin{align}
        \min        & \quad C(\eta_1, \eta_2, \theta_1, \theta_2)\\
        \text{s.t.} & \quad 0 \le \eta_1 \le \eta_2 < 1,\\
                    & \quad 0 < \theta_1 < 1 < \theta_2.
    \end{align}
\end{subequations}
Derivatives of~$C$ cannot be evaluated if they even exist.
Such a problem is then solved using \gls{dfo} methods, for example using the \gls{mads} method~\cite{Audet_Orban_2006}.
\Citeauthor{Audet_Digabel_Tribes_2019} modified the \gls{mads} method to solve problem~\cref{eq:tuning-algorithms-problem} with a controlled number of significant digits~\cite{Audet_Digabel_Tribes_2019}, to determine parameters that can be used by practitioners.
Interestingly, \gls{dfo} methods can be self-tuned using the method presented above.
The \gls{bfo} solver~\cite{Porcelli_Toint_2017}, a method for bound-constrained problems mixing continuous and discrete variables, is an example of self-tuned \gls{dfo} methods.

\subsection{Hyperparameter tuning in machine learning}
\label{subsec:machine-learning}

A more recent example of \gls{dfo} applications is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}.
For instance, Google solves hyperparameter tuning problems using Google Vizier~\cite{Golovin_Etal_2017}, the Google-internal service for performing black-box optimization.
To illustrate this example, we consider the following hyperparameter tuning problem of a \gls{svm} for binary classification.
Given a binary-labeled dataset~$\set{(x_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$, we build a \gls{svm} to classify the data with their respective labels.
A binary classification is obtained using a~$C$-\gls{svc}~\cite{Chang_Lin_2011} by solving the optimization problem
\begin{subequations}
    \label{eq:csvc}
    \begin{align}
        \min        & \quad \frac{1}{2} \norm{\omega}_2^2 + C \norm{\xi}_1\\
        \text{s.t.} & \quad y_i (\beta + \inner{\omega, \varphi_{\gamma}(x_i)}) \ge 1 - \xi_i, ~ i \in \set{1, 2, \dots, m},\\
                    & \quad \xi \ge 0,\\
                    & \quad (\omega, \beta, \xi) \in \R^{\ell} \times \R \times \R^m,
    \end{align}
\end{subequations}
where~$\varphi_{\gamma}$ is a function mapping the data to a higher-dimensional space~$\R^{\ell}$ and~$\gamma > 0$ and~$C > 0$ are parameters.
Given a solution~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast}) \in \R^{\ell} \times \R \times \R^m$ to problem~\cref{eq:csvc}, the~$C$-\gls{svc} classifies any data~$x \in \R^n$ according to
\begin{equation}
    \label{eq:csvc-classifier}
    \delta(x) \eqdef \sgn(\beta^{\ast} + \inner{\omega^{\ast}, \varphi_{\gamma}(x)}),
\end{equation}
which maps an observation~$x \in \R^n$ to a label in~$\set{\pm 1}$.
It is clear that~$\delta$ depends on the two parameters~$C$ and~$\gamma$, which can be chosen by solving an optimization problem.
The objective function~$P$ of this problem is a~$5$-fold cross-validation based on some performance measure of the model~\cref{eq:csvc-classifier}.
The general~$k$-fold cross-validation to define~$P(C, \gamma)$ is presented in \cref{alg:cross-validation}.

\begin{algorithm}[ht]
    \caption{$k$-fold cross-validation of an \glsfmtshort{svc} with parameters~$C$ and~$\gamma$}
    \label{alg:cross-validation}
    \DontPrintSemicolon
    \KwData{Labeled dataset~$\set{(x_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$ and fold number~$k > 0$.}
    Split the dataset into~$k$ balanced groups\;
    \For{$i = 1, 2, \dots, k$}{
        Calculate~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast})$ with the all the data except those in the~$i$th group\;
        Evaluate the performance~$p_i$ of the model~\cref{eq:csvc-classifier} on the data in the~$i$th group\;
    }
    Define~$P(C, \gamma)$ by summarizing the performances~$\set{p_1, p_2, \dots, p_k}$\;
\end{algorithm}

A typical example of model's performance used in the~$k$-fold cross-validation is the model's accuracy, i.e., the percentage of data correctly classified.
The~\gls{auc}~\cite{Hanley_Mcneil_1982} is another example of performance measure, particularly effective for imbalanced datasets~\cite{Bradley_1997}.
The hyperparameter tuning problem can be formulated as
\begin{equation*}
    \begin{aligned}
        \min        & \quad P(C, \gamma)\\
        \text{s.t.} & \quad C > 0,\\
                    & \quad \gamma > 0.
    \end{aligned}
\end{equation*}
It is clear that derivatives of the objective function of such a problem cannot be easily evaluated and may even not exist.
This problem may be solved using \gls{dfo} methods.

As in~\cite{Qian_Yu_2021}, \gls{dfo} be also be applied to reinforcement learning.
Instead of training a model on a fixed labelled dataset, reinforcement learning bases the training process on rewarding expected behaviors and punishing undesired ones.
Hence, it often consists in finding optimal parameters that maximize a reward.
However, these reward's derivatives often cannot be evaluated, and \gls{dfo} methods can be an approach to solving such problems.
This concept is often referred to as derivative-free reinforcement learning.


\subsection{Some industrial and engineering applications}

\Gls{dfo} methods are also widely used in industry and engineering, especially for solving problems that involve heavy simulations.
Such problems arise from helicopter rotor blade manufacturing~\cite{Booker_Etal_1998a,Booker_Etal_1998b,Serafini_1998}, aeroacoustic shape design~\cite{Marsden_2004,Marsden_Etal_2004}, computational fluid dynamics~\cite{Duvigneau_Visonneau_2004}, worst-case analysis of analog circuit~\cite{Latorre_Etal_2019}, rapid-cycling synchrotron accelerator modeling~\cite{Eldred_Etal_2021}, nuclear energy engineering~\cite{Kortelainen_Etal_2010,Kortelainen_Etal_2012,Kortelainen_Etal_2014}, reservoir engineering and engine calibration~\cite{Langouet_2011}, or groundwater supply and bioremediation engineering~\cite{Fowler_Etal_2008,Mugunthan_Shoemaker_Regis_2005,Yoon_Shoemaker_1999}, to name but a few.
More generally, industrial and engineering problems that involve sophisticated models, simulations, or experiments, give rise to \gls{dfo} problems.

A particular application of \gls{dfo} comes from \gls{mdo}.
It is a field that uses optimization methods to solve design problems defined by multiple disciplines.
The objective and constraint functions of a \gls{mdo} problem can be provided by different departments of the same company or even by different companies.
This is the case in aircraft engine engineering~\cite{Gazaix_Etal_2019}, where the design problem of one component is solved while taking into account constraints imposed by other components handled by different departments.
\Gls{mdo} problems often involve simulations or experiments and therefore, \gls{mdo} problems are solved using \gls{dfo} methods.
We will present in \cref{ch:pdfo} of this thesis a piece of software we implemented for solving \gls{dfo} problems.
It has been included in GEMSEO~\cite{Gallard_Etal_2018}, an engine for \gls{mdo} initiated by a team from IRT Saint Exup{\'{e}}ry in France.

\section{Optimality conditions for smooth optimization}

We discuss in this section necessary and sufficient conditions for optimality.
We do not assume any structure on the objective and constraint functions, except some smoothness.
More specialized results can be obtained by assuming that problem~\cref{eq:problem-introduction} is convex for example, but it is out of the scope of this work.

\subsection{Local and global solutions}

Before solving problem~\cref{eq:problem-introduction}, we must define what a solution is.
\Cref{def:global-solution} establishes the most natural understanding of a solution.

\begin{definition}[Global solution]
    \label{def:global-solution}
    To problem~\cref{eq:problem-introduction}, a point~$x^{\ast} \in \R^n$ is a \emph{global solution} if~$x^{\ast} \in \Omega$ and~$\obj(x) \ge \obj(x^{\ast})$ for all~$x \in \Omega$.
\end{definition}

However, it is known that finding a global solution to a nonconvex nonlinear problem is generally NP-hard~\cite{Murty_Kabadi_1987}.
Examples of such problems include reformulating the subset sum problem as an optimization problem.
There also exist convex optimization problems known to be NP-hard, such as copositive programming.
\Cref{def:local-solution} provides another understanding of solutions to problem~\cref{eq:problem-introduction}.

\begin{definition}[Local solution]
    \label{def:local-solution}
    To problem~\cref{eq:problem-introduction}, a point~$x^{\ast} \in \R^n$ is
    \begin{itemize}
        \item a \emph{local solution} if~$x^{\ast} \in \Omega$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$\obj(x) \ge \obj(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega$,
        \item a \emph{strict local solution} if~$x^{\ast} \in \Omega$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$\obj(x) > \obj(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega \setminus \set{x^{\ast}}$, and
        \item an \emph{isolated local solution} if there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that it is the only local solution in~$\mathcal{N} \cap \Omega$.
    \end{itemize}
\end{definition}

Note that an isolated local solution is strict, but the converse is not true.
From now on, we refer to as an \emph{optimal solution} to problem~\cref{eq:problem-introduction} a local solution, since we do not study global optimization.
The value of the objective function~$\obj$ at an optimal solution is referred to as the \emph{optimal value}.
However, we note that the numerical methods we will consider in this thesis do \emph{not} attempt to find any local solution to~\cref{eq:problem-introduction}, as such problem is also NP-hard in general~\cite{Murty_Kabadi_1987}.
In fact, asserting that a given point is a local solution is already NP-hard for many optimization problems.
Therefore, numerical methods attempt to find first- or second-order stationary points, presented hereinafter.

\subsection{Constraint qualifications}

Before introducing any necessary and sufficient conditions for local optimality, we discuss some regularity conditions on the constraints~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq}, referred to as the \emph{constraint qualifications}.
These assumptions will be required for the necessary and the sufficient conditions to hold.
We first introduce the notion of \emph{active set}.

\begin{definition}[Active set]
    The \emph{active set}~$\mathcal{A}(x) \subseteq \iub \cup \ieq$ for problem~\cref{eq:problem-introduction} at a point~$x \in \Omega$ is defined by
    \begin{equation*}
        \mathcal{A}(x) \eqdef \ieq \cup \set{i \in \iub : \con{i}(x) \ge 0}.
    \end{equation*}
\end{definition}

If a constraint belong to the active set at a given point, it is said to be \emph{active} at this point, and \emph{inactive} otherwise.
Note that a violated constraint is always considered active.
We introduce hereinafter two classical constraint qualifications.

\begin{definition}[Constraint qualification]
    Given~$x \in \Omega$, denote~$\mathcal{A}(x)$ the active set for problem~\cref{eq:problem-introduction} at~$x$, and assume that the constraint functions~$\con{i}$ are differentiable at~$x$ for all~$i \in \mathcal{A}(x)$.
    We say that
    \begin{itemize}
        \item the \gls{licq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \mathcal{A}(x)$, and
        \item the \gls{mfcq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \ieq$ and there exists a vector~$z \in \R^n$ such that
        \begin{subequations}
            \label{eq:mangasarian-fromovitz}
            \begin{empheq}[left=\empheqlbrace]{alignat=2}
                & \inner{\nabla \con{i}(x), z} < 0  && \quad \text{if~$i \in \mathcal{A}(x) \cap \iub$, and}\\
                & \inner{\nabla \con{i}(x), z} = 0  && \quad \text{if~$i \in \ieq$}.
            \end{empheq}
        \end{subequations}
    \end{itemize}
\end{definition}

\Cref{prop:licq-implies-mfcq} shows that although \gls{licq} is simpler to verify in practice, it is stronger than the \gls{mfcq}.

\begin{proposition}
    \label{prop:licq-implies-mfcq}
    Given~$x \in \Omega$, if the \gls{licq} holds at~$x$, then the \gls{mfcq} holds at~$x$.
\end{proposition}

\begin{proof}
    Let~$\mathcal{A}(x)$ be the active set for problem~\cref{eq:problem-introduction} at~$x$, and let~$\Gamma(x)$ be the real matrix with rows~$\nabla \con{i}(x)^{\T}$ for~$i \in \mathcal{A}(x)$.
    According to the \gls{licq}, the system~$\Gamma(x) z = v$ must have a solution for any right-hand side~$v$.
    Let, therefore,~$v$ be the vector defined by
    \begin{empheq}[left={v_i = \empheqlbrace}]{alignat*=2}
        & -1    && \quad \text{if~$i \in \mathcal{A}(x) \cap \iub$, and}\\
        & 0     && \quad \text{if~$i \in \ieq$},
    \end{empheq}
    and let~$z \in \R^n$ satisfy~$\Gamma(x) z = v$.
    Then~$z$ also satisfy the system~\cref{eq:mangasarian-fromovitz}.
\end{proof}

Many other constraint qualifications exist.
Some of the weakest constraint qualifications used in theoretical analyses are the \gls{acq} and the \gls{gcq}.
They are defined using tangent and linear cones and are hard to verify in practice.
There also exist several traditional constraint qualifications weaker than the \gls{mfcq}, such as the \gls{crcq}, the \gls{cpld}, or the \gls{qncq}.
However, considering such constraint qualifications usually complexifies theoretical analyses.
Therefore, we will consider the \gls{licq} and the \gls{mfcq} in this thesis.
We also note that when the problem has a particular structure, such as convexity, dedicated constraint qualifications may exist, such as the \gls{sc}.
However, we do not assume any particular structure in this thesis.

\subsection{First-order optimality conditions}

\subsubsection{Statement of the optimality conditions}

Let~$\lag$ be the \emph{Lagrangian function} to problem~\cref{eq:problem-introduction}, defined by
\begin{equation*}
    \lag(x, \lambda) \eqdef \obj(x) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lambda_i \con{i}(x),
\end{equation*}
where~$\lambda = (\lambda_i)_{i \in \iub \cup \ieq}$ with~$\lambda_i \in \R$ for all~$i \in \iub \cup \ieq$.
\Cref{thm:first-order-necessary-conditions} introduces the first-order necessary conditions for a point to be a local solution of problem~\cref{eq:problem-introduction}.

\begin{theorem}[First-order necessary conditions~\cite{Nocedal_Wright_2006}] % Theorem 12.1
    \label{thm:first-order-necessary-conditions}
    Let~$x^{\ast} \in \Omega$ be a local solution to problem~\cref{eq:problem-introduction}, assume that the functions~$\obj$ and~$\con{i}$, for all~$i \in \iub \cup \ieq$, are continuously differentiable in a neighborhood of~$x^{\ast}$, and that \gls{licq} holds at~$x^{\ast}$.
    Then there exists a Lagrange multiplier~$\lambda^{\ast} = (\lambda_i^{\ast})_{i \in \iub \cup \ieq}$ with~$\lambda_i^{\ast} \in \R$ for all~$i \in \iub \cup \ieq$ such that
    \begin{subequations}
        \label{eq:kkt-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=2}
            & \nabla_x \lag(x^{\ast}, \lambda^{\ast}) = 0,  && \label{eq:kkt-introduction-stationarity}\\
            & \con{i}(x^{\ast}) \le 0,                      && \quad \text{if~$i \in \iub$,} \label{eq:kkt-introduction-primal-feasibility-ub}\\
            & \con{i}(x^{\ast}) = 0,                        && \quad \text{if~$i \in \ieq$,} \label{eq:kkt-introduction-primal-feasibility-eq}\\
            & \lambda_i^{\ast} \con{i}(x^{\ast}) = 0,       && \quad \text{if~$i \in \iub$,} \label{eq:kkt-introduction-complementary-slackness}\\
            & \lambda_i^{\ast} \ge 0,                       && \quad \text{if~$i \in \iub$.} \label{eq:kkt-introduction-dual-feasibility}
        \end{empheq}
    \end{subequations}
\end{theorem}

The conditions~\cref{eq:kkt-introduction} are commonly referred to as the \gls{kkt} conditions~\cite{Karush_1939,Kuhn_Tucker_1951}.
More specifically, condition~\cref{eq:kkt-introduction-stationarity} is referred to as the \emph{stationarity} condition, conditions~\cref{eq:kkt-introduction-primal-feasibility-ub,eq:kkt-introduction-primal-feasibility-eq} as the \emph{primal feasibility} conditions, condition~\cref{eq:kkt-introduction-complementary-slackness} as the \emph{complementary slackness} condition, and condition~\cref{eq:kkt-introduction-dual-feasibility} as the \emph{dual feasibility} condition.
Note that this theorem holds for more general constraint qualifications, such as the \gls{mfcq} (see, e.g.,~\cite[p.~339]{Nocedal_Wright_2006}).
Any point~$x \in \Omega$ is referred to as a \emph{first-order stationary point} if it satisfies the \gls{kkt} conditions~\cref{eq:kkt-introduction}.
We emphasize however that a first-order stationary point may not be a local solution as \cref{thm:first-order-necessary-conditions} does not state any sufficient condition.

\subsubsection{Graphical insight of the first-order optimality conditions}

We do not provide a proof of \cref{thm:first-order-necessary-conditions}, but we illustrate graphically the main idea on the simple~$2$-dimensional example
\begin{subequations}
    \label{eq:kkt-description}
    \begin{align}
        \min        & \quad \obj(x) = x_1 + x_2\\
        \text{s.t.} & \quad \con{1}(x) = x_1^2 + x_2^2 - 2 \le 0, \label{eq:kkt-description-c1}\\
                    & \quad \con{2}(x) = -x_2 \le 0, \label{eq:kkt-description-c2}\\
                    & \quad x \in \R^2, \nonumber
    \end{align}
\end{subequations}
whose solution is~$x^{\ast} = (-\sqrt{2}, 0)$.
A graphical representation of problem~\cref{eq:kkt-description} is given in \cref{fig:kkt-description}, where the white area represents the feasible set.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[%
            xmin=-5,%
            xmax=2,%
            ymin=-2,%
            ymax=2,%
            axis equal image,%
            xlabel={$x_1$},%
            ylabel={$x_2$},%
            axis background/.style={%
                pattern=north west lines,%
                pattern color=black!20,%
                even odd rule,%
                insert path={let \p1=(axis cs:0,0), \p2=(axis cs:2^0.5,0), \n1={veclen(\x2-\x1,\y2-\y1)}, in (\p2) arc(0:180:\n1) -- cycle},%
            },%
        ]
            \draw[dashed] (0,0) circle[radius=2^0.5];
            \draw[dashed] (-5,0) -- (2,0);
            \draw[-latex] (-2^0.5,0) -- (-3*2^0.5,0) node[above right] {$\nabla \con{1}(x^{\ast})$};
            \draw[-latex] (-2^0.5,0) -- (-2^0.5,-1) node[below left] {$\nabla \con{2}(x^{\ast})$};
            \draw[-latex] (-2^0.5,0) -- (1-2^0.5,1) node[below right] {$\nabla \obj(x^{\ast})$};
            \addplot[BrickRed,mark=*,only marks] coordinates {(-2^0.5,0)};
            \node[above left] at (-2^0.5,0) {$x^{\ast}$};
        \end{axis}
    \end{tikzpicture}
    \caption{Graphical representation of problem~\cref{eq:kkt-description}}
    \label{fig:kkt-description}
\end{figure}

As manifest on \cref{fig:kkt-description}, there is no direction~$d \in \R^2$ such as
\begin{subequations}
    \label{eq:kkt-proof}
    \begin{empheq}[left=\empheqlbrace]{alignat=1}
        & \inner{\nabla \obj(x^{\ast}), d} < 0,\\
        & \inner{\nabla \con{1}(x^{\ast}), d} \le 0,\\
        & \inner{\nabla \con{2}(x^{\ast}), d} \le 0.
    \end{empheq}
\end{subequations}
This is because the direction~$d$ would be feasible and would progress towards a point with a least objective function value.
The constraint qualification assumption is necessary to prove this statement.
The Farkas' lemma~\cite{Farkas_1902} shows actually that no vector~$d \in \R^2$ satisfy system~\cref{eq:kkt-proof} only when there exists a nonnegative Lagrange multiplier~$\lambda^{\ast} = (\lambda_1^{\ast}, \lambda_2^{\ast})$ such that~$\nabla_x \lag(x^{\ast}, \lambda^{\ast}) = 0$, since
\begin{equation*}
    \nabla_x \lag(x^{\ast}, \lambda^{\ast}) = \nabla \obj(x^{\ast}) + \lambda_1^{\ast} \nabla \con{i}(x^{\ast}) + \lambda_1^{\ast} \nabla \con{i}(x^{\ast}).
\end{equation*}
For a formal proof of \cref{thm:first-order-necessary-conditions}, see, e.g.,~\cite[\S~12.4]{Nocedal_Wright_2006}.

\subsection{Second-order optimality conditions}

It is known that at a local solution of a smooth unconstrained optimization problem, the gradient of the objective function is zero and its Hessian matrix is positive semidefinite.
\Cref{thm:second-order-necessary-conditions} generalizes this fact for the constrained optimization problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order necessary conditions~\cite{Nocedal_Wright_2006}] % Theorem 12.5
    \label{thm:second-order-necessary-conditions}
    Let~$x^{\ast} \in \Omega$ be a local solution to problem~\cref{eq:problem-introduction}, assume that the functions~$\obj$ and~$\con{i}$, for all~$i \in \iub \cup \ieq$, are twice continuously differentiable in a neighborhood or~$x^{\ast}$, and that \gls{licq} holds at~$x^{\ast}$.
    Denote~$\mathcal{A}(x^{\ast})$ the active set for problem~\cref{eq:problem-introduction} at~$x^{\ast}$.
    Let~$\lambda^{\ast} = (\lambda_i^{\ast})_{i \in \iub \cup \ieq}$ with~$\lambda_i^{\ast} \in \R$ for all~$i \in \iub \cup \ieq$ be a Lagrange multiplier satisfying the KKT condition~\cref{eq:kkt-introduction}, and let~$z \in \R^n$ be any vector such that
    \begin{subequations}
        \label{eq:second-order-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=2}
            & \inner{\nabla \con{i}(x^{\ast}), z} = 0,      && \quad \text{if~$i \in \ieq$,}\\
            & \inner{\nabla \con{i}(x^{\ast}), z} = 0,      && \quad \text{if~$i \in \mathcal{A}(x^{\ast}) \cap \iub$ and~$\lambda_i^{\ast} > 0$, and}\\
            & \inner{\nabla \con{i}(x^{\ast}), z} \ge 0,    && \quad \text{if~$i \in \mathcal{A}(x^{\ast}) \cap \iub$ and~$\lambda_i^{\ast} = 0$.}
        \end{empheq}
    \end{subequations}
    Then~$\inner{z, \nabla_{x, x}^2 \lag(x^{\ast}, \lambda^{\ast}) z} \ge 0$.
\end{theorem}

A first-order stationary point~$x \in \Omega$ is referred to as a \emph{second-order stationary point} if it satisfies the conclusion of \cref{thm:second-order-necessary-conditions}.
We emphasize again that a second-order stationary point may not be a local solution as \cref{thm:second-order-necessary-conditions} does not state any sufficient condition.
However, it is known in smooth unconstrained optimization that a point at which the gradient of the objective function is zero and its Hessian matrix is positive definite is a strict local solution.
\Cref{thm:second-order-sufficient-conditions} generalizes this fact for the constrained optimization problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order sufficient conditions~\cite{Nocedal_Wright_2006}] % Theorem 12.6
    \label{thm:second-order-sufficient-conditions}
    Let~$x^{\ast} \in \Omega$ be a given point, and assume that the functions~$\obj$ and~$\con{i}$, for all~$i \in \iub \cup \ieq$, are twice continuously differentiable in a neighborhood of~$x^{\ast}$, that \gls{licq} holds at~$x^{\ast}$, and that there exists a Lagrange multiplier~$\lambda^{\ast} = (\lambda_i^{\ast})_{i \in \iub \cup \ieq}$ with~$\lambda_i^{\ast} \in \R$ for all~$i \in \iub \cup \ieq$ satisfying the KKT condition~\cref{eq:kkt-introduction}.
    If for all nonzero vector~$z \in \R^n \setminus \set{0}$ satisfying the conditions~\cref{eq:second-order-introduction} we have~$\inner{z, \nabla_{x, x}^2 \lag(x^{\ast}, \lambda^{\ast}) z} > 0$, then~$x^{\ast}$ is a strict local solution to problem~\cref{eq:problem-introduction}.
\end{theorem}

However, as of today, no necessary and sufficient condition is known for a point to be a local solution to problem~\cref{eq:problem-introduction} in the general case.

\section{Methodology of \glsfmtlong{dfo}}

As summarized in~\cite{Conn_Scheinberg_Vicente_2009b}, two main strategies have been developed for solving \gls{dfo} problems.
One strategy consists of sampling the objective function around the current iterate and choosing the next iterate among the sampled points based on simple comparisons.
The \emph{direct-search} methods~\cite{Kolda_Lewis_Torczon_2003} are based on this framework.
The other strategy builds iteratively models that approximate the problems (e.g., using polynomials) around the current iterate and choose the next iterate according to the approximated problems.
These methods are referred to as \emph{model-based} methods.
Hybrid methods also exist, such as implicit filtering~\cite{Kelley_2011}, and \gls{sidpsm}~\cite{Custodio_Rocha_Vicente_2009}, which combine direct search and models.

Some methods can solve \gls{dfo} problems but are not covered by the above categories.
Examples include random search methods~\cite{Zhigljavsky_1991}, simulated annealing methods~\cite{Kirkpatrick_Gelatt_Vecchi_1983}, the genetic algorithm~\cite{Jong_1975,Holland_1975}, and Bayesian optimization methods~\cite{Mockus_1975,Shahriari_Etal_2016}.
For more information on these methods, we refer to the review~\cite{Larson_Menickelly_Wild_2019} and the reference therein.

\subsection{Direct-search methods}

An early example of \gls{dfo} is a method from \citeauthor{Fermi_Metropolis_1952}~\cite{Fermi_Metropolis_1952}, who developed in \citeyear{Fermi_Metropolis_1952} a nonlinear least-squares solver on MANIAC, an computer based on the von Neumann architecture.
From a modern viewpoint, this method is a coordinate search method, a particular example of direct-search methods, where the search directions are defined to be the coordinate axes.

Several direct-search methods appeared to solve general and dedicated optimization problems in the next few years.
\citeauthor{Rosenbrock_1960}, for example, designed an unconstrained direct-search method, which chooses the search directions among linear combinations of previous steps that led to actual reductions of the objective function~\cite{Rosenbrock_1960}.
Another famous early direct-search method is the Hooke-Jeeves method~\cite{Hooke_Jeeves_1961}.
This method combines exploratory moves toward coordinates axes with pattern moves, aiming to reuse the previously computed directions.
In \citeyear{Nelder_Mead_1965}, \citeauthor{Nelder_Mead_1965} introduced a method for solving unconstrained optimization problems, which evaluates the objective function at the vertices of a given simplex, and modifies this simplex, using reflection, expansion, contraction, and shink steps, one vertex at a time~\cite{Nelder_Mead_1965}.
Among the various direct-search solvers, this method is interesting as it is still used today, underlying the MATLAB function \verb|fminsearch|.
Many other works on direct search appeared in these years.
These comprise, for example, the works of \citeauthor{Powell_1964}~\cite{Powell_1964,Powell_1975a}, \citeauthor{Matyas_1965}~\cite{Matyas_1965}, \citeauthor{Fletcher_1965}~\cite{Fletcher_1965}, and \citeauthor{Box_1966}~\cite{Box_1966}, for instance.
At the end of the millennium, with the notable works of \citeauthor{Wright_1995}~\cite{Wright_1995} and others, \gls{dfo} became the broad research area that exists today, encouraged by recent engineering needs.
Nowadays, the direct-search paradigm offers many other algorithms, such as the \gls{gps} methods~\cite{Booker_Etal_1999}, later extended to the \gls{mads} methods~\cite{Abramson_Audet_2006,Abramson_Etal_2009,Audet_Dennis_2006,Audet_Dennis_Digabel_2008,Digabel_2011}, the implicit filtering method~\cite{Kelley_2011}, and \gls{bfo}~\cite{Porcelli_Toint_2017}.
A more recent work by \citeauthor{Porcelli_Toint_2022} attempts to incorporate partial separability knowledge in a direct-search framework~\cite{Porcelli_Toint_2022}.

\subsection{Model-based methods}

Unlike direct-search methods, model-based methods approximate locally the functions involved in the optimization problems.
To globalize their convergence properties, the approximated problems are usually embedded in a globalization strategy, such as a \emph{line-search} method~\cite[ch.~3]{Nocedal_Wright_2006} or a \emph{trust-region} methods~\cite{Conn_Gould_Toint_2000,Yuan_2015}.
Most of the existing model-based methods use linear or quadratic approximations, although attempts have been made to use other functions, such as \gls{rbf}~\cite{Oeuvray_2005}.

From an empirical viewpoint, the model-based methods are highly appealing, as they have excellent performances in real applications.
Examples of such algorithms include the method of \citeauthor{Conn_Toint_1996}~\cite{Conn_Toint_1996} for unconstrained optimization, which models the objective function with either sub-quadratic or quadratic functions obtained by underdetermined interpolations (see \cref{sec:underdetermined-interpolation}), for which the freedom bequeathed by the interpolation conditions is taken out by minimizing the~$\ell_2$-norm of the coefficients of the models.
Particular care is given to the geometry of the interpolation set to attempt to maintain models with reasonable accuracies.
Another example of such algorithms for unconstrained optimization is \gls{mnh}~\cite{Wild_2008}, whose models are also quadratic functions obtained by underdetermined interpolation.
However, these models do not minimize the~$\ell_2$-norm of the coefficients but rather the Frobenius norm of their Hessian matrices.
The Wedge method~\cite{Marazzi_Nocedal_2002} of \citeauthor{Marazzi_Nocedal_2002} is another instance of \gls{dfo} trust-region solvers.
It has, however, the unique feature that it does not include any geometry-improvement steps, as most solvers do.
Instead, it adds a new constraint to the trust-region subproblems to prevent the trial steps from lying in a region that is likely to worsen the geometry of the interpolation set.
Other examples of trust-region methods are \gls{csv2}~\cite{Billups_Larson_Graf_2013}, which determines its models by regression and not interpolation, \gls{dfols}~\cite{Cartis_Etal_2019}, which aims at solving nonlinear least-squares problems, or the Powell's \gls{dfo} solvers, presented later in this document.
We thus heed that some solvers use nonpolynomial models, such as \gls{orbit}~\cite{Wild_Regis_Shoemaker_2008}, \gls{conorbit}~\cite{Regis_Wild_2017}, and \gls{boosters}~\cite{Oeuvray_Bierlaire_2009}, which use cubic \gls{rbf}.

\subsection{Comment on finite-difference approximations}

Perhaps to simplest approach for solving \gls{dfo} problems is to use derivative-based methods with finite-difference approximations of the derivatives (see, e.g.,~\cite{Shi_Etal_2021}).
For example, the~$i$th coordinate of the gradient of a differentiable function~$\obj : \R^n \to \R$ at a point~$x \in \R^n$ can be approximated by the forward difference
\begin{equation*}
    \frac{\partial \obj}{\partial x_i}(x) \approx \frac{\obj(x + h e_i) - \obj(x)}{h},
\end{equation*}
or the central difference
\begin{equation*}
    \frac{\partial \obj}{\partial x_i}(x) \approx \frac{\obj(x + h e_i) - \obj(x - h e_i)}{2h},
\end{equation*}
where~$h > 0$ is a given difference parameter and~$e_i \in \R^n$ is the~$i$th standard coordinate vector of~$\R^n$.
It is known that such methods are competitive with \gls{dfo} methods when the problems considered are smooth and noiseless.
However, in the presence of noise, the performance of finite-difference based methods usually drop.
Moreover, the choice of the difference parameter~$h$ depends on the noise level, which is usually unknown.
It is clear that estimating~$\nabla f(x)$ necessitates~$\mathcal{O}(n)$ function evaluations around~$x$ along the coordinate axes.
Since each function evaluation is expensive, one should attempt to reuse as much as possible function evaluations.
However, it is unlikely that these evaluations are reused by the gradient-based method without any modification.
Thus, we do not consider finite-difference based method in this thesis.


\section{Benchmarking tools for \glsfmtlong{dfo} methods}

We introduce in this section the benchmarking tools that we will use throughout this thesis to compare \gls{dfo} solvers.
We denote~$\mathcal{S}$ the set of solvers to benchmark and~$\mathcal{P}$ a set of problems to be solved by all solvers in~$\mathcal{S}$, assumed to be representative of the problems for which the solvers have been designed.
Let~$t_{p, s}$ be the performance measure for the solver~$s \in \mathcal{S}$ to achieve a given convergence test on the problem~$p \in \mathcal{P}$.
The performance measure of a \gls{dfo} solver we consider in this thesis is the number of function evaluations.
Therefore,~$t_{p, s}$ denotes the number of function evaluations required by the solver~$s \in \mathcal{S}$ to solve the problem~$p \in \mathcal{P}$ for the given convergence test.

To compare the performances of the solvers in~$\mathcal{S}$, we use the concept of performance and data profiles~\cite{Dolan_More_2002,More_Wild_2009} presented hereinafter.
The numerical experiments presented in this thesis define~$\mathcal{P}$ as subsets of the CUTEst dataset~\cite{Gould_Orban_Toint_2015}.
Traditional convergence tests necessitate derivative information, which is assumed unavailable in this work.
We consider the following convergence test for \gls{dfo} solvers, proposed by \citeauthor{More_Wild_2009}~\cite{More_Wild_2009}.
We assume that each problem~$p \in \mathcal{P}$ has a merit function~$\varphi_p$, i.e., a function that properly balances the possible infeasibility of a point with the corresponding value of the objective function.
Let~$x_p^0 \in \R^{n_p}$ be the initial guess of a given problem~$p \in \mathcal{P}$ and~$x_p^{\ast} \in \R^{n_p}$ be the point that achieves the least value of~$\varphi_p$ among all iterates of all solvers in~$\mathcal{S}$, where~$n_p$ denotes the dimension of the problem~$p$.
Given a tolerance~$\tau \in (0, 1)$, a point~$x \in \R^{n_p}$ satisfies the convergence test if
\begin{equation*}
    \varphi_p(x) \le \varphi_p(x_p^{\ast}) + \tau [\varphi_p(x_p^0) - \varphi_p(x_p^{\ast})].
\end{equation*}
This convergence test can be interpreted as follows.
A point~$x \in \R^n$ satisfies the convergence test if the reduction~$\varphi_p(x_{\hat{p}}^0) - \varphi_p(x)$ is at least~$1 - \tau$ times the best possible reduction~$\varphi_p(x_p^0) - \varphi_p(x_p^{\ast})$.
Therefore, we assume that at least one of the solvers converged for each problem, as the actual solutions of the problems may be unknown.

\subsection{Performance profiles}

For any fixed solver~$\hat{s} \in \mathcal{S}$ and problem~$\hat{p} \in \mathcal{P}$, define their \emph{performance ratio} by
\begin{equation*}
    r_{\hat{p}, \hat{s}} \eqdef \frac{t_{\hat{p}, \hat{s}}}{\min \set{t_{\hat{p}, s} : s \in \mathcal{S}}}.
\end{equation*}
When the solver~$\hat{s}$ fails to solve the problem~$\hat{p}$, we use the convention~$r_{\hat{p}, \hat{s}} = \infty$.
Given any fixed solver~$\hat{s} \in \mathcal{S}$ and a threshold~$\alpha \ge 1$, the \emph{performance profile} of~$\hat{s}$ is defined as the proportion of problems in~$\mathcal{P}$ that are solved with a performance ratio at most~$\alpha$, that is
\begin{equation*}
    \rho_{\hat{s}}(\alpha) \eqdef \frac{1}{\card \mathcal{P}} \card \set{p \in \mathcal{P} : r_{p, \hat{s}} \le \alpha},
\end{equation*}
where~$\card$ denotes the cardinal operator.
Therefore, $\rho_{\hat{s}}(1)$ is the proportion of problems in~$\mathcal{P}$ that are solved faster by the solver~$\hat{s}$ than by any other solvers in~$\mathcal{S}$, and $\rho_{\hat{s}}(\alpha)$ is the proportion of problems in~$\mathcal{P}$ that are solved by the solver~$\hat{s}$ without any budget restriction for any~$\alpha \ge \max \set{r_{p, \hat{s}} : p \in \mathcal{P}}$.

We consider the following experiment as an example.
We evaluate the performance profiles of different solvers, namely \gls{newuoa} (see \cref{subsec:newuoa-bobyqa-lincoa}) and two gradient-based solvers with forward finite-difference approximations (\gls{bfgs} and \gls{cg}), with the difference parameter~$h = \sqrt{u}$ where~$u$ is the unit roundoff.
The set~$\mathcal{P}$ is defined to be the unconstrained CUTEst problems of dimension at most~$50$, and the convergence tolerance is~$\tau = 10^{-5}$.
A graphical representation of the performance profiles is given in \cref{fig:performance-profile-example}.
The abscissa is the performance ratio in base-$2$ logarithmic scale and the performance profiles are on the ordinate.

\begin{figure}[ht]
    \centering
    \drawprofiles{{"NEWUOA","BFGS","CG"}}{plain-1-50-perf-bfgs-cg-newuoa-U.csv}{5}
    \caption{Example of performance profiles}
    \label{fig:performance-profile-example}
\end{figure}

On this example, we could argue that \gls{bfgs} provides better performance than the two other solvers.
However, we cannot compare \gls{newuoa} and \gls{cg} on this plot.
In fact, performance profiles provide a clear measure when comparing two solvers, but they may fail comparing a solver relatively to another one that is not the best when comparing more than two solvers~\cite{Gould_Scott_2016}.

\subsection{Data profiles}

We can also use data profiles~\cite{More_Wild_2009} to compare different solvers.
Unlike performance profiles, data profiles provide an absolute comparison.
For any fixed solver~$\hat{s} \in \mathcal{S}$ and threshold~$\alpha \ge 0$, the \emph{data profile} of~$\hat{s}$ is defined by
\begin{equation*}
    d_{\hat{s}}(\alpha) \eqdef \frac{1}{\card \mathcal{P}} \card \set[\bigg]{p \in \mathcal{P} : \frac{t_{p, \hat{s}}}{n_p + 1} \le \alpha}.
\end{equation*}
Therefore,~$d_{\hat{s}}(0) = 0$ and~$d_{\hat{s}}(\alpha) = \rho_{\hat{s}}(\alpha)$ for any~$\alpha \ge \max \set{t_{p, \hat{s}} : p \in \mathcal{P}}$, since~$t_{p, s} \ge 1$ for any~$p \in \mathcal{P}$ and~$s \in \mathcal{S}$.

\todo[noline]{Why~$n_p + 1$?}
