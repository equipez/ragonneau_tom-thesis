%% contents/introduction.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Introduction}

\nomenclature[Fa]{$\obj$}{Real-valued objective function defined on~$\R^n$}%
\nomenclature[Fb]{$\con{i}$}{Real-valued constraint function defined on~$\R^n$, with~$i \in \iub \cup \ieq$}%
\nomenclature[Fc]{$\lag$}{Lagrangian function}%
\nomenclature[Na]{$\in$}{Set membership notation}%
\nomenclature[Nb]{$\subseteq$}{Set inclusion notation}%
\nomenclature[Nc]{$\qedsymbol$}{Halmos symbol}%
\nomenclature[Nd]{$A^{\T}$,~$v^{\T}$}{Transpose of a matrix or a vector}%
\nomenclature[Ne]{$I_n$}{Identity matrix on~$\R^{n \times n}$}%
\nomenclature[Nf]{$e_k$}{Standard coordinate vector of~$\R^n$ ($k$-th column of~$I_n$), with~$1 \le k \le n$}%
\nomenclature[Ng]{$\mathcal{O}(\cdot)$}{Big-O notation}%
\nomenclature[Nh]{$o(\cdot)$}{Little-O notation}%
\nomenclature[Oa]{$[\cdot]_{+}$}{Elementwise positive-part operator}%
\nomenclature[Ob]{$[\cdot]_{-}$}{Elementwise negative-part operator}%
\nomenclature[Oc]{$\abs{\cdot}$}{Elementwise modulus operator}%
\nomenclature[Od]{$\inner{\cdot, \cdot}$}{Inner-product operator (may be subscripted for sake of clarity)}%
\nomenclature[Oe]{$\norm{\cdot}$}{Norm of a vector or a matrix (may be subscripted for sake of clarity)}%
\nomenclature[Of]{$\nabla$}{Gradient operator (elements~$\partial / \partial x_i$, with~$i \in \set{1, 2, \dots, n}$)}%
\nomenclature[Og]{$\nabla^2$}{Hessian operator (elements~$\partial^2 / \partial x_i \partial x_j$, with~$i, j \in \set{1, 2, \dots, n}$)}%
\nomenclature[Oh]{$\land$}{Logic and operator}%
\nomenclature[Oi]{$\card$}{Cardinal operator}%
\nomenclature[Sa]{$\emptyset$}{Empty set}%
\nomenclature[Sb]{$[a, b]$}{Closed set~$\set{x \in \R : a \le x \le b}$ with~$a \le b$}%
\nomenclature[Sc]{$(a, b)$}{Open set~$\set{x \in \R : a < x < b}$ with~$a < b$}%
\nomenclature[Sd]{$[a, b)$}{Semi-open set~$\set{x \in \R : a \le x < b}$ with~$a < b$}%
\nomenclature[Se]{$(a, b]$}{Semi-open set~$\set{x \in \R : a < x \le b}$ with~$a < b$}%
\nomenclature[Sf]{$\R$}{Set of real numbers}%
\nomenclature[Sg]{$\R^n$}{Real coordinate space of dimension~$n$}%
\nomenclature[Sh]{$\R^{m \times n}$}{Real matrix space of dimension~$m \times n$}%
\nomenclature[Si]{$\Omega$}{Feasible set, included in~$\R^n$}%
\nomenclature[Sj]{$\ieq$}{Set of indices of the equality constraints}%
\nomenclature[Sk]{$\iub$}{Set of indices of the inequality constraints}%
\todo[noline]{Replace the nomenclature items}

\section{Overview of \glsfmtlong{dfo}}
\label{sec:overview}

Optimization is the study of extremal points and values of mathematical functions.
It aims at minimizing (or maximizing) a real-valued function~$\obj$, referred to as the \emph{objective function}, within a given set of points~$\Omega \subseteq \R^n$, referred to as the \emph{feasible set}.
It is well known that essential information for optimization is embraced in the (possibly generalized) derivatives of the functions involved.
However, in practice, evaluations of such derivatives may be unreliable or prohibitively expensive, if not impossible.
It motivates the study of \gls{dfo}~\cite{Conn_Scheinberg_Vicente_2009b,Audet_Hare_2017,Custodio_Scheinberg_Vicente_2017,Larson_Menickelly_Wild_2019}, where problems are solved using only function values.
This thesis focuses on methods and software for \gls{dfo}.

\Gls{dfo} problems arise naturally when the objective function or the feasible set results from complex experiments or simulations.
Regarding these functions as black boxes, people often refer to those problems as \gls{bbo} problems~\cite{Audet_Hare_2017}, which constitute a significant type of \gls{dfo} problem in practice.
Note that \gls{dfo} differs from nonsmooth optimization~\cite{Clark_1983,Cui_Pang_2021}, which studies problems involving nonsmooth functions.
In \gls{dfo}, the major difficulty is not the possible nonsmoothness of the functions involved but the lack of knowledge about the structures of the problems.
In theoretical analysis of \gls{dfo} methods, it is not uncommon to assume that the underlying functions enjoy some smoothness, although algorithms cannot retrieve their (classical or generalized) derivatives.
We emphasize that if any derivative information can be evaluated at an affordable cost or approximated well enough, \gls{dfo} methods are not recommended, as they are very unlikely to outperform methods that use derivatives.
Consider, for example, minimizing an objective function defined by a sophisticated simulation whose source code is available.
One may then attempt to evaluate derivatives using automatic differentiation tools~\cite{Griewank_2003,Griewank_Walther_2008} and apply derivative-based methods.

For \gls{dfo} methods, the leading complexity measure we consider is the number of function evaluations.
In practice, each function evaluation may require several minutes or even several days to complete~\cite[\S~1.4]{Audet_Hare_2017}.
For instance, a recent application of \gls{dfo} is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}, for which every objective function evaluation necessitates training a machine learning model (see \cref{subsec:machine-learning}).
Hence, in \gls{dfo} methods, the expense of numerical linear algebra is less of a concern, although we will maintain it acceptable.

In this introduction, we consider the nonlinearly-constrained problem
\begin{subequations}
    \label{eq:problem-introduction}
    \begin{align}
        \min        & \quad \obj(x)\\
        \text{s.t.} & \quad \con{i}(x) \le 0, ~ i \in \iub, \label{eq:problem-introduction-cub}\\
                    & \quad \con{i}(x) = 0, ~ i \in \ieq, \label{eq:problem-introduction-ceq}\\
                    & \quad x \in \R^n, \nonumber
    \end{align}
\end{subequations}
where the \emph{objective} and \emph{constraint functions}~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are real-valued functions on~$\R^n$, and where the sets of indices~$\iub$ and~$\ieq$ are finite (perhaps empty) and disjoint.
The feasible set of this problem is
\begin{equation*}
    \Omega \eqdef \set{x \in \R^n : \con{i}(x) \le 0 ~ \text{for~$i \in \iub$}, ~ \con{i}(x) = 0 ~ \text{for~$i \in \ieq$}}.
\end{equation*}
If~$\obj$ is convex, while~$\con{i}$ is convex for all~$i \in \iub$ and affine for all~$i \in \ieq$, then problem~\cref{eq:problem-introduction} is said \emph{convex}.
However, this thesis does \emph{not} assume convexity for problem~\cref{eq:problem-introduction}.

We emphasize that~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq} may include bound constraints.
We do not extract them explicitly in this chapter, but they may need to be handled differently from other constraints because they often represent inalienable physical or theoretical restrictions.
We will later in this thesis introduce a new \gls{dfo} method, namely \gls{cobyqa}, which will consider this (see \cref{ch:cobyqa-introduction}).

\section{Examples of applications}

\subsection{Automatic error analysis}

A typical example of \gls{dfo} applications is automatic error analysis~\cite{Higham_1993,Higham_2002}, which formulates numerical computation's accuracies and stabilities using optimization problems.
Consider, for instance, the Gaussian elimination with partial pivoting of a matrix~$A \in \R^{n \times n}$, given in \cref{alg:gaussian-elimination}, where the superscripts denote iteration numbers.

\begin{algorithm}[ht]
    \caption{Gaussian elimination with partial pivoting}
    \label{alg:gaussian-elimination}
    \DontPrintSemicolon
    \KwData{Matrix~$A \in \R^{n \times n}$.}
    Initialize $A^{(0)} \gets A$\;
    \For{$k = 1, 2, \dots, n - 1$}{
        Determine the pivot index~$j = \argmax \set[\big]{\abs[\big]{A_{i, k}^{(k - 1)}} : k \le i \le n}$\;
        \eIf{$A_{j, k}^{(k - 1)} \neq 0$}{
            Exchange the~$k$th and the~$j$th rows of~$A^{(k - 1)}$\;
            Evaluate the multiplier~$\tau^k \in \R^n$ with components
            \begin{algoempheq}[left={\tau_i^k = \empheqlbrace}]{alignat*=2}
                & A_{i, k}^{(k - 1)} / A_{k, k}^{(k - 1)}   && \quad \text{if~$i > k$, and}\\
                & 0                                         && \quad \text{otherwise}
            \end{algoempheq}
            Update~$A^{(k)} \gets (I_n - \tau^k e_k^{\T})A^{(k - 1)}$\;
        }{
            Set~$A^{(k)} \gets A^{(k - 1)}$\;
        }
    }
\end{algorithm}

\todo[noline]{Write down explicitly the conventions on the subscripts and superscripts.}

\Citeauthor{Wilkinson_1963}'s backward error analysis (see, e.g., equation~(25.14) of chapter~3 in~\cite{Wilkinson_1963}, where~$t$ is introduced at the beginning of \P~10 and~$g$ at the end of p.~97) demonstrates that the growth factor of the Gaussian elimination, defined as
\begin{equation}
    \label{eq:gaussian-elimination-growth-factor}
    \rho_n(A) \eqdef \frac{\max_{0 \le k \le n - 1} \norm{A^{(k)}}_{\max}}{\norm{A}_{\max}},
\end{equation}
determinates the numerical stability of \cref{alg:gaussian-elimination}, where~$\norm{\cdot}_{\max}$ denotes the max norm of a matrix, i.e., the largest absolute value of the matrix's entries.
More specifically, the~$\ell_{\infty}$-norm of the backward error of the computed solution is bounded from above by a term proportional to~$\rho_n(A)$.
To study the worst-case scenario, we wish to determine how large\todo{Grammar checking done until here}~$\rho_n$ can be and hence, to solve
\begin{equation}
    \label{eq:gaussian-elimination-problem}
    \max_{A \in \R^{n \times n}} \rho_n(A).
\end{equation}
Note that~$\R^{n \times n}$ is isomorphic to~$\R^{n^2}$ and hence, problem~\cref{eq:gaussian-elimination-problem} can straightforwardly be formulated as problem~\cref{eq:problem-introduction}.
Besides, although the growth factor is defined everywhere, it may not be continuous at the points yielding a tie in the selection of the pivot element.
Moreover, it is not differentiable at the points yielding a tie in any maximum operator in equation~\cref{eq:gaussian-elimination-growth-factor}.
Hence, optimization methods based on derivative information cannot be used for this problem.
In such a case, \gls{dfo} methods can help solving problem~\cref{eq:gaussian-elimination-problem}.
Note that the optimal value and all local solutions to problem~\cref{eq:gaussian-elimination-problem} are known~\cite{Higham_Higham_1989}, but \gls{dfo} methods can be used to help the theoretical development~\cite{Higham_1993}.

\subsection{Tuning nonlinear optimization methods}

Another well-known example of \gls{dfo} applications is the parameter tuning of nonlinear optimization methods~\cite{Audet_Orban_2006}.
For example, consider \cref{alg:trust-region}, a basic trust-region method for solving problem~\cref{eq:problem-introduction} when~$\iub = \ieq = \emptyset$.

\begin{algorithm}[ht]
    \caption{Basic trust-region method for unconstrained optimization}
    \label{alg:trust-region}
    \DontPrintSemicolon
    \KwData{Objective function~$\obj$, initial guess~$x^0 \in \R^n$, initial trust-region radius~$\Delta_0 > 0$, and parameters~$0 < \eta_1 \le \eta_2 < 1$ and~$0 < \theta_1 < 1 < \theta_2$.}
    \For{$k = 0, 1, \dots$}{
        Define a simple function~$m_k$ such that~$m_k(d) \approx f(x^k + d)$ for~$\norm{d} \le \Delta_k$\;
        Set the trial step~$d^k$ to an approximate solution to
        \begin{algomathdisplay}
            \begin{aligned}
                \min        & \quad m_k(d)\\
                \text{s.t.} & \quad \norm{d} \le \Delta_k,\\
                            & \quad d \in \R^n
            \end{aligned}
        \end{algomathdisplay}
        Evaluate the trust-region ratio
        \begin{algomathdisplay}
            \rho_k \gets \frac{\obj(x^k) - \obj(x^k + d^k)}{m_k(0) - m_k(d^k)}
        \end{algomathdisplay}
        \eIf{$\rho_k \ge \eta_1$}{ \nllabel{alg:trust-region-success}
            Update the trial point~$x^{k + 1} \gets x^k + d^k$\;
        }{
            Retain the trial point~$x^{k + 1} \gets x^k$\;
        }
        Update the trust-region radius
        \begin{algoempheq}[left={\Delta_{k + 1} \gets \empheqlbrace}]{alignat*=2}
            & \theta_1 \Delta_k && \quad \text{if~$\rho_k \le \eta_1$,}\\
            & \Delta_k          && \quad \text{if~$\eta_1 < \rho_k \le \eta_2$, and}\\
            & \theta_2 \Delta_k && \quad \text{otherwise}
        \end{algoempheq}
    }
\end{algorithm}

The most important simplification in \cref{alg:trust-region} lies in \cref{alg:trust-region-success}.
A complete framework includes a parameter~$\eta_0 \ge 0$ satisfying~$\eta_0 \le \eta_1$, and the condition in \cref{alg:trust-region-success} is replaced by~$\rho_k \ge \eta_0$.
In practice, we usually have~$\eta_0 = 0$.
However, this parameter genuinely complexifies the theoretical analysis of the trust-region method, and hence, we omit it here for sake of simplicity.
We consider only the four parameters~$\eta_1$,~$\eta_2$,~$\theta_1$, and~$\theta_2$.
To choose those parameters, we minimize some measure of the method's expense (e.g., the \glsxtrshort{cpu} time to solve a given set of optimization problems),~$C$ say.
In other words, we wish to solve the optimization problem
\begin{subequations}
    \label{eq:tuning-algorithms-problem}
    \begin{align}
        \min        & \quad C(\eta_1, \eta_2, \theta_1, \theta_2)\\
        \text{s.t.} & \quad 0 \le \eta_1 \le \eta_2 < 1,\\
                    & \quad 0 < \theta_1 < 1 < \theta_2.
    \end{align}
\end{subequations}
Derivatives of~$C$ cannot be evaluated if they even exist.
Such a problem is then solved using \gls{dfo} methods, for example using the \gls{mads} method~\cite{Audet_Orban_2006}.
\Citeauthor{Audet_Digabel_Tribes_2019} modified the \gls{mads} method to solve problem~\cref{eq:tuning-algorithms-problem} with a controlled number of significant digits~\cite{Audet_Digabel_Tribes_2019}, to determine parameters that can be used by practitioners.
Interestingly, \gls{dfo} methods can be self-tuned using the method presented above.
The \gls{bfo}~\cite{Porcelli_Toint_2017}, a method for bound-constrained problems mixing continuous and discrete variables, is an example of self-tuned \gls{dfo} methods.

\subsection{Hyperparameter tuning in machine learning}
\label{subsec:machine-learning}

A more recent example of \gls{dfo} applications is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}.
For instance, Google solves hyperparameter tuning problems using Google Vizier~\cite{Golovin_Etal_2017}, the Google-internal service for performing black-box optimization.
To illustrate this example, we consider the following hyperparameter tuning problem of a \gls{svm} for binary classification.
Given a binary-labeled dataset~$\set{(x_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$, we build a \gls{svm} to classify the data with their respective labels.
A binary classification is obtained using a~$C$-\gls{svc}~\cite{Chang_Lin_2011} by solving the optimization problem
\begin{subequations}
    \label{eq:csvc}
    \begin{align}
        \min        & \quad \frac{1}{2} \norm{\omega}_2^2 + C \norm{\xi}_1\\
        \text{s.t.} & \quad y_i (\beta + \inner{\omega, \varphi_{\gamma}(x_i)}) \ge 1 - \xi_i, ~ i \in \set{1, 2, \dots, m},\\
                    & \quad \xi \ge 0,\\
                    & \quad (\omega, \beta, \xi) \in \R^{\ell} \times \R \times \R^m,
    \end{align}
\end{subequations}
where~$\varphi_{\gamma}$ is a function mapping the data to a higher-dimensional space~$\R^{\ell}$ and~$\gamma > 0$ and~$C > 0$ are parameters.
Given a solution~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast}) \in \R^{\ell} \times \R \times \R^m$ to problem~\cref{eq:csvc}, the~$C$-\gls{svc} classifies any data~$x \in \R^n$ according to
\begin{equation}
    \label{eq:csvc-classifier}
    \delta(x) \eqdef \sgn(\beta^{\ast} + \inner{\omega^{\ast}, \varphi_{\gamma}(x)}),
\end{equation}
which maps an observation~$x \in \R^n$ to a label in~$\set{\pm 1}$.
It is clear that~$\delta$ depends on the two parameters~$C$ and~$\gamma$, which can be chosen by solving an optimization problem.
The objective function~$P$ of this problem is a~$5$-fold cross-validation based on some performance measure of the model~\cref{eq:csvc-classifier}.
The general~$k$-fold cross-validation to define~$P(C, \gamma)$ is presented in \cref{alg:cross-validation}.

\begin{algorithm}[ht]
    \caption{$k$-fold cross-validation of an \glsfmtshort{svc} with parameters~$C$ and~$\gamma$}
    \label{alg:cross-validation}
    \DontPrintSemicolon
    \KwData{Labeled dataset~$\set{(x_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$ and fold number~$k > 0$.}
    Split the dataset into~$k$ balanced groups\;
    \For{$i = 1, 2, \dots, k$}{
        Calculate~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast})$ with the all the data except those in the~$i$th group\;
        Evaluate the performance~$p_i$ of the model~\cref{eq:csvc-classifier} on the data in the~$i$th group\;
    }
    Define~$P(C, \gamma)$ by summarizing the performances~$\set{p_1, p_2, \dots, p_k}$\;
\end{algorithm}

A typical example of model's performance used in the~$k$-fold cross-validation is the model's accuracy, i.e., the percentage of data correctly classified.
The~\gls{auc}~\cite{Hanley_Mcneil_1982} is another example of performance measure, particularly effective for imbalanced datasets~\cite{Bradley_1997}.
The hyperparameter tuning problem can be formulated as
\begin{equation*}
    \begin{aligned}
        \min        & \quad P(C, \gamma)\\
        \text{s.t.} & \quad C > 0,\\
                    & \quad \gamma > 0.
    \end{aligned}
\end{equation*}
It is clear that derivatives of the objective function of such a problem cannot be easily evaluated and may even not exist.
This problem may be solved using \gls{dfo} methods.

As in~\cite{Qian_Yu_2021}, \gls{dfo} be also be applied to reinforcement learning.
Instead of training a model on a fixed labelled dataset, reinforcement learning bases the training process on rewarding expected behaviors and punishing undesired ones.
Hence, it often consists in finding optimal parameters that maximize a reward.
However, these reward's derivatives often cannot be evaluated, and \gls{dfo} methods can be an approach to solving such problems.
This concept is often referred to as derivative-free reinforcement learning.


\subsection{Some industrial and engineering applications}

\Gls{dfo} methods are also widely used in industry and engineering, especially for solving problems that involve heavy simulations.
Such problems arise from helicopter rotor blade manufacturing~\cite{Booker_Etal_1998a,Booker_Etal_1998b,Serafini_1998}, aeroacoustic shape design~\cite{Marsden_2004,Marsden_Etal_2004}, computational fluid dynamics~\cite{Duvigneau_Visonneau_2004}, worst-case analysis of analog circuit~\cite{Latorre_Etal_2019}, rapid-cycling synchrotron accelerator modeling~\cite{Eldred_Etal_2021}, nuclear energy engineering~\cite{Kortelainen_Etal_2010,Kortelainen_Etal_2012,Kortelainen_Etal_2014}, reservoir engineering and engine calibration~\cite{Langouet_2011}, and groundwater supply and bioremediation engineering~\cite{Fowler_Etal_2008,Mugunthan_Shoemaker_Regis_2005,Yoon_Shoemaker_1999}, to name but a few.
In general, industrial and engineering problems that involve sophisticated models, simulations, or experiments, give rise to \gls{dfo} problems.

A particular application of \gls{dfo} comes from \gls{mdo} in industry.
It is an field that uses optimization methods to solve design problems defined by multiple disciplines.
The objective and constraint functions of a \gls{mdo} problem can be provided by different departments of the same company or even by different companies.
This is the case in aircraft engine engineering~\cite{Gazaix_Etal_2019}, where the design problem of one component is solved while taking into account constraints imposed by other components handled by different departments.
\Gls{mdo} problems often involve simulations or experiments and therefore, \gls{dfo} methods are often needed.
We will present in \cref{ch:pdfo} of this thesis a piece of software we implemented for solving \gls{dfo} problems based on methods by Powell~\cite{Powell_1994,Powell_2002,Powell_2006,Powell_2009,Powell_2015}.
It has been included in GEMSEO~\cite{Gallard_Etal_2018}, an engine for \gls{mdo} initiated by a team from IRT Saint Exup{\'{e}}ry\footnote{\url{https://www.irt-saintexupery.com}} in France.

\section{Optimality conditions for smooth optimization}

We discuss in this section optimality conditions for problem~\cref{eq:problem-introduction}.
We do not assume any structure on the objective and constraint functions, except some smoothness.
More specialized results can be obtained by assuming that problem~\cref{eq:problem-introduction} is convex for example, but it is out of the scope of this work.

\subsection{Local and global solutions}

Before solving problem~\cref{eq:problem-introduction}, we must define what a solution is.
\Cref{def:global-solution} presents the most natural understanding of a solution.

\begin{definition}[Global solution]
    \label{def:global-solution}
    To problem~\cref{eq:problem-introduction}, a point~$x^{\ast} \in \R^n$ is a \emph{global solution} if~$x^{\ast} \in \Omega$ and~$\obj(x) \ge \obj(x^{\ast})$ for all~$x \in \Omega$.
\end{definition}

The following relaxed concept of solutions to problem~\cref{eq:problem-introduction} is of interest both in theory and in practice.

\begin{definition}[Local solution]
    % \label{def:local-solution}
    To problem~\cref{eq:problem-introduction}, a point~$x^{\ast} \in \R^n$ is
    \begin{itemize}
        \item a \emph{local solution} if~$x^{\ast} \in \Omega$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$\obj(x) \ge \obj(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega$,
        \item a \emph{strict local solution} if~$x^{\ast} \in \Omega$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$\obj(x) > \obj(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega \setminus \set{x^{\ast}}$, and
        \item an \emph{isolated local solution} if there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that it is the only local solution in~$\mathcal{N} \cap \Omega$.
    \end{itemize}
\end{definition}

It is known that finding a local solution to a general nonconvex problem is NP-hard.
In many cases, it is already NP-hard to check whether a given point is a local solution.
There also exist convex optimization problems that are NP-hard, such as copositive programming, for which checking the feasibility of a given point is indeed NP-hard.
See~\cite{Murty_Kabadi_1987} for more discussions.

The methods we consider in this thesis are local.
They attempt to approximately find local solutions to problem~\cref{eq:problem-introduction}.
However, in general, theoretical analyses of these methods can only guarantee approximations of stationary points, which will be introduced hereinafter.

\subsection{Constraint qualifications}

Before introducing any necessary and sufficient conditions for local optimality, we discuss some regularity conditions on the constraints~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq}, referred to as \emph{constraint qualifications}.
They will be required for the necessary conditions to hold.
We first introduce the notion of \emph{active set}.

\begin{definition}[Active set]
    The \emph{active set}~$\mathcal{A}(x) \subseteq \iub \cup \ieq$ for problem~\cref{eq:problem-introduction} at a point~$x \in \R^n$ is defined as
    \begin{equation*}
        \mathcal{A}(x) \eqdef \ieq \cup \set{i \in \iub : \con{i}(x) \ge 0}.
    \end{equation*}
\end{definition}

If a constraint belongs to the active set\footnote{For simplicity, we do not distinguish a constraint from its index.} at a given point, it is said to be \emph{active} at this point, and \emph{inactive} otherwise.
Note that a violated constraint is always considered active.

We introduce hereinafter two classical constraint qualifications.

\begin{definition}[Constraint qualifications]
    Given~$x \in \Omega$, denote~$\mathcal{A}(x)$ the active set for problem~\cref{eq:problem-introduction} at~$x$, and assume that the constraint functions~$\con{i}$ are differentiable at~$x$ for all~$i \in \mathcal{A}(x)$.
    We say that
    \begin{itemize}
        \item the \gls{licq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \mathcal{A}(x)$, and
        \item the \gls{mfcq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \ieq$ and there exists a vector~$z \in \R^n$ such that
        \begin{subequations}
            \label{eq:mangasarian-fromovitz}
            \begin{empheq}[left=\empheqlbrace]{alignat=2}
                & \inner{\nabla \con{i}(x), z} < 0  && \quad \text{if~$i \in \mathcal{A}(x) \cap \iub$, and}\\
                & \inner{\nabla \con{i}(x), z} = 0  && \quad \text{if~$i \in \ieq$}.
            \end{empheq}
        \end{subequations}
    \end{itemize}
\end{definition}

The \gls{licq} is stronger than the \gls{mfcq}.
If the \gls{licq} holds at~$x \in \Omega$, then the system~\cref{eq:mangasarian-fromovitz} is consistent because of the linear independance of all~$\nabla \con{i}(x)$ for~$i \in \mathcal{A}(x)$.

Many other constraint qualifications exist.
Examples include the \gls{acq} and the \gls{gcq}, which are formulated using tangent and linearized cones of the feasible set.
There also exist several traditional constraint qualifications weaker than the \gls{mfcq}, such as the \gls{crcq}, the \gls{cpld}, or the \gls{qncq}.
We also note that when the problem has a particular structure, such as convexity, dedicated constraint qualifications may exist, such as the \gls{sc}.
In this thesis, we will focus on the \gls{licq} and the \gls{mfcq}.

\subsection{First-order optimality conditions}

\subsubsection{Statement of the optimality conditions}

Let~$\lag$ be the \emph{Lagrangian function} to problem~\cref{eq:problem-introduction}, defined as
\begin{equation*}
    \lag(x, \lambda) \eqdef \obj(x) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lambda_i \con{i}(x),
\end{equation*}
where~$\lambda = (\lambda_i)_{i \in \iub \cup \ieq}$ with~$\lambda_i \in \R$ for all~$i \in \iub \cup \ieq$.
\Cref{thm:first-order-necessary-conditions} introduces the first-order necessary conditions for a point to be a local solution of problem~\cref{eq:problem-introduction}.

\begin{theorem}[First-order necessary conditions~\cite{Nocedal_Wright_2006}] % Theorem 12.1
    \label{thm:first-order-necessary-conditions}
    Let~$x^{\ast} \in \Omega$ be a local solution to problem~\cref{eq:problem-introduction}, assume that the functions~$\obj$ and~$\con{i}$ are continuously differentiable in a neighborhood of~$x^{\ast}$ for all~$i \in \iub \cup \ieq$, and that the \gls{licq} holds at~$x^{\ast}$.
    Then there exists a Lagrange multiplier~$\lambda^{\ast} = (\lambda_i^{\ast})_{i \in \iub \cup \ieq}$ with~$\lambda_i^{\ast} \in \R$ for all~$i \in \iub \cup \ieq$ such that
    \begin{subequations}
        \label{eq:kkt-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=2}
            & \nabla_x \lag(x^{\ast}, \lambda^{\ast}) = 0,  && \label{eq:kkt-introduction-stationarity}\\
            & \con{i}(x^{\ast}) \le 0,                      && \quad \text{if~$i \in \iub$,} \label{eq:kkt-introduction-primal-feasibility-ub}\\
            & \con{i}(x^{\ast}) = 0,                        && \quad \text{if~$i \in \ieq$,} \label{eq:kkt-introduction-primal-feasibility-eq}\\
            & \lambda_i^{\ast} \con{i}(x^{\ast}) = 0,       && \quad \text{if~$i \in \iub$,} \label{eq:kkt-introduction-complementary-slackness}\\
            & \lambda_i^{\ast} \ge 0,                       && \quad \text{if~$i \in \iub$.} \label{eq:kkt-introduction-dual-feasibility}
        \end{empheq}
    \end{subequations}
\end{theorem}

We present~\cref{thm:first-order-necessary-conditions} with the \gls{licq} as an example, but similar conclusion can be established with other constraint qualifications, such as the \gls{mfcq} (see, e.g.,~\cite[p.~339]{Nocedal_Wright_2006}).
The conditions~\cref{eq:kkt-introduction} are commonly referred to as the \gls{kkt} conditions~\cite{Karush_1939,Kuhn_Tucker_1951}.
More specifically, condition~\cref{eq:kkt-introduction-stationarity} is referred to as the \emph{stationarity} condition, conditions~\cref{eq:kkt-introduction-primal-feasibility-ub,eq:kkt-introduction-primal-feasibility-eq} as the \emph{primal feasibility} conditions, condition~\cref{eq:kkt-introduction-complementary-slackness} as the \emph{complementary slackness} condition, and condition~\cref{eq:kkt-introduction-dual-feasibility} as the \emph{dual feasibility} condition.
Any point~$x \in \R^n$ is referred to as a \emph{first-order stationary point} if it satisfies the \gls{kkt} conditions~\cref{eq:kkt-introduction}.
Such a point may not be a local solution.

\subsubsection{An illustration of the first-order optimality conditions}

We do not provide a proof of \cref{thm:first-order-necessary-conditions}, but we illustrate graphically the main idea on the simple~$2$-dimensional example
\begin{subequations}
    \label{eq:kkt-description}
    \begin{align}
        \min        & \quad \obj(x) = x_1 + x_2\\
        \text{s.t.} & \quad \con{1}(x) = x_1^2 + x_2^2 - 2 \le 0, \label{eq:kkt-description-c1}\\
                    & \quad \con{2}(x) = -x_2 \le 0, \label{eq:kkt-description-c2}\\
                    & \quad x \in \R^2, \nonumber
    \end{align}
\end{subequations}
whose solution is~$x^{\ast} = (-\sqrt{2}, 0)$.
A graphical representation of problem~\cref{eq:kkt-description} is given in \cref{fig:kkt-description}, where the white area represents the feasible set.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[%
            xmin=-5,%
            xmax=2,%
            ymin=-2,%
            ymax=2,%
            axis equal image,%
            xlabel={$x_1$},%
            ylabel={$x_2$},%
            axis background/.style={%
                pattern=north west lines,%
                pattern color=black!20,%
                even odd rule,%
                insert path={let \p1=(axis cs:0,0), \p2=(axis cs:2^0.5,0), \n1={veclen(\x2-\x1,\y2-\y1)}, in (\p2) arc(0:180:\n1) -- cycle},%
            },%
        ]
            \draw[dashed] (0,0) circle[radius=2^0.5];
            \draw[dashed] (-5,0) -- (2,0);
            \draw[-latex] (-2^0.5,0) -- (-3*2^0.5,0) node[above right] {$\nabla \con{1}(x^{\ast})$};
            \draw[-latex] (-2^0.5,0) -- (-2^0.5,-1) node[below left] {$\nabla \con{2}(x^{\ast})$};
            \draw[-latex] (-2^0.5,0) -- (1-2^0.5,1) node[below right] {$\nabla \obj(x^{\ast})$};
            \addplot[BrickRed,mark=*,only marks] coordinates {(-2^0.5,0)};
            \node[above left] at (-2^0.5,0) {$x^{\ast}$};
        \end{axis}
    \end{tikzpicture}
    \caption{Graphical representation of problem~\cref{eq:kkt-description}}
    \label{fig:kkt-description}
\end{figure}

As manifest in \cref{fig:kkt-description}, there does not exist any direction~$d \in \R^2$ satisfying
\begin{subequations}
    \label{eq:kkt-proof}
    \begin{empheq}[left=\empheqlbrace]{alignat=1}
        & \inner{\nabla \obj(x^{\ast}), d} < 0,\\
        & \inner{\nabla \con{1}(x^{\ast}), d} \le 0,\\
        & \inner{\nabla \con{2}(x^{\ast}), d} \le 0.
    \end{empheq}
\end{subequations}
The Farkas' lemma~\cite{Farkas_1902} ensures, therefore, that there exists a nonnegative Lagrange multiplier~$\lambda^{\ast} = (\lambda_1^{\ast}, \lambda_2^{\ast})$ such that
\begin{equation*}
    \nabla \obj(x^{\ast}) + \lambda_1^{\ast} \nabla \con{1}(x^{\ast}) + \lambda_2^{\ast} \nabla \con{2}(x^{\ast}) = 0.
\end{equation*}
This validates condition~\cref{eq:kkt-introduction-stationarity}, while~\cref{eq:kkt-introduction-primal-feasibility-ub,eq:kkt-introduction-primal-feasibility-eq,eq:kkt-introduction-complementary-slackness,eq:kkt-introduction-dual-feasibility} are obvious.
A solution to system~\cref{eq:kkt-proof} is both a descent direction for~$\obj$ and a linearized feasible direction for the constraints.
In the context of~\cref{thm:first-order-necessary-conditions}, the nonexistence of such a direction is guaranteed by the \gls{licq}.
See~\cite[\S~12.4]{Nocedal_Wright_2006} for a complete proof of \cref{thm:first-order-necessary-conditions}.

\subsection{Second-order optimality conditions}

It is known that at a local solution of a smooth unconstrained optimization problem, the gradient of the objective function is zero and its Hessian matrix is positive semidefinite.
\Cref{thm:second-order-necessary-conditions} generalizes this fact to the optimization problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order necessary conditions~\cite{Nocedal_Wright_2006}] % Theorem 12.5
    \label{thm:second-order-necessary-conditions}
    Let~$x^{\ast} \in \Omega$ be a local solution to problem~\cref{eq:problem-introduction}.
    Assume that the functions~$\obj$ and~$\con{i}$ are twice continuously differentiable in a neighborhood or~$x^{\ast}$ for all~$i \in \iub \cup \ieq$, and that \gls{licq} holds at~$x^{\ast}$.
    Denote the active set for problem~\cref{eq:problem-introduction} at~$x^{\ast}$ by~$\mathcal{A}(x^{\ast})$.
    Let~$\lambda^{\ast} = (\lambda_i^{\ast})_{i \in \iub \cup \ieq}$ with~$\lambda_i^{\ast} \in \R$ for all~$i \in \iub \cup \ieq$ be a Lagrange multiplier satisfying the KKT condition~\cref{eq:kkt-introduction}, and let~$z \in \R^n$ be any vector such that
    \begin{subequations}
        \label{eq:second-order-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=2}
            & \inner{\nabla \con{i}(x^{\ast}), z} = 0,      && \quad \text{if~$i \in \ieq$,}\\
            & \inner{\nabla \con{i}(x^{\ast}), z} = 0,      && \quad \text{if~$i \in \mathcal{A}(x^{\ast}) \cap \iub$ and~$\lambda_i^{\ast} > 0$, and}\\
            & \inner{\nabla \con{i}(x^{\ast}), z} \ge 0,    && \quad \text{if~$i \in \mathcal{A}(x^{\ast}) \cap \iub$ and~$\lambda_i^{\ast} = 0$.}
        \end{empheq}
    \end{subequations}
    Then~$\inner{z, \nabla_{x, x}^2 \lag(x^{\ast}, \lambda^{\ast}) z} \ge 0$. 
\end{theorem}

A first-order stationary point~$x \in \Omega$ is called a \emph{second-order stationary point} if it satisfies the conclusion of \cref{thm:second-order-necessary-conditions}.
We emphasize that a second-order stationary point may not be a local solution.
However, it is known in smooth unconstrained optimization that a point at which the gradient of the objective function is zero and its Hessian matrix is positive definite is a strict local solution.
\Cref{thm:second-order-sufficient-conditions} generalizes this fact to the optimization problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order sufficient conditions~\cite{Nocedal_Wright_2006}] % Theorem 12.6
    \label{thm:second-order-sufficient-conditions}
    Let~$x^{\ast} \in \Omega$ be a given point.
    Assume that the functions~$\obj$ and~$\con{i}$ are twice continuously differentiable in a neighborhood of~$x^{\ast}$ for all~$i \in \iub \cup \ieq$, and that there exists a Lagrange multiplier~$\lambda^{\ast} = (\lambda_i^{\ast})_{i \in \iub \cup \ieq}$ with~$\lambda_i^{\ast} \in \R$ for all~$i \in \iub \cup \ieq$ satisfying the KKT condition~\cref{eq:kkt-introduction}.
    If for all nonzero vector~$z \in \R^n \setminus \set{0}$ satisfying the conditions~\cref{eq:second-order-introduction} we have~$\inner{z, \nabla_{x, x}^2 \lag(x^{\ast}, \lambda^{\ast}) z} > 0$, then~$x^{\ast}$ is a strict local solution to problem~\cref{eq:problem-introduction}.
\end{theorem}

\section{Methodology of \glsfmtlong{dfo}}

As summarized in~\cite{Conn_Scheinberg_Vicente_2009b}, two main strategies have been developed for solving \gls{dfo} problems.
One strategy consists of sampling the objective function around the current iterate and choosing the next iterate among the sampled points based on simple comparisons.
The \emph{direct-search} methods~\cite{Kolda_Lewis_Torczon_2003} are based on this framework.
The other strategy builds iteratively models that approximate the problems (e.g., using polynomials) around the current iterate and choose the next iterate according to the approximated problems.
These methods are referred to as \emph{model-based} methods.
Hybrid methods also exist, such as implicit filtering~\cite{Kelley_2011}, and \gls{sidpsm}~\cite{Custodio_Rocha_Vicente_2009}, which combine direct search and models.

Some methods can solve \gls{dfo} problems but are not covered by the above categories.
Examples include random search methods~\cite{Zhigljavsky_1991}, simulated annealing methods~\cite{Kirkpatrick_Gelatt_Vecchi_1983}, the genetic algorithm~\cite{Jong_1975,Holland_1975}, and Bayesian optimization methods~\cite{Mockus_1975,Shahriari_Etal_2016}.
For more information on these methods, we refer to the review~\cite{Larson_Menickelly_Wild_2019} and the reference therein.

\subsection{Direct-search methods}

An early example of \gls{dfo} is a method from \citeauthor{Fermi_Metropolis_1952}~\cite{Fermi_Metropolis_1952}, who developed in \citeyear{Fermi_Metropolis_1952} a nonlinear least-squares solver on MANIAC, an computer based on the von Neumann architecture.
From a modern viewpoint, this method is a coordinate search method, a particular example of direct-search methods, where the search directions are defined to be the coordinate axes.

Several direct-search methods appeared thereafter.
\citeauthor{Rosenbrock_1960} designed an direct-search method for unconstrained problems.
It constructs an orthogonal basis using previous steps and searching along the directions in this basis~\cite{Rosenbrock_1960}.
Another famous early direct-search method is the Hooke-Jeeves method~\cite{Hooke_Jeeves_1961}.
This method combines exploratory moves along coordinates axes with pattern moves, aiming to exploit the pattern revealed by previous successful directions.
In \citeyear{Nelder_Mead_1965}, \citeauthor{Nelder_Mead_1965}~\cite{Nelder_Mead_1965} introduced the simplex method\footnote{Note that it is different from the simplex method in linear programming.}, which evaluates the objective function at the vertices of a simplex, and updates this simplex according to these function values, one vertex at each iteration.
It is arguably the most widely used \gls{dfo} method, available as the \verb|fminsearch| function in MATLAB, and many variation exist~\cite{Wright_2012}.
Many other works on direct search appeared in the same period and summaries can be found in~\cite{Fletcher_1965,Box_1966}.
Nowadays, the direct-search paradigm offers an abundance of algorithms~\cite{Kolda_Lewis_Torczon_2003}, such as the \gls{gps} methods~\cite{Booker_Etal_1999}, later extended to the \gls{mads} methods~\cite{Audet_Dennis_2006,Abramson_Audet_2006,Abramson_Etal_2009,Audet_Dennis_Digabel_2008,Digabel_2011}.
A very recent example of direct-search methods is the \gls{bfo}~\cite{Porcelli_Toint_2017,Porcelli_Toint_2022}, which handles integer and categorical variables, and can be self-tuned.

\Citeauthor{Gratton_Etal_2015} recently proposed to incorporate stochastic strategies in direct-search methods~\cite{Gratton_Etal_2015,Gratton_Etal_2019}.
This improves both the performance and the worst-case complexity bounds compared with traditional direct-search methods.

\subsection{Model-based methods}
\label{subsec:model-based-methods}

Unlike direct-search methods, model-based methods approximate locally the functions involved in the optimization problems by simple functions, called \emph{models} or \emph{surrogates}.
To make the methods globally convergent, the models are exploited by a globalization strategy, such as a \emph{line-search} framework~\cite[ch.~3]{Nocedal_Wright_2006} or a \emph{trust-region} framework~\cite{Conn_Gould_Toint_2000,Yuan_2015}.
Most of the existing model-based methods use linear or quadratic approximations~\cite{Powell_1994,Conn_Scheinberg_Vicente_2008a,Conn_Scheinberg_Vicente_2008b}, although other models have also been successfully used, such as \glspl{rbf}~\cite{Oeuvray_2005}.

Model-based methods are highly appealing in practice, as they provide excellent performances in real applications.
Compared with direct-search methods, the information contained in the function values is better exploited due to the use of models.
The first trust-region \gls{dfo} method was developed by~\citeauthor{Winfield_1969}~\cite{Winfield_1969,Winfield_1973} in \citeyear{Winfield_1969}.
This is also regarded as the first trust-region method with or without derivatives~\cite[\S~1.2]{Conn_Gould_Toint_2000}.
A similar method was later developed by \citeauthor{Powell_2002}, namely \gls{uobyqa}~\cite{Powell_2002}.
Both methods use fully-determined quadratic interpolating models (see \cref{sec:multivariate-interpolation} for detailed discussions).
By updating the Lagrange functions of the interpolation problem, \gls{uobyqa} needs only~$\mathcal{O}(n^4)$ computer operations to obtain each quadratic interpolant, whereas the complexity is~$\mathcal{O}(n^6)$ in \citeauthor{Winfield_1969}'s method.

The methods of both \citeauthor{Winfield_1969} and \citeauthor{Powell_2002} require~$\mathcal{O}(n^2)$ function evaluations to establish each quadratic model.
Such an amount of function values are needed to initialize the methods, although most of them will be reused at subsequent iterations.
This amount of function evaluations is however not scalable to moderately large problems.
This motivates methods that use underdetermined quadratic interpolating models.
Such models typically require~$\mathcal{O}(n)$ function values to be built, and the remaining freedom bequeathed by the interpolation conditions is taken up by minimizing a functional that reflects the regularity of the model.
For example, the method of \citeauthor{Conn_Toint_1996}~\cite{Conn_Toint_1996} uses such models with least~$\ell_2$-norm of the coefficients.
Powell's methods, namely \gls{newuoa}~\cite{Powell_2006}, \gls{bobyqa}~\cite{Powell_2009}, and \gls{lincoa}~\cite{Powell_2015}, use models that minimize the Frobenius norm of the change to their Hessian matrices.
Another example of such algorithms is \gls{mnh}~\cite{Wild_2008}, whose models are underdetermined quadratic interpolants with least Frobenius norm of their Hessian matrices.

Particular care must be given to the geometry of the interpolation set in order to maintain reasonable accuracies of the models (see \cref{sec:poisedness}).
Most model-based methods make explicit geometry-improving steps when it is necessary~\cite{Conn_Scheinberg_Vicente_2008a,Conn_Scheinberg_Vicente_2008b}.
The Wedge method~\cite{Marazzi_Nocedal_2002} of \citeauthor{Marazzi_Nocedal_2002}, however, does not include such steps.
Instead, it adds a so-called wedge constraint to the trust-region subproblems to prevent the trial steps from lying in a region that is likely to worsen the geometry of the interpolation set.
The method of \citeauthor{Fasano_Morales_Nocedal_2009}~\cite{Fasano_Morales_Nocedal_2009} does not include geometry-improving steps either but performs surprisingly well.
The authors conjecture that a self-correcting mechanism may be at play and prevents the geometry from deterioring.
\citeauthor{Scheinberg_Toint_2010}~\cite{Scheinberg_Toint_2010} point out that the geometry improvement cannot be dispensed in general, but a slight modification of the algorithm in~\cite{Fasano_Morales_Nocedal_2009} does enjoy a self-correcting mechanism that garantees the convergence without taking explicit geometry-improving steps.

There are many other trust-region \gls{dfo} methods.
For instance, \gls{csv2}~\cite{Billups_Larson_Graf_2013}, which determines its models by regression and not interpolation.
Examples of solvers that use nonpolynomial models include \gls{orbit}~\cite{Wild_Regis_Shoemaker_2008}, \gls{conorbit}~\cite{Regis_Wild_2017}, and \gls{boosters}~\cite{Oeuvray_Bierlaire_2009}, which use cubic \gls{rbf}.
There also exist methods for more specific problems, such as \gls{dfls}~\cite{Zhang_Conn_Scheinberg_2010} and \gls{dfols}~\cite{Cartis_Etal_2019}, which aim at solving nonlinear least-squares problems without using derivatives.

Randomization is also exploited to improve the performance of trust-region \gls{dfo} methods.
This idea is first proposed by \citeauthor{Bandeira_Scheinberg_Vicente_2012}~\cite{Bandeira_Scheinberg_Vicente_2012}, who establish the global convergence of a trust-region method based on random models.
They only require to approximate the objective function well enough with a certain probability, and such models can be obtained by interpolation on randomly selected points~\cite{Bandeira_Scheinberg_Vicente_2014}.
The global convergence rate of this method is established by \citeauthor{Gratton_Etal_2018}~\cite{Gratton_Etal_2018}, using an idea elaborated earlier in~\cite[\S~6]{Gratton_Etal_2015}.
Similar results are established independently by \citeauthor{Cartis_Scheinberg_2018}~\cite{Cartis_Scheinberg_2018}, but for a more general class of methods.
The work of \citeauthor{Bandeira_Scheinberg_Vicente_2012} has motivated many investigations on methods that use randomized models, such as~\cite{Chen_Menickelly_Scheinberg_2018}.

\subsection{Comments on methods based on finite differences}
\label{subsec:finite-difference}

Perhaps the most straightforward approach to solving \gls{dfo} problems is to make finite-difference approximations of the derivatives and then employ derivative-based methods (see, e.g.,~\cite{Shi_Etal_2021}).
In this section, we briefly discuss the advantages and disadvantages of this approach.

The~$i$th coordinate of the gradient of a smooth function~$\obj : \R^n \to \R$ at a point~$x \in \R^n$ can be approximated by the forward difference
\begin{equation}
    \label{eq:forward-difference}
    \frac{\partial \obj}{\partial x_i}(x) = \frac{\obj(x + h e_i) - \obj(x)}{h} + \mathcal{O}(h),
\end{equation}
or the central difference
\begin{equation}
    \label{eq:central-difference}
    \frac{\partial \obj}{\partial x_i}(x) = \frac{\obj(x + h e_i) - \obj(x - h e_i)}{2h} + \mathcal{O}(h^2),
\end{equation}
where~$h > 0$ is a given difference parameter,~$e_i \in \R^n$ is the~$i$th standard coordinate vector of~$\R^n$, and the order of precisions require standard smoothness assumptions (see, e.g.,~\cite[\S~8.1]{Nocedal_Wright_2006}).

Methods based on finite-difference have several advantages.
Firstly, they are relatively easy to implement, whereas the implementation of many commonly used \gls{dfo} methods is highly nontrivial and challenging\footnote{For example, \citeauthor{Powell_2006}~\cite{Powell_2006} wrote \enquote{The development of \gls{newuoa} has taken nearly three years. The work was very frustrating [\dots]}}.
Moreover, there are a profusion of well-established derivative-based methods that can be explored with finite-difference approximations.
Finally, we highlight that the function evaluations needed by the approximated derivatives in~\cref{eq:forward-difference,eq:central-difference} can be straightforwardly parallelized, making the resulting algorithm scalable in many situations.

Meanwhile, such methods come with disadvantages as well.
First of all, if the problem is noisy, it is nontrivial to choose the difference parameter~$h$.
From a theoretical standpoint, the optimal choice for the difference parameter depends on the noise level and the Lipschitz constants of the derivatives of the function~$\obj$ (see, e.g.,~\cite[\S~8.1]{Nocedal_Wright_2006} and~\cite[eq.~(2.13) and~(2.14)]{Shi_Etal_2021}).
There exist procedures to estimate these quantities (see, e.g.,~\cite[\S~3]{More_Wild_2011} and~\cite[proc.~I]{Shi_Etal_2021}).
However, in practice, these procedures are often costly in terms of function evaluations.
In addition, it is difficult to reuse the function values in methods based on finite differences.
This is because a finite difference at~$x \in \R^n$ require the function to be evaluated on a mesh of~$\mathcal{O}(n)$ points around~$x$ oriented along the coordinate directions.
When~$x$ changes, most of the mesh points change completely and hence, the function needs to be evaluated at another batch of~$\mathcal{O}(n)$ points.

We also mention that there is no sharp division between methods based on finite difference and those based on interpolating models.
Indeed, the forward difference~\cref{eq:forward-difference} produces the gradient of the linear function that interpolates~$f$ at
\begin{equation*}
    \set{x, x + h e_1, x + h e_2, \dots, x + h e_n}.
\end{equation*}
Similarly, the central difference~\cref{eq:central-difference} generates the gradient of the unique quadratic function with diagonal Hessian matrix that interpolates~$f$ at
\begin{equation*}
    \set{x, x + h e_1, x - h e_1, x + h e_2, x - h e_2, \dots, x + h e_n, x - h e_n}.
\end{equation*}
Interpolation can be regarded as a generalization of finite difference, in the sense that we have more freedom to choose the interpolation set.
This freedom allows us to reuse most of the interpolation points, which is essential for the efficiency of model-based methods.

In section \cref{sec:benchmarking-tools}, we will present some simple numerical experiments after introducing the benchmarking tools we use in this thesis for comparing \gls{dfo} solvers.
These results will demonstrate that the methods based on finite difference are highly sensitive to noise.
Unsurprisingly, the methods suffer if the difference parameter does not take into account the noise level and the smoothness of the functions.

\section{Benchmarking tools for \glsfmtlong{dfo} methods}
\label{sec:benchmarking-tools}

We introduce in this section the benchmarking tools that we will use throughout this thesis for the comparison of \gls{dfo} solvers.
We use the performance and data profiles~\cite{Dolan_More_2002,More_Wild_2009}, developed by \citeauthor{Dolan_More_2002} and \citeauthor{More_Wild_2009} respectively.
Most of the information in this section can be found in the aforementioned articles.

\subsection{Expense measure and convergence test}
\label{subsec:convergence-test}

We denote by~$\mathcal{S}$ the set of solvers to benchmark and~$\mathcal{P}$ a set of test problems, assumed to represent the problems for which the solvers have been designed.
Let~$t_{p, s}$ be the expense for the solver~$s \in \mathcal{S}$ to achieve a given convergence test on the problem~$p \in \mathcal{P}$.
As mentioned in \cref{sec:overview}, the major cost of \gls{dfo} in practice is the function evaluations.
Therefore,~$t_{p, s}$ usually\todo{Correct the sentence} denotes the number of function evaluations required by the solver~$s \in \mathcal{S}$ to solve the problem~$p \in \mathcal{P}$ up to the convergence test.
When the solver~$s$ fails to satisfy the test for the problem~$p$ within a given budget restriction (e.g., a maximum number of function evaluations), we define~$t_{p, s} = \infty$ by convention.

In this thesis, the numerical experiments select~$\mathcal{P}$ from the CUTEst set~\cite{Gould_Orban_Toint_2015}.
We consider the following convergence test for \gls{dfo} solvers, following~\cite[\S~2]{More_Wild_2009}.
We define for each problem~$p \in \mathcal{P}$ a merit function~$\varphi_p$, i.e., a function that measures the quality of a point, taking into account the values of both the constraint and the objective functions.
The smaller value of~$\varphi_p$, the better.
Let~$x_p^0$ be the initial guess of a given problem~$p \in \mathcal{P}$ and~$\varphi_p^{\ast}$ be the least value of~$\varphi_p$ obtained by the solvers in~$\mathcal{S}$.
Given a tolerance~$\tau \in (0, 1)$, a point~$x$ satisfies the convergence test if
\begin{equation}
    \label{eq:convergence-test-profiles}
    \varphi_p(x) \le \varphi_p^{\ast} + \tau [\varphi_p(x_p^0) - \varphi_p^{\ast}],
\end{equation}
in which case we say that~$x$ solves problem~$p$ up to the tolerance~$\tau$.
This convergence test can be interpreted as follows.
A point~$x$ satisfies the convergence test if the reduction~$\varphi_p(x_{\hat{p}}^0) - \varphi_p(x)$ is at least~$1 - \tau$ times the best possible reduction~$\varphi_p(x_p^0) - \varphi_p^{\ast}$ achieved by all solvers under consideration.

In the convergence test~\cref{eq:convergence-test-profiles}, it is tempting to define~$\varphi_p^{\ast}$ as the merit function value at a local minimizer of~$p$, which is the case when testing derivative-based solvers~\cite{Dolan_More_2002}.
However, according to~\cite{More_Wild_2009}, this may not be appropriate in a \gls{dfo} context, because it may happen that no solver in~$\mathcal{S}$ achieves the test within the given computational budget if the function evaluations are expensive.

\subsection{Performance profiles}

Fix a~$\tau \in (0, 1)$ in the convergence test~\cref{eq:convergence-test-profiles}.
For a solver~$\hat{s} \in \mathcal{S}$ and a problem~$\hat{p} \in \mathcal{P}$, define the \emph{performance ratio} by
\begin{equation*}
    r_{\hat{p}, \hat{s}} \eqdef \frac{t_{\hat{p}, \hat{s}}}{\min \set{t_{\hat{p}, s} : s \in \mathcal{S}}},
\end{equation*}
which is the relative expense for~$\hat{s}$ to solve~$\hat{p}$ compared with the most efficient solver in~$\mathcal{S}$ for this problem.
The \emph{performance profile} of~$\hat{s}$ is defined as
\begin{equation*}
    \rho_{\hat{s}}(\alpha) \eqdef \frac{1}{\card (\mathcal{P})} \card \big(\set{p \in \mathcal{P} : r_{p, \hat{s}} \le \alpha} \big), \quad \text{for~$\alpha \ge 1$},
\end{equation*}
where~$\card(\cdot)$ denotes the cardinal of a set.
Clearly,~$\rho_{\hat{s}}(\alpha)$ is the proportion of problems in~$\mathcal{P}$ that are solved by~$\hat{s}$ with a performance ratio at most~$\alpha$.
In particular, $\rho_{\hat{s}}(1)$ is the proportion of problems that~$\hat{s}$ solves faster than any other solver in~$\mathcal{S}$.
Meanwhile,
\begin{equation*}
    \lim_{\alpha \to \infty} \rho_{\hat{s}}(\alpha)
\end{equation*}
is the proportion of problems that are solved by~$\hat{s}$ within the budget restriction.

\subsection{Data profiles}

We now introduce data profiles, another benchmarking tool proposed by~\cite{More_Wild_2009}.
The \emph{data profile} of a solver~$\hat{s} \in \mathcal{S}$ is defined as
\begin{equation}
    \label{eq:data-profile}
    d_{\hat{s}}(\alpha) \eqdef \frac{1}{\card \mathcal{P}} \card \set[\bigg]{p \in \mathcal{P} : \frac{t_{p, \hat{s}}}{n_p + 1} \le \alpha}, \quad \text{for~$\alpha \ge 0$}.
\end{equation}
where~$n_p$ is the dimension of the problem~$p$.
Therefore,~$d_{\hat{s}}(\alpha)$ is the proportion of problems in~$\mathcal{P}$ that are solved by~$\hat{s}$ with at most~$\alpha (n_p + 1)$ function evaluations\todo{Change accordingly}.
In particular,~$d_{\hat{s}}(0) = 0$ and, the same as~$\rho_{\hat{s}}$,
\begin{equation*}
    \lim_{\alpha \to \infty} d_{\hat{s}}(\alpha)
\end{equation*}
is the proportion of problems that are solved by~$\hat{s}$ within the budget restriction.
In equation~\cref{eq:data-profile}, the denominator~$n_p + 1$ is a unit cost that serves to normalize $t_{p, \hat{s}}$, so that the computationl expenses for different problems are comparable even if their dimensions are quite different.
Note that~$n_p + 1$ is the number of function evaluations needed for a evaluating a simplex gradient~\cite{Bortz_Kelley_1998}.
Therefore,~$d_{\hat{s}}(\alpha)$ is the proportion of problems solved by~$\hat{s}$ given a budget equivalent to~$\alpha$ simplex gradient estimates.

\subsection{An illustrative example}

In this section, as an example, we compare three solvers using performance and data profiles.
These solvers are
\begin{itemize}
    \item \gls{newuoa} (see \cref{subsec:newuoa-bobyqa-lincoa}), a model-based \gls{dfo} method; and
    \item \gls{bfgs} and \gls{cg}, two gradient-based solvers using forward finite-difference approximate gradients, with the difference parameter~$h = \sqrt{u}$, where~$u$ is the unit roundoff.
\end{itemize}
The set~$\mathcal{P}$ contains all the smooth unconstrained CUTEst problems of dimension at most~$50$, and the convergence tolerance in~\cref{eq:convergence-test-profiles} is~$\tau = 10^{-3}$.

We conduct two experiments as follows.
\begin{enumerate}
    \item The first experiment is made without modifying the problems in~$\mathcal{P}$, and~$t_{p, s}$ is defined to be the number of function evaluations that the solver~$s \in \mathcal{S}$ needed to solve the problem~$p \in \mathcal{P}$.
    \item The second experiment we consider is a noisy variation of the previous one.
    The objective functions of the problems in~$\mathcal{P}$ are perturbed as follows.
    The objective function~$\obj_p$ of each problem~$p \in \mathcal{P}$ are replaced by
    \begin{equation*}
        \tilde{\obj}_p(x) \eqdef [1 + \epsilon(x)] \obj_p(x),
    \end{equation*}
    where~$\epsilon(x) \sim \mathcal{N}(0, \sigma^2)$ for different values of~$\sigma > 0$.
    Each problem is solved ten times with each solver, and the definition of~$\varphi_p^{\ast}$ described in \cref{subsec:convergence-test} is modified as follows. In this experiment, it is set to be the least value of the objective function obtained by all solvers on the noise-free problem and on the ten runs of the noisy problem.
    Moreover,~$t_{p, s}$ is set to the average number of function evaluations that the solver~$s \in \mathcal{S}$ needed to solve the problem~$p \in \mathcal{P}$.
\end{enumerate}

We plot the performance profiles related to this experiment in \cref{fig:performance-profile-example}.
The abscissas are the performance ratios in base-$2$ logarithmic scale and the performance profiles are on the ordinates.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{plain-1-50-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noise-free problems}
        \label{fig:performance-profile-example-noiseless}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-10-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy problems with~$\sigma = 10^{-10}$}
        \label{fig:performance-profile-example-noisy-10}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-8-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy problems with~$\sigma = 10^{-8}$}
        \label{fig:performance-profile-example-noisy-8}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-6-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy problems with~$\sigma = 10^{-6}$}
        \label{fig:performance-profile-example-noisy-6}
    \end{subfigure}
    \caption{Example of performance profiles with~$\tau = 10^{-3}$}
    \label{fig:performance-profile-example}
\end{figure}

On this example, we could argue that \gls{bfgs} provides better performance than the two other solvers on noise-free problems, but \gls{newuoa} clearly overcomes the two other solvers on noisy problems.
This behavior is perfectly expected, and does not contradict the observations of \citeauthor{Shi_Etal_2021}~\cite{Shi_Etal_2021}, as we did not modify the difference parameters of the two finite-difference based solvers.
We rather kept the default value provided for the solvers \gls{bfgs} and \gls{cg} by SciPy~\cite{Virtanen_Etal_2020}.
However, we cannot compare \gls{newuoa} and \gls{cg} on~\cref{fig:performance-profile-example-noiseless} nor \gls{bfgs} and \gls{cg} on~\cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6}.
In fact, performance profiles provide a clear measure when comparing two solvers, but they may fail comparing a solver relatively to another one that is not the best when comparing more than two solvers~\cite{Gould_Scott_2016}.

Similarly, we plot the data profiles related to this experiment in~\cref{fig:data-profile-example}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{plain-1-50-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noise-free problems}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-10-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy problems with~$\sigma = 10^{-10}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-8-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy problems with~$\sigma = 10^{-8}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-6-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy problems with~$\sigma = 10^{-6}$}
    \end{subfigure}
    \caption{Example of data profiles with~$\tau = 10^{-3}$}
    \label{fig:data-profile-example}
\end{figure}

On this example, we could observe the same results than previously.
However, based on \cref{fig:data-profile-example}, we could also argue that \gls{newuoa} overcomes \gls{cg} on smooth problems and that \gls{bfgs} and \gls{cg} provide approximately the same performance on noised problems.
This is because the comparisons made on data profiles are absolute, and the result of a solver does not influence the result of another, except for the convergence test.
Throughout this thesis, we will use both performance and data profiles to compare the performances of different \gls{dfo} solvers.
