%% contents/introduction.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Introduction}

\nomenclature[Fa]{$\obj$}{Real-valued objective function defined on~$\R^n$}%
\nomenclature[Fb]{$\con{i}$}{Real-valued constraint function defined on~$\R^n$, with~$i \in \iub \cup \ieq$}%
\nomenclature[Fc]{$\lag$}{Lagrangian function}%
\nomenclature[Na]{$\in$}{Set membership notation}%
\nomenclature[Nb]{$\subseteq$}{Set inclusion notation}%
\nomenclature[Nc]{$\qedsymbol$}{Halmos symbol}%
\nomenclature[Nd]{$A^{\mathsf{T}}$,~$v^{\mathsf{T}}$}{Transpose of a matrix or a vector}%
\nomenclature[Ne]{$I_n$}{Identity matrix on~$\R^{n \times n}$}%
\nomenclature[Nf]{$e_k$}{Standard coordinate vector of~$\R^n$ ($k$-th column of~$I_n$), with~$1 \le k \le n$}%
\nomenclature[Ng]{$\mathcal{O}(\cdot)$}{Big-O notation}%
\nomenclature[Nh]{$o(\cdot)$}{Little-O notation}%
\nomenclature[Oa]{$[\cdot]_{+}$}{Elementwise positive-part operator}%
\nomenclature[Ob]{$[\cdot]_{-}$}{Elementwise negative-part operator}%
\nomenclature[Oc]{$\abs{\cdot}$}{Elementwise modulus operator}%
\nomenclature[Od]{$\inner{\cdot, \cdot}$}{Inner-product operator (may be subscripted for sake of clarity)}%
\nomenclature[Oe]{$\norm{\cdot}$}{Norm of a vector or a matrix (may be subscripted for sake of clarity)}%
\nomenclature[Of]{$\nabla$}{Gradient operator (elements~$\partial / \partial x_i$, with~$i \in \set{1, 2, \dots, n}$)}%
\nomenclature[Og]{$\nabla^2$}{Hessian operator (elements~$\partial^2 / \partial x_i \partial x_j$, with~$i, j \in \set{1, 2, \dots, n}$)}%
\nomenclature[Oh]{$\land$}{Logic and operator}%
\nomenclature[Sa]{$\emptyset$}{Empty set}%
\nomenclature[Sb]{$[a, b]$}{Closed set~$\set{x \in \R : a \le x \le b}$ with~$a \le b$}%
\nomenclature[Sc]{$(a, b)$}{Open set~$\set{x \in \R : a < x < b}$ with~$a < b$}%
\nomenclature[Sd]{$[a, b)$}{Semi-open set~$\set{x \in \R : a \le x < b}$ with~$a < b$}%
\nomenclature[Se]{$(a, b]$}{Semi-open set~$\set{x \in \R : a < x \le b}$ with~$a < b$}%
\nomenclature[Sf]{$\R$}{Set of real numbers}%
\nomenclature[Sg]{$\R^n$}{Real coordinate space of dimension~$n$}%
\nomenclature[Sh]{$\R^{m \times n}$}{Real matrix space of dimension~$m \times n$}%
\nomenclature[Si]{$\Omega$}{Feasible set, included in~$\R^n$}%
\nomenclature[Sj]{$\ieq$}{Set of indices of the equality constraints}%
\nomenclature[Sk]{$\iub$}{Set of indices of the inequality constraints}%
\todo[noline]{Replace the nomenclature items.}

\section{Overview of \glsfmtlong{dfo}}

Optimization is the study of extremal points and values of mathematical functions.
Traditional optimization aims at minimizing (or maximizing) a real-valued function~$\obj$, referred to as the \emph{objective function}, within a given nonempty set of points~$\Omega \subseteq \R^n$, referred to as the \emph{feasible set}.
It is well known that the most helpful information to optimization is embraced in the derivatives of~$\obj$ and the curves of~$\Omega$.
However, such derivative information may not exist, be unassessable, unreliable, or prohibitively expensive to evaluate.
It is within this context, referred to as \gls{dfo}~\cite{Audet_Hare_2017,Conn_Scheinberg_Vicente_2009b}, that our research focuses.
Problems for which derivative information is unavailable arise naturally when the objective function or the feasible set results from complex experiments or simulations.
We emphasize that we are \emph{not} studying nonsmooth optimization.
Classical or generalized derivatives of the optimization problem's functions may be well-defined, but we assume they cannot be numerically assessed.

The leading complexity measure we consider is the number of objective function evaluations.
We presume that an objective function evaluation dawdles and requires several minutes or even several days to complete.
For instance, a modern application of \gls{dfo} is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}, for which one objective function evaluation necessitates training a machine learning model (see~\cref{subsec:machine-learning}).
Hence, the algebraic complexity of the methods we consider is not our leading issue, although we will strive to maintain it acceptable.

Within this chapter, we consider the nonlinearly-constrained problem
\begin{subequations}
    \label{eq:nlcp-intro}
    \begin{align}
        \min        & \quad \obj(x)\\
        \text{s.t.} & \quad \con{i}(x) \le 0, ~ i \in \iub, \label{eq:nlcp-intro-cub}\\
                    & \quad \con{i}(x) = 0, ~ i \in \ieq, \label{eq:nlcp-intro-ceq}\\
                    & \quad x \in \R^n, \nonumber
    \end{align}
\end{subequations}
where the \emph{objective} and \emph{constraint functions}~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are real-valued functions on~$\R^n$, and where the sets of indices~$\iub$ and~$\ieq$ are finite (perhaps empty).
The above-mentioned feasible set~$\Omega \subseteq \R^n$ is, therefore, the set of points satisfying the inequality constraints~\cref{eq:nlcp-intro-cub} and the equality constraints~\cref{eq:nlcp-intro-ceq}, that is
\begin{equation*}
    \Omega \eqdef \set{x \in \R^n : \con{i}(x) \le 0, ~ i \in \iub \land \con{i}(x) = 0, ~ i \in \ieq}.
\end{equation*}

\section{Examples of \glsfmtlong{dfo} applications}

\subsection{Automatic error analysis}

A typical example of \gls{dfo} applications is automatic error analysis~\cite{Higham_1993,Higham_2002}, which formulates numerical computation's accuracies and stabilities using optimization problems.
Consider, for instance, the Gaussian elimination with partial pivoting of a matrix~$A \in \R^{n \times n}$, given in~\cref{alg:gepp}.

\begin{algorithm}[htp]
    \DontPrintSemicolon
    \KwData{Matrix~$A \in \R^{n \times n}$.}
    Initialize $A^{(0)} \gets A$\;
    \For{$k = 1, 2, \dots, n - 1$}{
        Determine the pivot index~$j = \argmax \set[\big]{\abs[\big]{A_{i, k}^{(k - 1)}} : k \le i \le n}$\;
        \eIf{$A_{j, k}^{(k - 1)} \neq 0$}{
            Exchange the~$k$th and the~$j$th rows of~$A^{(k - 1)}$\;
            Evaluate the multiplier~$\tau^{(k)} \in \R^n$ with components
            \begin{algomathdisplay}
                \tau_i^{(k)} =
                \begin{cases}
                    A_{i, k}^{(k - 1)} / A_{k, k}^{(k - 1)} & \text{if~$i > k$, and}\\
                    0                                       & \text{otherwise}
                \end{cases}
            \end{algomathdisplay}
            Update~$A^{(k)} \gets (I_n - \tau^{(k)} e_k^{\mathsf{T}})A^{(k - 1)}$\;
        }{
            Set~$A^{(k)} \gets A^{(k - 1)}$\;
        }
    }
    \caption{Gaussian elimination with partial pivoting}
    \label{alg:gepp}
\end{algorithm}

\Citeauthor{Wilkinson_1961}'s backward error analysis~\cite{Wilkinson_1961} demonstrates that the growth factor
\begin{equation}
    \label{eq:gepp-growth-factor}
    \rho_n(A) \eqdef \frac{\max_{0 \le k \le n - 1} \norm{A^{(k)}}_{\max}}{\norm{A}_{\max}},
\end{equation}
determinates the numerical stability of~\cref{alg:gepp}, where~$\norm{\cdot}_{\max}$ denotes the max norm of a matrix, i.e., the largest entry of the matrix in absolute value.
More specifically, the~$\ell_{\infty}$-norm of the backward error of the computed solution is bounded from above by a term proportional to~$\rho_n(A)$.
To study the worse-case scenario of~\cref{alg:gepp}, we wish to determine how large~$\rho_n$ can be and hence, to solve
\begin{equation}
    \label{eq:gepp-opti}
    \max_{A \in \R^{n \times n}} \rho_n(A).
\end{equation}
Note that~$\R^{n \times n}$ is isomorphic to~$\R^{n^2}$ and hence, problem~\cref{eq:gepp-opti} could straightforwardly be formulated as problem~\cref{eq:nlcp-intro}.
Besides, although the growth factor is defined everywhere, it may not be continuous at the points yielding a tie in the selection of the pivot element.
Moreover, it is not differentiable at the points yielding a tie in any maximum operator in equation~\cref{eq:gepp-growth-factor}.
Hence, optimization methods based on derivative information cannot be used for such a problem.
In such a case, \gls{dfo} methods can help determine the optimal solution to problem~\cref{eq:gepp-opti}, and \citeauthor{Higham_Higham_1989} have used \gls{dfo} methods to determine the set of matrices yielding the optimal solution~\cite{Higham_Higham_1989}.

\subsection{Tuning nonlinear optimization methods}

Another well-known example of \gls{dfo} applications is the parameter tuning of nonlinear optimization methods~\cite{Audet_Orban_2006}.
Consider, for example, the simplified version of the direct-search method for solving problem~\cref{eq:nlcp-intro} when~$\iub = \ieq = \emptyset$ given in~\cref{alg:direct-search}.

\begin{algorithm}[htp]
    \DontPrintSemicolon
    \KwData{Objective function~$f$, positive spanning set~$\mathcal{D} \subset \R^n$, initial guess~$x^{(0)} \in \R^n$, steplengths~$0 < \alpha_{\min} < \alpha_0 < \alpha_{\max}$, and parameters~$0 < \theta < 1 \le \gamma$.}
    \For{$k = 0, 1, \dots$}{
        Evaluate~$d^{(k)} = \argmin \set{f(x^{(k)} + \alpha_k d) : d \in \mathcal{D}}$\;
        \eIf{$f(x^{(k)} + \alpha_k d^{(k)}) < f(x^{(k)})$}{
            Update~$x^{(k + 1)} \gets x^{(k)} + \alpha_k d^{(k)}$\;
            Increase the steplength~$\alpha_{k + 1} = \min \set{\gamma \alpha_k, \alpha_{\max}}$
        }{
            Update~$x^{(k + 1)} \gets x^{(k)}$\;
            Decrease the steplength~$\alpha_{k + 1} = \max \set{\theta \alpha_k, \alpha_{\min}}$
        }
        \If{$\alpha_k = \alpha_{\min}$}{
            Stop the computations\;
        }
    }
    \caption{Direct search for unconstrained optimization}
    \label{alg:direct-search}
\end{algorithm}

Such an algorithm depends on two parameters, namely~$\theta$ and~$\gamma$.
To choose those parameters, we wish to minimize some measure of the method's performance (e.g., the sum of the number of function evaluations required to solve a given set of optimization problems),~$\rho$ say.
In other words, we wish to solve the optimization problem
\begin{subequations}
    \label{eq:tuning-opti}
    \begin{align}
        \min        & \quad \rho(\theta, \gamma) \label{eq:tuning-opti-obj}\\
        \text{s.t.} & \quad \theta \in (0, 1), \label{eq:tuning-opti-theta}\\
                    & \quad \gamma \ge 1. \label{eq:tuning-opti-gamma}
    \end{align}
\end{subequations}
Derivatives of~$\rho$ are unassessable and may not even exist.
Such a problem is then solved using \gls{dfo} methods.
In this example, since the considered optimization method is itself a \gls{dfo} method, we could even solve problem~\cref{eq:tuning-opti} using~\cref{alg:direct-search} given some estimates of the parameters~$\theta$ and~$\gamma$.

\subsection{Hyperparameter tuning in machine learning}
\label{subsec:machine-learning}

A newfangled example of \gls{dfo} applications is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}.
Consider, for example, the following~$k$-ary classification problem.
Let~$\mathcal{X}$ be any given finite vector space.
Assume that we want to arrange each element of~$\mathcal{X}$ into one of~$k$ given categories.
To that end, we are given a finite dataset of labelled elements~$\set{(x_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \mathcal{X} \times \set{1, 2, \dots, k}$ and a machine learning model~$\obj_p : \mathcal{X} \times \mathcal{W} \to [0, 1]^k$, where~$\mathcal{W}$ denotes the finite vector space of the internal coefficients of~$\obj_p$.
Moreover, the machine learning model~$\obj_p$ depends on the fixed vector of hyperparameters~$p \in \Omega \subseteq \R^n$.

%\begin{figure}[htp]
%    \centering
%    \begin{tikzpicture}[every text node part/.style={align=center}]
%        \draw[thick,rounded corners] (0,0) rectangle (3,1.5);
%        \draw[thick,rounded corners,pattern=north west lines,pattern color=OliveGreen!50] (0,2) rectangle (3,3.5);
%        \draw[thick,rounded corners] (0,4) rectangle (3,5.5);
%        \draw[thick,rounded corners] (4,2) rectangle (7,3.5);
%        \draw[thick,rounded corners] (8,0.75) rectangle (11,2.25);
%        \draw[thick,rounded corners,pattern=north west lines,pattern color=OliveGreen!50] (8,3.25) rectangle (11,4.75);
%        \draw[thick] (3,0.5) -- (7.5,0.5) -- (7.5,2.5) -- (7,2.5);
%        \draw[thick] (3,5) -- (7.5,5) -- (7.5,3) -- (7,3);
%        \draw[thick,-latex] (3,1) -- (5.5,1) -- (5.5,2);
%        \draw[thick,-latex] (3,2.75) -- (4,2.75);
%        \draw[thick,-latex] (7.5,1.5) -- (8,1.5);
%        \draw[thick,-latex] (7.5,4) -- (8,4);
%        \node at (1.5,0.75) {Training\\ dataset};
%        \node at (1.5,2.75) {Hyper-\\ parameters};
%        \node at (1.5,4.75) {Testing\\ dataset};
%        \node at (5.5,2.75) {Machine\\ learning};
%        \node at (9.5,1.5) {Training\\ accuracy};
%        \node at (9.5,4) {Testing\\ accuracy};
%    \end{tikzpicture}
%    \caption{Training and testing flow of a machine learning model}
%\end{figure}

Reinforcement learning~\cite{Qian_Yu_2021}

\subsection{Some industrial applications}

Other industrial examples of \gls{dfo} applications include molecular conformational analysis~\cite{Alberto_Etal_2004,Meza_Martinez_1994}, helicopter rotor blade design~\cite{Booker_Etal_1998a,Booker_Etal_1998b,Serafini_1998}, groundwater supply and bioremediation engineering~\cite{Fowler_Etal_2008,Mugunthan_Shoemaker_Regis_2005,Yoon_Shoemaker_1999}, aeroacoustic shape design~\cite{Marsden_2004,Marsden_Etal_2004}, hydrodynamic design~\cite{Duvigneau_Visonneau_2004}, registration in medical imaging~\cite{Oeuvray_2005,Oeuvray_Bierlaire_2007}, dynamic pricing~\cite{Levina_Etal_2009}, reservoir engineering and engine calibration~\cite{Langouet_2011}, analog circuit design~\cite{Latorre_Etal_2019}, aircraft engine engineering~\cite{Gazaix_Etal_2019}, and chemical product design~\cite{Sun_Etal_2020} for instance.

\section{Optimality conditions for smooth optimization}

\subsection{Local and global solutions}

\begin{definition}[Global solution]
    A point~$x^{\ast} \in \R^n$ is referred to as a \emph{global solution} to problem~\cref{eq:nlcp-intro} if~$x^{\ast} \in \Omega$ and~$f(x) \ge f(x^{\ast})$ for all~$x \in \Omega$.
\end{definition}

\begin{definition}[Local solution]
    A points~$x^{\ast} \in \R^n$ is referred to as
    \begin{itemize}
        \item a \emph{local solution} to problem~\cref{eq:nlcp-intro} if~$x^{\ast} \in \Omega$ and there exists an open neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$f(x) \ge f(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega$.
        \item a \emph{strict local solution} to problem~\cref{eq:nlcp-intro} if~$x^{\ast} \in \Omega$ and there exists an open neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$f(x) > f(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega \setminus \set{x^{\ast}}$.
        \item an \emph{isolated local solution} to problem~\cref{eq:nlcp-intro} if there exists an open neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that it is the only local solution in~$\mathcal{N} \cap \Omega$.
    \end{itemize}
\end{definition}

\subsection{Constraint qualifications}

\begin{definition}[Active set]
    The \emph{active set}~$\mathcal{A}(x) \subseteq \iub \cup \ieq$ for problem~\cref{eq:nlcp-intro} at a point~$x \in \Omega$ is defined by
    \begin{equation*}
        \mathcal{A}(x) \eqdef \ieq \cup \set{i \in \iub : \con{i}(x) \ge 0}.
    \end{equation*}
\end{definition}

\begin{definition}[Constraint qualification]
    Let~$x \in \Omega$ be any feasible point, denote~$\mathcal{A}(x)$ the active set for problem~\cref{eq:nlcp-intro} at~$x$, and assume that the constraints function~$\con{i}$ are differentiable at~$x$ for all~$i \in \mathcal{A}(x)$.
    We say that
    \begin{itemize}
        \item the \gls{licq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \mathcal{A}(x)$, and
        \item the \gls{mfcq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \ieq$ and there exists a vector~$d \in \R^n$ such that
        \begin{equation*}
            \begin{cases}
                \inner{\nabla \con{i}(x), d} < 0    & \text{if~$i \in \mathcal{A}(x) \cap \iub$, and}\\
                \inner{\nabla \con{i}(x), d} = 0    & \text{if~$i \in \ieq$}.
            \end{cases}
        \end{equation*}
    \end{itemize}
\end{definition}

\begin{itemize}
    \item \gls{acq}.
    \item \gls{gcq}.
    \item \gls{lcq}.
    \item \gls{crcq}.
    \item \gls{cpld}.
    \item \gls{qncq}.
    \item \gls{sc}.
\end{itemize}

\subsection{First-order optimality conditions}

\subsection{Second-order optimality conditions}

\section{Methodology of \glsfmtlong{dfo} algorithms}

\subsection{Frameworks and algorithms for \glsfmtlong{dfo}}

\begin{itemize}
    \item Direct-search and model-based methods.
    \item Line-search and trust-region methods.
    \item Filter methods and hybrid methods.
    \item implicit filtering methods (hybrid between direct-search and line-search).
\end{itemize}

\subsection{Examples of \glsfmtlong{dfo} methods}

\section{Benchmarking tools for \glsfmtlong{dfo} methods}
