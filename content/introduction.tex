%% contents/introduction.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Introduction}

\nomenclature[Fa]{$\obj$}{Real-valued objective function defined on~$\R^n$}%
\nomenclature[Fb]{$\con{i}$}{Real-valued constraint function defined on~$\R^n$, with~$i \in \iub \cup \ieq$}%
\nomenclature[Fc]{$\lag$}{Lagrangian function}%
\nomenclature[Na]{$\in$}{Set membership notation}%
\nomenclature[Nb]{$\subseteq$}{Set inclusion notation}%
\nomenclature[Nc]{$\qedsymbol$}{Halmos symbol}%
\nomenclature[Nd]{$A^{\T}$,~$v^{\T}$}{Transpose of a matrix or a vector}%
\nomenclature[Ne]{$I_n$}{Identity matrix on~$\R^{n \times n}$}%
\nomenclature[Nf]{$e_k$}{Standard coordinate vector of~$\R^n$ ($k$-th column of~$I_n$), with~$1 \le k \le n$}%
\nomenclature[Ng]{$\mathcal{O}(\cdot)$}{Big-O notation}%
\nomenclature[Nh]{$o(\cdot)$}{Little-O notation}%
\nomenclature[Oa]{$[\cdot]_{+}$}{Elementwise positive-part operator}%
\nomenclature[Ob]{$[\cdot]_{-}$}{Elementwise negative-part operator}%
\nomenclature[Oc]{$\abs{\cdot}$}{Elementwise modulus operator}%
\nomenclature[Od]{$\inner{\cdot, \cdot}$}{Inner-product operator (may be subscripted for sake of clarity)}%
\nomenclature[Oe]{$\norm{\cdot}$}{Norm of a vector or a matrix (may be subscripted for sake of clarity)}%
\nomenclature[Of]{$\nabla$}{Gradient operator (elements~$\partial / \partial x_i$, with~$i \in \set{1, 2, \dots, n}$)}%
\nomenclature[Og]{$\nabla^2$}{Hessian operator (elements~$\partial^2 / \partial x_i \partial x_j$, with~$i, j \in \set{1, 2, \dots, n}$)}%
\nomenclature[Oh]{$\land$}{Logic and operator}%
\nomenclature[Sa]{$\emptyset$}{Empty set}%
\nomenclature[Sb]{$[a, b]$}{Closed set~$\set{x \in \R : a \le x \le b}$ with~$a \le b$}%
\nomenclature[Sc]{$(a, b)$}{Open set~$\set{x \in \R : a < x < b}$ with~$a < b$}%
\nomenclature[Sd]{$[a, b)$}{Semi-open set~$\set{x \in \R : a \le x < b}$ with~$a < b$}%
\nomenclature[Se]{$(a, b]$}{Semi-open set~$\set{x \in \R : a < x \le b}$ with~$a < b$}%
\nomenclature[Sf]{$\R$}{Set of real numbers}%
\nomenclature[Sg]{$\R^n$}{Real coordinate space of dimension~$n$}%
\nomenclature[Sh]{$\R^{m \times n}$}{Real matrix space of dimension~$m \times n$}%
\nomenclature[Si]{$\Omega$}{Feasible set, included in~$\R^n$}%
\nomenclature[Sj]{$\ieq$}{Set of indices of the equality constraints}%
\nomenclature[Sk]{$\iub$}{Set of indices of the inequality constraints}%
\todo[noline]{Replace the nomenclature items}

\section{Overview of \glsfmtlong{dfo}}

Optimization is the study of extremal points and values of mathematical functions.
It aims at minimizing (or maximizing) a real-valued function~$\obj$, referred to as the \emph{objective function}, within a given set of points~$\Omega \subseteq \R^n$, referred to as the \emph{feasible set}.
It is well known that essential information to optimization is embraced in the (possibly generalized) derivatives of the functions involved.
However, in practice, evaluations of such derivatives may be unreliable, or prohibitively expensive, if not impossible.
It motivates the study of \gls{dfo}~\cite{Conn_Scheinberg_Vicente_2009b,Audet_Hare_2017,Custodio_Scheinberg_Vicente_2017,Larson_Menickelly_Wild_2019}, where problems are solved using only function values.
This thesis focuses on methods and software for \gls{dfo}.

\Gls{dfo} problems arise naturally when the objective function or the feasible set results from complex experiments or simulations.
Regarding these functions as black boxes, people often refer to those problems as \gls{bbo} problems~\cite{Audet_Hare_2017}, which constitute a major type of \gls{dfo} problems in practice.
Note that \gls{dfo} differs from nonsmooth optimization~\cite{Clark_1983,Cui_Pang_2021}, which studies problems that involve nonsmooth functions.
In \gls{dfo}, the major difficulty is not the possible nonsmoothness of the functions involved, but the lack of knowledge about the structures of the problems.
In theoretical analysis of \gls{dfo} methods, it is not uncommon to assume that the underlying functions enjoy some smoothness, although algorithms cannot retrieve their (classical or generalized) derivatives.
We emphasize that if any derivative information can be evaluated at an affordable cost or approximated well enough, \gls{dfo} methods are not recommended, as they are very unlikely to outperform methods that use derivatives.
Consider for example minimizing an objective function defined by a sophisticated simulation whose source code is available.
One may then attempt to evaluate derivatives using automatic differentiation tools~\cite{Griewank_2003,Griewank_Walther_2008} and apply derivative-based methods.

For \gls{dfo} methods, the leading complexity measure we consider is the number of function evaluations.
In practice, each function evaluation may require several minutes or even several days to complete~\cite[\S~1.4]{Audet_Hare_2017}.
For instance, a recent application of \gls{dfo} is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}, for which every objective function evaluation necessitates training a machine learning model (see \cref{subsec:machine-learning}).
Hence, in \gls{dfo} methods, the expense of numerical linear algebra is less of a concern, although we will maintain it acceptable.

In this introduction, we consider the nonlinearly-constrained problem
\begin{subequations}
    \label{eq:problem-introduction}
    \begin{align}
        \min        & \quad \obj(x)\\
        \text{s.t.} & \quad \con{i}(x) \le 0, ~ i \in \iub, \label{eq:problem-introduction-cub}\\
                    & \quad \con{i}(x) = 0, ~ i \in \ieq, \label{eq:problem-introduction-ceq}\\
                    & \quad x \in \R^n, \nonumber
    \end{align}
\end{subequations}
where the \emph{objective} and \emph{constraint functions}~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are real-valued functions on~$\R^n$, and where the sets of indices~$\iub$ and~$\ieq$ are finite (perhaps empty) and disjoint.
The feasible set of this problem is
\begin{equation*}
    \Omega \eqdef \set{x \in \R^n : \con{i}(x) \le 0 ~ \text{for~$i \in \iub$}, ~ \con{i}(x) = 0 ~ \text{for~$i \in \ieq$}}.
\end{equation*}
If~$\obj$ is convex, while~$\con{i}$ are convex for~$i \in \iub$ and affine for~$i \in \ieq$, then problem~\cref{eq:problem-introduction} is said \emph{convex}.
However, in this thesis, we do \emph{not} assume convexity for problem~\cref{eq:problem-introduction}.

We emphasize that~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq} may include bound constraints.
We do not extract them explicitly in this chapter, but they may need to be handled differently from other constraints, because they often represent inalienable physical or theoretical restrictions.
We will later in this thesis introduce a new \gls{dfo} method, namely \gls{cobyqa}, which will take this into consideration (see \cref{ch:cobyqa-introduction}).

\section{Examples of applications}

\subsection{Automatic error analysis}

A typical example of \gls{dfo} applications is automatic error analysis~\cite{Higham_1993,Higham_2002}, which formulates numerical computation's accuracies and stabilities using optimization problems.
Consider, for instance, the Gaussian elimination with partial pivoting of a matrix~$A \in \R^{n \times n}$, given in \cref{alg:gaussian-elimination}, where the superscripts denote iteration numbers.

\begin{algorithm}[ht]
    \caption{Gaussian elimination with partial pivoting}
    \label{alg:gaussian-elimination}
    \DontPrintSemicolon
    \KwData{Matrix~$A \in \R^{n \times n}$.}
    Initialize $A^{(0)} \gets A$\;
    \For{$k = 1, 2, \dots, n - 1$}{
        Determine the pivot index~$j = \argmax \set[\big]{\abs[\big]{A_{i, k}^{(k - 1)}} : k \le i \le n}$\;
        \eIf{$A_{j, k}^{(k - 1)} \neq 0$}{
            Exchange the~$k$th and the~$j$th rows of~$A^{(k - 1)}$\;
            Evaluate the multiplier~$\tau^k \in \R^n$ with components
            \begin{algoempheq}[left={\tau_i^k = \empheqlbrace}]{alignat*=2}
                & A_{i, k}^{(k - 1)} / A_{k, k}^{(k - 1)}   && \quad \text{if~$i > k$, and}\\
                & 0                                         && \quad \text{otherwise}
            \end{algoempheq}
            Update~$A^{(k)} \gets (I_n - \tau^k e_k^{\T})A^{(k - 1)}$\;
        }{
            Set~$A^{(k)} \gets A^{(k - 1)}$\;
        }
    }
\end{algorithm}

\todo[noline]{Write down explicitly the conventions on the subscripts and superscripts.}

\Citeauthor{Wilkinson_1963}'s backward error analysis (see, e.g., equation~(25.14) of chapter~3 in~\cite{Wilkinson_1963}, where~$t$ is introduced at the beginning of paragraph~10, and~$g$ at the end of page~97) demonstrates that the growth factor of the Gaussian elimination, defined as
\begin{equation}
    \label{eq:gaussian-elimination-growth-factor}
    \rho_n(A) \eqdef \frac{\max_{0 \le k \le n - 1} \norm{A^{(k)}}_{\max}}{\norm{A}_{\max}},
\end{equation}
determinates the numerical stability of \cref{alg:gaussian-elimination}, where~$\norm{\cdot}_{\max}$ denotes the max norm of a matrix, i.e., the largest absolute value of the matrix's entries.
More specifically, the~$\ell_{\infty}$-norm of the backward error of the computed solution is bounded from above by a term proportional to~$\rho_n(A)$.
To study the worst-case scenario, we wish to determine how large~$\rho_n$ can be and hence, to solve
\begin{equation}
    \label{eq:gaussian-elimination-problem}
    \max_{A \in \R^{n \times n}} \rho_n(A).
\end{equation}
Note that~$\R^{n \times n}$ is isomorphic to~$\R^{n^2}$ and hence, problem~\cref{eq:gaussian-elimination-problem} can straightforwardly be formulated as problem~\cref{eq:problem-introduction}.
Besides, although the growth factor is defined everywhere, it may not be continuous at the points yielding a tie in the selection of the pivot element.
Moreover, it is not differentiable at the points yielding a tie in any maximum operator in equation~\cref{eq:gaussian-elimination-growth-factor}.
Hence, optimization methods based on derivative information cannot be used for this problem.
In such a case, \gls{dfo} methods can help solving problem~\cref{eq:gaussian-elimination-problem}.
Note that the optimal value and all local solutions to problem~\cref{eq:gaussian-elimination-problem} are known~\cite{Higham_Higham_1989}, but \gls{dfo} methods can be used to help the theoretical development~\cite{Higham_1993}.

\subsection{Tuning nonlinear optimization methods}

Another well-known example of \gls{dfo} applications is the parameter tuning of nonlinear optimization methods~\cite{Audet_Orban_2006}.
For example, consider \cref{alg:trust-region}, a basic trust-region method for solving problem~\cref{eq:problem-introduction} when~$\iub = \ieq = \emptyset$.

\begin{algorithm}[ht]
    \caption{Basic trust-region method for unconstrained optimization}
    \label{alg:trust-region}
    \DontPrintSemicolon
    \KwData{Objective function~$\obj$, initial guess~$x^0 \in \R^n$, initial trust-region radius~$\Delta_0 > 0$, and parameters~$0 < \eta_1 \le \eta_2 < 1$ and~$0 < \theta_1 < 1 < \theta_2$.}
    \For{$k = 0, 1, \dots$}{
        Define a simple function~$m_k$ such that~$m_k(d) \approx f(x^k + d)$ for~$\norm{d} \le \Delta_k$\;
        Set the trial step~$d^k$ to an approximate solution to
        \begin{algomathdisplay}
            \begin{aligned}
                \min        & \quad m_k(d)\\
                \text{s.t.} & \quad \norm{d} \le \Delta_k,\\
                            & \quad d \in \R^n
            \end{aligned}
        \end{algomathdisplay}
        Evaluate the trust-region ratio
        \begin{algomathdisplay}
            \rho_k \gets \frac{\obj(x^k) - \obj(x^k + d^k)}{m_k(0) - m_k(d^k)}
        \end{algomathdisplay}
        \eIf{$\rho_k \ge \eta_1$}{ \nllabel{alg:trust-region-success}
            Update the trial point~$x^{k + 1} \gets x^k + d^k$\;
        }{
            Retain the trial point~$x^{k + 1} \gets x^k$\;
        }
        Update the trust-region radius
        \begin{algoempheq}[left={\Delta_{k + 1} \gets \empheqlbrace}]{alignat*=2}
            & \theta_1 \Delta_k && \quad \text{if~$\rho_k \le \eta_1$,}\\
            & \Delta_k          && \quad \text{if~$\eta_1 < \rho_k \le \eta_2$, and}\\
            & \theta_2 \Delta_k && \quad \text{otherwise}
        \end{algoempheq}
    }
\end{algorithm}

The most important simplification in \cref{alg:trust-region} lies in \cref{alg:trust-region-success}.
A complete framework includes a parameter~$\eta_0 \ge 0$ satisfying~$\eta_0 \le \eta_1$, and the condition in \cref{alg:trust-region-success} is replaced by~$\rho_k \ge \eta_0$.
In practice, we usually have~$\eta_0 = 0$.
However, this parameter genuinely complexifies the theoretical analysis of the trust-region method, and hence, we omit it here for sake of simplicity.
We consider only the four parameters~$\eta_1$,~$\eta_2$,~$\theta_1$, and~$\theta_2$.
To choose those parameters, we minimize some measure of the method's expense (e.g., the \glsxtrshort{cpu} time to solve a given set of optimization problems),~$C$ say.
In other words, we wish to solve the optimization problem
\begin{subequations}
    \label{eq:tuning-algorithms-problem}
    \begin{align}
        \min        & \quad C(\eta_1, \eta_2, \theta_1, \theta_2)\\
        \text{s.t.} & \quad 0 \le \eta_1 \le \eta_2 < 1,\\
                    & \quad 0 < \theta_1 < 1 < \theta_2.
    \end{align}
\end{subequations}
Derivatives of~$C$ cannot be evaluated if they even exist.
Such a problem is then solved using \gls{dfo} methods, for example using the \gls{mads} method~\cite{Audet_Orban_2006}.
\Citeauthor{Audet_Digabel_Tribes_2019} modified the \gls{mads} method to solve problem~\cref{eq:tuning-algorithms-problem} with a controlled number of significant digits~\cite{Audet_Digabel_Tribes_2019}, to determine parameters that can be used by practitioners.
Interestingly, \gls{dfo} methods can be self-tuned using the method presented above.
The \gls{bfo} solver~\cite{Porcelli_Toint_2017}, a method for bound-constrained problems mixing continuous and discrete variables, is an example of self-tuned \gls{dfo} methods.

\subsection{Hyperparameter tuning in machine learning}
\label{subsec:machine-learning}

A more recent example of \gls{dfo} applications is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}.
For instance, Google solves hyperparameter tuning problems using Google Vizier~\cite{Golovin_Etal_2017}, the Google-internal service for performing black-box optimization.
To illustrate this example, we consider the following hyperparameter tuning problem of a \gls{svm} for binary classification.
Given a binary-labeled dataset~$\set{(x_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$, we build a \gls{svm} to classify the data with their respective labels.
A binary classification is obtained using a~$C$-\gls{svc}~\cite{Chang_Lin_2011} by solving the optimization problem
\begin{subequations}
    \label{eq:csvc}
    \begin{align}
        \min        & \quad \frac{1}{2} \norm{\omega}_2^2 + C \norm{\xi}_1\\
        \text{s.t.} & \quad y_i (\beta + \inner{\omega, \varphi_{\gamma}(x_i)}) \ge 1 - \xi_i, ~ i \in \set{1, 2, \dots, m},\\
                    & \quad \xi \ge 0,\\
                    & \quad (\omega, \beta, \xi) \in \R^{\ell} \times \R \times \R^m,
    \end{align}
\end{subequations}
where~$\varphi_{\gamma}$ is a function mapping the data to a higher-dimensional space~$\R^{\ell}$ and~$\gamma > 0$ and~$C > 0$ are parameters.
Given a solution~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast}) \in \R^{\ell} \times \R \times \R^m$ to problem~\cref{eq:csvc}, the~$C$-\gls{svc} classifies any data~$x \in \R^n$ according to
\begin{equation}
    \label{eq:csvc-classifier}
    \delta(x) \eqdef \sgn(\beta^{\ast} + \inner{\omega^{\ast}, \varphi_{\gamma}(x)}),
\end{equation}
which maps an observation~$x \in \R^n$ to a label in~$\set{\pm 1}$.
It is clear that~$\delta$ depends on the two parameters~$C$ and~$\gamma$, which can be chosen by solving an optimization problem.
The objective function~$P$ of this problem is a~$5$-fold cross-validation based on some performance measure of the model~\cref{eq:csvc-classifier}.
The general~$k$-fold cross-validation to define~$P(C, \gamma)$ is presented in \cref{alg:cross-validation}.

\begin{algorithm}[ht]
    \caption{$k$-fold cross-validation of an \glsfmtshort{svc} with parameters~$C$ and~$\gamma$}
    \label{alg:cross-validation}
    \DontPrintSemicolon
    \KwData{Labeled dataset~$\set{(x_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$ and fold number~$k > 0$.}
    Split the dataset into~$k$ balanced groups\;
    \For{$i = 1, 2, \dots, k$}{
        Calculate~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast})$ with the all the data except those in the~$i$th group\;
        Evaluate the performance~$p_i$ of the model~\cref{eq:csvc-classifier} on the data in the~$i$th group\;
    }
    Define~$P(C, \gamma)$ by summarizing the performances~$\set{p_1, p_2, \dots, p_k}$\;
\end{algorithm}

A typical example of model's performance used in the~$k$-fold cross-validation is the model's accuracy, i.e., the percentage of data correctly classified.
The~\gls{auc}~\cite{Hanley_Mcneil_1982} is another example of performance measure, particularly effective for imbalanced datasets~\cite{Bradley_1997}.
The hyperparameter tuning problem can be formulated as
\begin{equation*}
    \begin{aligned}
        \min        & \quad P(C, \gamma)\\
        \text{s.t.} & \quad C > 0,\\
                    & \quad \gamma > 0.
    \end{aligned}
\end{equation*}
It is clear that derivatives of the objective function of such a problem cannot be easily evaluated and may even not exist.
This problem may be solved using \gls{dfo} methods.

As in~\cite{Qian_Yu_2021}, \gls{dfo} be also be applied to reinforcement learning.
Instead of training a model on a fixed labelled dataset, reinforcement learning bases the training process on rewarding expected behaviors and punishing undesired ones.
Hence, it often consists in finding optimal parameters that maximize a reward.
However, these reward's derivatives often cannot be evaluated, and \gls{dfo} methods can be an approach to solving such problems.
This concept is often referred to as derivative-free reinforcement learning.


\subsection{Some industrial and engineering applications}

\Gls{dfo} methods are also widely used in industry and engineering, especially for solving problems that involve heavy simulations.
Such problems arise from helicopter rotor blade manufacturing~\cite{Booker_Etal_1998a,Booker_Etal_1998b,Serafini_1998}, aeroacoustic shape design~\cite{Marsden_2004,Marsden_Etal_2004}, computational fluid dynamics~\cite{Duvigneau_Visonneau_2004}, worst-case analysis of analog circuit~\cite{Latorre_Etal_2019}, rapid-cycling synchrotron accelerator modeling~\cite{Eldred_Etal_2021}, nuclear energy engineering~\cite{Kortelainen_Etal_2010,Kortelainen_Etal_2012,Kortelainen_Etal_2014}, reservoir engineering and engine calibration~\cite{Langouet_2011}, and groundwater supply and bioremediation engineering~\cite{Fowler_Etal_2008,Mugunthan_Shoemaker_Regis_2005,Yoon_Shoemaker_1999}, to name but a few.
In general, industrial and engineering problems that involve sophisticated models, simulations, or experiments, give rise to \gls{dfo} problems.

A particular application of \gls{dfo} comes from \gls{mdo} in industry.
It is an field that uses optimization methods to solve design problems defined by multiple disciplines.
The objective and constraint functions of a \gls{mdo} problem can be provided by different departments of the same company or even by different companies.
This is the case in aircraft engine engineering~\cite{Gazaix_Etal_2019}, where the design problem of one component is solved while taking into account constraints imposed by other components handled by different departments.
\Gls{mdo} problems often involve simulations or experiments and therefore, \gls{dfo} methods are often needed.
We will present in \cref{ch:pdfo} of this thesis a piece of software we implemented for solving \gls{dfo} problems based on methods by Powell~\cite{Powell_1994,Powell_2002,Powell_2006,Powell_2009,Powell_2015}.
It has been included in GEMSEO~\cite{Gallard_Etal_2018}, an engine for \gls{mdo} initiated by a team from IRT Saint Exup{\'{e}}ry\footnote{\url{https://www.irt-saintexupery.com}} in France.

\section{Optimality conditions for smooth optimization}

We discuss in this section optimality conditions for problem~\cref{eq:problem-introduction}.
We do not assume any structure on the objective and constraint functions, except some smoothness.
More specialized results can be obtained by assuming that problem~\cref{eq:problem-introduction} is convex for example, but it is out of the scope of this work.

\subsection{Local and global solutions}

Before solving problem~\cref{eq:problem-introduction}, we must define what a solution is.
\Cref{def:global-solution} presents the most natural understanding of a solution.

\begin{definition}[Global solution]
    \label{def:global-solution}
    To problem~\cref{eq:problem-introduction}, a point~$x^{\ast} \in \R^n$ is a \emph{global solution} if~$x^{\ast} \in \Omega$ and~$\obj(x) \ge \obj(x^{\ast})$ for all~$x \in \Omega$.
\end{definition}

The following relaxed concept of solutions to problem~\cref{eq:problem-introduction} is of interest both in theory and in practice.

\begin{definition}[Local solution]
    % \label{def:local-solution}
    To problem~\cref{eq:problem-introduction}, a point~$x^{\ast} \in \R^n$ is
    \begin{itemize}
        \item a \emph{local solution} if~$x^{\ast} \in \Omega$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$\obj(x) \ge \obj(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega$,
        \item a \emph{strict local solution} if~$x^{\ast} \in \Omega$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$\obj(x) > \obj(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega \setminus \set{x^{\ast}}$, and
        \item an \emph{isolated local solution} if there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that it is the only local solution in~$\mathcal{N} \cap \Omega$.
    \end{itemize}
\end{definition}

It is known that finding a local solution to a general nonconvex problem is NP-hard.
In many cases, it is already NP-hard to check whether a given point is a local solution.
There also exist convex optimization problems that are NP-hard, such as copositive programming, for which checking the feasibility of a given point is indeed NP-hard.
See~\cite{Murty_Kabadi_1987} for more discussions.

The methods we consider in this thesis are local.
They attempt to approximately find local solutions to problem~\cref{eq:problem-introduction}.
However, in general, theoretical analyses of these methods can only guarantee approximations of stationary points, which will be introduced hereinafter.

\subsection{Constraint qualifications}

Before introducing any necessary and sufficient conditions for local optimality, we discuss some regularity conditions on the constraints~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq}, referred to as \emph{constraint qualifications}.
They will be required for the necessary conditions to hold.
We first introduce the notion of \emph{active set}.

\begin{definition}[Active set]
    The \emph{active set}~$\mathcal{A}(x) \subseteq \iub \cup \ieq$ for problem~\cref{eq:problem-introduction} at a point~$x \in \R^n$ is defined as
    \begin{equation*}
        \mathcal{A}(x) \eqdef \ieq \cup \set{i \in \iub : \con{i}(x) \ge 0}.
    \end{equation*}
\end{definition}

If a constraint belongs to the active set\footnote{For simplicity, we do not distinguish a constraint from its index.} at a given point, it is said to be \emph{active} at this point, and \emph{inactive} otherwise.
Note that a violated constraint is always considered active.

We introduce hereinafter two classical constraint qualifications.

\begin{definition}[Constraint qualifications]
    Given~$x \in \Omega$, denote~$\mathcal{A}(x)$ the active set for problem~\cref{eq:problem-introduction} at~$x$, and assume that the constraint functions~$\con{i}$ are differentiable at~$x$ for all~$i \in \mathcal{A}(x)$.
    We say that
    \begin{itemize}
        \item the \gls{licq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \mathcal{A}(x)$, and
        \item the \gls{mfcq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \ieq$ and there exists a vector~$z \in \R^n$ such that
        \begin{subequations}
            \label{eq:mangasarian-fromovitz}
            \begin{empheq}[left=\empheqlbrace]{alignat=2}
                & \inner{\nabla \con{i}(x), z} < 0  && \quad \text{if~$i \in \mathcal{A}(x) \cap \iub$, and}\\
                & \inner{\nabla \con{i}(x), z} = 0  && \quad \text{if~$i \in \ieq$}.
            \end{empheq}
        \end{subequations}
    \end{itemize}
\end{definition}

The \gls{licq} is stronger than the \gls{mfcq}.
If the \gls{licq} holds at~$x \in \Omega$, then the system~\cref{eq:mangasarian-fromovitz} is consistent because of the linear independance of all~$\nabla \con{i}(x)$ for~$i \in \mathcal{A}(x)$.

Many other constraint qualifications exist.
Examples include the \gls{acq} and the \gls{gcq}, which are formulated using tangent and linearized cones of the feasible set.
There also exist several traditional constraint qualifications weaker than the \gls{mfcq}, such as the \gls{crcq}, the \gls{cpld}, or the \gls{qncq}.
We also note that when the problem has a particular structure, such as convexity, dedicated constraint qualifications may exist, such as the \gls{sc}.
In this thesis, we will focus on the \gls{licq} and the \gls{mfcq}.

\subsection{First-order optimality conditions}

\subsubsection{Statement of the optimality conditions}

Let~$\lag$ be the \emph{Lagrangian function} to problem~\cref{eq:problem-introduction}, defined as
\begin{equation*}
    \lag(x, \lambda) \eqdef \obj(x) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lambda_i \con{i}(x),
\end{equation*}
where~$\lambda = (\lambda_i)_{i \in \iub \cup \ieq}$ with~$\lambda_i \in \R$ for all~$i \in \iub \cup \ieq$.
\Cref{thm:first-order-necessary-conditions} introduces the first-order necessary conditions for a point to be a local solution of problem~\cref{eq:problem-introduction}.

\begin{theorem}[First-order necessary conditions~\cite{Nocedal_Wright_2006}] % Theorem 12.1
    \label{thm:first-order-necessary-conditions}
    Let~$x^{\ast} \in \Omega$ be a local solution to problem~\cref{eq:problem-introduction}, assume that the functions~$\obj$ and~$\con{i}$ are continuously differentiable in a neighborhood of~$x^{\ast}$ for all~$i \in \iub \cup \ieq$, and that the \gls{licq} holds at~$x^{\ast}$.
    Then there exists a Lagrange multiplier~$\lambda^{\ast} = (\lambda_i^{\ast})_{i \in \iub \cup \ieq}$ with~$\lambda_i^{\ast} \in \R$ for all~$i \in \iub \cup \ieq$ such that
    \begin{subequations}
        \label{eq:kkt-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=2}
            & \nabla_x \lag(x^{\ast}, \lambda^{\ast}) = 0,  && \label{eq:kkt-introduction-stationarity}\\
            & \con{i}(x^{\ast}) \le 0,                      && \quad \text{if~$i \in \iub$,} \label{eq:kkt-introduction-primal-feasibility-ub}\\
            & \con{i}(x^{\ast}) = 0,                        && \quad \text{if~$i \in \ieq$,} \label{eq:kkt-introduction-primal-feasibility-eq}\\
            & \lambda_i^{\ast} \con{i}(x^{\ast}) = 0,       && \quad \text{if~$i \in \iub$,} \label{eq:kkt-introduction-complementary-slackness}\\
            & \lambda_i^{\ast} \ge 0,                       && \quad \text{if~$i \in \iub$.} \label{eq:kkt-introduction-dual-feasibility}
        \end{empheq}
    \end{subequations}
\end{theorem}

We present~\cref{thm:first-order-necessary-conditions} with the \gls{licq} as an example, but similar conclusion can be established with other constraint qualifications, such as the \gls{mfcq} (see, e.g.,~\cite[p.~339]{Nocedal_Wright_2006}).
The conditions~\cref{eq:kkt-introduction} are commonly referred to as the \gls{kkt} conditions~\cite{Karush_1939,Kuhn_Tucker_1951}.
More specifically, condition~\cref{eq:kkt-introduction-stationarity} is referred to as the \emph{stationarity} condition, conditions~\cref{eq:kkt-introduction-primal-feasibility-ub,eq:kkt-introduction-primal-feasibility-eq} as the \emph{primal feasibility} conditions, condition~\cref{eq:kkt-introduction-complementary-slackness} as the \emph{complementary slackness} condition, and condition~\cref{eq:kkt-introduction-dual-feasibility} as the \emph{dual feasibility} condition.
Any point~$x \in \R^n$ is referred to as a \emph{first-order stationary point} if it satisfies the \gls{kkt} conditions~\cref{eq:kkt-introduction}.
Such a point may not be a local solution.

\subsubsection{An illustration of the first-order optimality conditions}

We do not provide a proof of \cref{thm:first-order-necessary-conditions}, but we illustrate graphically the main idea on the simple~$2$-dimensional example
\begin{subequations}
    \label{eq:kkt-description}
    \begin{align}
        \min        & \quad \obj(x) = x_1 + x_2\\
        \text{s.t.} & \quad \con{1}(x) = x_1^2 + x_2^2 - 2 \le 0, \label{eq:kkt-description-c1}\\
                    & \quad \con{2}(x) = -x_2 \le 0, \label{eq:kkt-description-c2}\\
                    & \quad x \in \R^2, \nonumber
    \end{align}
\end{subequations}
whose solution is~$x^{\ast} = (-\sqrt{2}, 0)$.
A graphical representation of problem~\cref{eq:kkt-description} is given in \cref{fig:kkt-description}, where the white area represents the feasible set.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[%
            xmin=-5,%
            xmax=2,%
            ymin=-2,%
            ymax=2,%
            axis equal image,%
            xlabel={$x_1$},%
            ylabel={$x_2$},%
            axis background/.style={%
                pattern=north west lines,%
                pattern color=black!20,%
                even odd rule,%
                insert path={let \p1=(axis cs:0,0), \p2=(axis cs:2^0.5,0), \n1={veclen(\x2-\x1,\y2-\y1)}, in (\p2) arc(0:180:\n1) -- cycle},%
            },%
        ]
            \draw[dashed] (0,0) circle[radius=2^0.5];
            \draw[dashed] (-5,0) -- (2,0);
            \draw[-latex] (-2^0.5,0) -- (-3*2^0.5,0) node[above right] {$\nabla \con{1}(x^{\ast})$};
            \draw[-latex] (-2^0.5,0) -- (-2^0.5,-1) node[below left] {$\nabla \con{2}(x^{\ast})$};
            \draw[-latex] (-2^0.5,0) -- (1-2^0.5,1) node[below right] {$\nabla \obj(x^{\ast})$};
            \addplot[BrickRed,mark=*,only marks] coordinates {(-2^0.5,0)};
            \node[above left] at (-2^0.5,0) {$x^{\ast}$};
        \end{axis}
    \end{tikzpicture}
    \caption{Graphical representation of problem~\cref{eq:kkt-description}}
    \label{fig:kkt-description}
\end{figure}

As manifest in \cref{fig:kkt-description}, there does not exist any direction~$d \in \R^2$ satisfying
\begin{subequations}
    \label{eq:kkt-proof}
    \begin{empheq}[left=\empheqlbrace]{alignat=1}
        & \inner{\nabla \obj(x^{\ast}), d} < 0,\\
        & \inner{\nabla \con{1}(x^{\ast}), d} \le 0,\\
        & \inner{\nabla \con{2}(x^{\ast}), d} \le 0.
    \end{empheq}
\end{subequations}
The Farkas' lemma~\cite{Farkas_1902} ensures, therefore, that there exists a nonnegative Lagrange multiplier~$\lambda^{\ast} = (\lambda_1^{\ast}, \lambda_2^{\ast})$ such that
\begin{equation*}
    \nabla \obj(x^{\ast}) + \lambda_1^{\ast} \nabla \con{1}(x^{\ast}) + \lambda_2^{\ast} \nabla \con{2}(x^{\ast}) = 0.
\end{equation*}
This validates condition~\cref{eq:kkt-introduction-stationarity}, while~\cref{eq:kkt-introduction-primal-feasibility-ub,eq:kkt-introduction-primal-feasibility-eq,eq:kkt-introduction-complementary-slackness,eq:kkt-introduction-dual-feasibility} are obvious.
A solution to system~\cref{eq:kkt-proof} is both a descent direction for~$\obj$ and a linearized feasible direction for the constraints.
In the context of~\cref{thm:first-order-necessary-conditions}, the nonexistence of such a direction is guaranteed by the \gls{licq}.
See~\cite[\S~12.4]{Nocedal_Wright_2006} for a complete proof of \cref{thm:first-order-necessary-conditions}.

\subsection{Second-order optimality conditions}

It is known that at a local solution of a smooth unconstrained optimization problem, the gradient of the objective function is zero and its Hessian matrix is positive semidefinite.
\Cref{thm:second-order-necessary-conditions} generalizes this fact to the optimization problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order necessary conditions~\cite{Nocedal_Wright_2006}] % Theorem 12.5
    \label{thm:second-order-necessary-conditions}
    Let~$x^{\ast} \in \Omega$ be a local solution to problem~\cref{eq:problem-introduction}.
    Assume that the functions~$\obj$ and~$\con{i}$ are twice continuously differentiable in a neighborhood or~$x^{\ast}$ for all~$i \in \iub \cup \ieq$, and that \gls{licq} holds at~$x^{\ast}$.
    Denote the active set for problem~\cref{eq:problem-introduction} at~$x^{\ast}$ by~$\mathcal{A}(x^{\ast})$.
    Let~$\lambda^{\ast} = (\lambda_i^{\ast})_{i \in \iub \cup \ieq}$ with~$\lambda_i^{\ast} \in \R$ for all~$i \in \iub \cup \ieq$ be a Lagrange multiplier satisfying the KKT condition~\cref{eq:kkt-introduction}, and let~$z \in \R^n$ be any vector such that
    \begin{subequations}
        \label{eq:second-order-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=2}
            & \inner{\nabla \con{i}(x^{\ast}), z} = 0,      && \quad \text{if~$i \in \ieq$,}\\
            & \inner{\nabla \con{i}(x^{\ast}), z} = 0,      && \quad \text{if~$i \in \mathcal{A}(x^{\ast}) \cap \iub$ and~$\lambda_i^{\ast} > 0$, and}\\
            & \inner{\nabla \con{i}(x^{\ast}), z} \ge 0,    && \quad \text{if~$i \in \mathcal{A}(x^{\ast}) \cap \iub$ and~$\lambda_i^{\ast} = 0$.}
        \end{empheq}
    \end{subequations}
    Then~$\inner{z, \nabla_{x, x}^2 \lag(x^{\ast}, \lambda^{\ast}) z} \ge 0$. 
\end{theorem}

A first-order stationary point~$x \in \Omega$ is called a \emph{second-order stationary point} if it satisfies the conclusion of \cref{thm:second-order-necessary-conditions}.
We emphasize that a second-order stationary point may not be a local solution.
However, it is known in smooth unconstrained optimization that a point at which the gradient of the objective function is zero and its Hessian matrix is positive definite is a strict local solution.
\Cref{thm:second-order-sufficient-conditions} generalizes this fact to the optimization problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order sufficient conditions~\cite{Nocedal_Wright_2006}] % Theorem 12.6
    \label{thm:second-order-sufficient-conditions}
    Let~$x^{\ast} \in \Omega$ be a given point.
    Assume that the functions~$\obj$ and~$\con{i}$ are twice continuously differentiable in a neighborhood of~$x^{\ast}$ for all~$i \in \iub \cup \ieq$, and that there exists a Lagrange multiplier~$\lambda^{\ast} = (\lambda_i^{\ast})_{i \in \iub \cup \ieq}$ with~$\lambda_i^{\ast} \in \R$ for all~$i \in \iub \cup \ieq$ satisfying the KKT condition~\cref{eq:kkt-introduction}.
    If for all nonzero vector~$z \in \R^n \setminus \set{0}$ satisfying the conditions~\cref{eq:second-order-introduction} we have~$\inner{z, \nabla_{x, x}^2 \lag(x^{\ast}, \lambda^{\ast}) z} > 0$, then~$x^{\ast}$ is a strict local solution to problem~\cref{eq:problem-introduction}.
\end{theorem}

\section{Methodology of \glsfmtlong{dfo}}

As summarized in~\cite{Conn_Scheinberg_Vicente_2009b}, two main strategies have been developed for solving \gls{dfo} problems.
One strategy consists of sampling the objective function around the current iterate and choosing the next iterate among the sampled points based on simple comparisons.
The \emph{direct-search} methods~\cite{Kolda_Lewis_Torczon_2003} are based on this framework.
The other strategy builds iteratively models that approximate the problems (e.g., using polynomials) around the current iterate and choose the next iterate according to the approximated problems.
These methods are referred to as \emph{model-based} methods.
Hybrid methods also exist, such as implicit filtering~\cite{Kelley_2011}, and \gls{sidpsm}~\cite{Custodio_Rocha_Vicente_2009}, which combine direct search and models.

Some methods can solve \gls{dfo} problems but are not covered by the above categories.
Examples include random search methods~\cite{Zhigljavsky_1991}, simulated annealing methods~\cite{Kirkpatrick_Gelatt_Vecchi_1983}, the genetic algorithm~\cite{Jong_1975,Holland_1975}, and Bayesian optimization methods~\cite{Mockus_1975,Shahriari_Etal_2016}.
For more information on these methods, we refer to the review~\cite{Larson_Menickelly_Wild_2019} and the reference therein.

\subsection{Direct-search methods}

An early example of \gls{dfo} is a method from \citeauthor{Fermi_Metropolis_1952}~\cite{Fermi_Metropolis_1952}, who developed in \citeyear{Fermi_Metropolis_1952} a nonlinear least-squares solver on MANIAC, an computer based on the von Neumann architecture.
From a modern viewpoint, this method is a coordinate search method, a particular example of direct-search methods, where the search directions are defined to be the coordinate axes.

Several direct-search methods appeared thereafter.
\citeauthor{Rosenbrock_1960} designed an direct-search method for unconstrained problems.
It constructs an orthogonal basis using previous steps and searching along the directions in this basis~\cite{Rosenbrock_1960}.
Another famous early direct-search method is the Hooke-Jeeves method~\cite{Hooke_Jeeves_1961}.
This method combines exploratory moves toward coordinates axes with pattern moves, aiming to reuse the previously computed directions.
In \citeyear{Nelder_Mead_1965}, \citeauthor{Nelder_Mead_1965} introduced a method for solving unconstrained optimization problems, which evaluates the objective function at the vertices of a given simplex, and modifies this simplex, using reflection, expansion, contraction, and shink steps, one vertex at a time~\cite{Nelder_Mead_1965}.
Among the various direct-search solvers, this method is interesting as it is still used today, underlying the MATLAB function \verb|fminsearch|.
Many other works on direct search appeared in these years.
These comprise, for example, the works of \citeauthor{Powell_1964}~\cite{Powell_1964,Powell_1975a}, \citeauthor{Matyas_1965}~\cite{Matyas_1965}, \citeauthor{Fletcher_1965}~\cite{Fletcher_1965}, and \citeauthor{Box_1966}~\cite{Box_1966}, for instance.
At the end of the millennium, with the notable works of \citeauthor{Wright_1995}~\cite{Wright_1995} and others, \gls{dfo} became the broad research area that exists today, encouraged by recent engineering needs.
Nowadays, the direct-search paradigm offers many other algorithms, such as the \gls{gps} methods~\cite{Booker_Etal_1999}, later extended to the \gls{mads} methods~\cite{Abramson_Audet_2006,Abramson_Etal_2009,Audet_Dennis_2006,Audet_Dennis_Digabel_2008,Digabel_2011}, the implicit filtering method~\cite{Kelley_2011}, and \gls{bfo}~\cite{Porcelli_Toint_2017}.
A more recent work by \citeauthor{Porcelli_Toint_2022} attempts to incorporate partial separability knowledge in a direct-search framework~\cite{Porcelli_Toint_2022}.

\subsection{Model-based methods}

Unlike direct-search methods, model-based methods approximate locally the functions involved in the optimization problems.
To globalize their convergence properties, the approximated problems are usually embedded in a globalization strategy, such as a \emph{line-search} method~\cite[ch.~3]{Nocedal_Wright_2006} or a \emph{trust-region} methods~\cite{Conn_Gould_Toint_2000,Yuan_2015}.
Most of the existing model-based methods use linear or quadratic approximations, although attempts have been made to use other functions, such as \gls{rbf}~\cite{Oeuvray_2005}.

From an empirical viewpoint, the model-based methods are highly appealing, as they have excellent performances in real applications.
Examples of such algorithms include the method of \citeauthor{Conn_Toint_1996}~\cite{Conn_Toint_1996} for unconstrained optimization, which models the objective function with either sub-quadratic or quadratic functions obtained by underdetermined interpolations (see \cref{sec:underdetermined-interpolation}), for which the freedom bequeathed by the interpolation conditions is taken out by minimizing the~$\ell_2$-norm of the coefficients of the models.
Particular care is given to the geometry of the interpolation set to attempt to maintain models with reasonable accuracies.
Another example of such algorithms for unconstrained optimization is \gls{mnh}~\cite{Wild_2008}, whose models are also quadratic functions obtained by underdetermined interpolation.
However, these models do not minimize the~$\ell_2$-norm of the coefficients but rather the Frobenius norm of their Hessian matrices.
The Wedge method~\cite{Marazzi_Nocedal_2002} of \citeauthor{Marazzi_Nocedal_2002} is another instance of \gls{dfo} trust-region solvers.
It has, however, the unique feature that it does not include any geometry-improvement steps, as most solvers do.
Instead, it adds a new constraint to the trust-region subproblems to prevent the trial steps from lying in a region that is likely to worsen the geometry of the interpolation set.
Other examples of trust-region methods are \gls{csv2}~\cite{Billups_Larson_Graf_2013}, which determines its models by regression and not interpolation, \gls{dfols}~\cite{Cartis_Etal_2019}, which aims at solving nonlinear least-squares problems, or the Powell's \gls{dfo} solvers, presented later in this document.
We thus heed that some solvers use nonpolynomial models, such as \gls{orbit}~\cite{Wild_Regis_Shoemaker_2008}, \gls{conorbit}~\cite{Regis_Wild_2017}, and \gls{boosters}~\cite{Oeuvray_Bierlaire_2009}, which use cubic \gls{rbf}.

\subsection{Comment on finite-difference approximations}
\label{subsec:finite-difference}

Perhaps to simplest approach for solving \gls{dfo} problems is to use derivative-based methods with finite-difference approximations of the derivatives (see, e.g.,~\cite{Shi_Etal_2021}).
For example, the~$i$th coordinate of the gradient of a differentiable function~$\obj : \R^n \to \R$ at a point~$x \in \R^n$ can be approximated by the forward difference
\begin{equation*}
    \frac{\partial \obj}{\partial x_i}(x) \approx \frac{\obj(x + h e_i) - \obj(x)}{h},
\end{equation*}
or the central difference
\begin{equation*}
    \frac{\partial \obj}{\partial x_i}(x) \approx \frac{\obj(x + h e_i) - \obj(x - h e_i)}{2h},
\end{equation*}
where~$h > 0$ is a given difference parameter and~$e_i \in \R^n$ is the~$i$th standard coordinate vector of~$\R^n$.
It is known that such methods are competitive with \gls{dfo} methods when the problems considered are smooth and noiseless.
However, in the presence of noise, the performance of finite-difference based methods usually drop.
Moreover, the choice of the difference parameter~$h$ depends on the noise level, which is usually unknown.
It is clear that estimating~$\nabla f(x)$ necessitates~$\mathcal{O}(n)$ function evaluations around~$x$ along the coordinate axes.
Since each function evaluation is expensive, one should attempt to reuse as much as possible function evaluations.
However, it is unlikely that these evaluations are reused by the gradient-based method without any modification.
Thus, we do not consider finite-difference based method in this thesis.

\section{Benchmarking tools for \glsfmtlong{dfo} methods}

We introduce in this section the benchmarking tools that we will use throughout this thesis to compare \gls{dfo} solvers.
We denote by~$\mathcal{S}$ the set of solvers to benchmark and~$\mathcal{P}$ a set of problems to be solved by all solvers in~$\mathcal{S}$, assumed to represent the problems for which the solvers have been designed.
Let~$t_{p, s}$ be the performance measure for the solver~$s \in \mathcal{S}$ to achieve a given convergence test on the problem~$p \in \mathcal{P}$.
The performance measure of a \gls{dfo} solver we consider in this thesis is the number of function evaluations.
Therefore,~$t_{p, s}$ usually denotes the number of function evaluations required by the solver~$s \in \mathcal{S}$ to solve the problem~$p \in \mathcal{P}$ for the given convergence test.

To compare the performances of the solvers in~$\mathcal{S}$, we use the concept of performance and data profiles~\cite{Dolan_More_2002,More_Wild_2009} presented hereafter.
The numerical experiments presented in this thesis define~$\mathcal{P}$ as subsets of the CUTEst dataset~\cite{Gould_Orban_Toint_2015}.
Traditional convergence tests necessitate derivative information, which is assumed unavailable in this work.
We consider the following convergence test for \gls{dfo} solvers, proposed by \citeauthor{More_Wild_2009}~\cite{More_Wild_2009}.
We assume that each problem~$p \in \mathcal{P}$ has a merit function~$\varphi_p$, i.e., a function that properly balances the possible infeasibility of a point with the corresponding value of the objective function.
Let~$x_p^0 \in \R^{n_p}$ be the initial guess of a given problem~$p \in \mathcal{P}$ and~$x_p^{\ast} \in \R^{n_p}$ be the point that achieves the least value of~$\varphi_p$ among all iterates of all solvers in~$\mathcal{S}$, where~$n_p$ denotes the dimension of the problem~$p$.
Given a tolerance~$\tau \in (0, 1)$, a point~$x \in \R^{n_p}$ satisfies the convergence test if
\begin{equation*}
    \varphi_p(x) \le \varphi_p(x_p^{\ast}) + \tau [\varphi_p(x_p^0) - \varphi_p(x_p^{\ast})].
\end{equation*}
This convergence test can be interpreted as follows.
A point~$x \in \R^n$ satisfies the convergence test if the reduction~$\varphi_p(x_{\hat{p}}^0) - \varphi_p(x)$ is at least~$1 - \tau$ times the best possible reduction~$\varphi_p(x_p^0) - \varphi_p(x_p^{\ast})$.
Therefore, we assume that at least one of the solvers converged for each problem, since the actual solutions of the problems may be unknown.

\subsection{An experiment example}
\label{subsec:experiment-example}

Throughout this section, we consider the following experiment as an example.
We evaluate the performance profiles of three solvers, namely
\begin{itemize}
    \item \gls{newuoa} (see \cref{subsec:newuoa-bobyqa-lincoa}), a model-based \gls{dfo} method; and
    \item \gls{bfgs} and \gls{cg}, two gradient-based solvers with forward finite-difference approximations, with the difference parameter~$h = \sqrt{u}$ where~$u$ is the unit roundoff.
\end{itemize}
The set~$\mathcal{P}$ is defined to be the smooth unconstrained CUTEst problems of dimension at most~$50$, and the convergence tolerance is~$\tau = 10^{-5}$.
The first experiment is made without modifying the problems in~$\mathcal{P}$, and~$t_{p, s}$ is defined to be the number of function evaluations that the solver~$s \in \mathcal{S}$ necessitated to solve the problem~$p \in \mathcal{P}$.
The second experiment we consider is a noised variation of the previous one.
The objective functions of the problems in~$\mathcal{P}$ are perturbed with a relative Gaussian noise~$\mathcal{N}(0, 10^{-20})$.
Each problem is solved ten times with each solver, and~$t_{p, s}$ is set to the average number of function evaluations that the solver~$s \in \mathcal{S}$ necessitated to solve the problem~$p \in \mathcal{P}$.

\subsection{Performance profiles}

For any fixed solver~$\hat{s} \in \mathcal{S}$ and problem~$\hat{p} \in \mathcal{P}$, define their \emph{performance ratio} by
\begin{equation*}
    r_{\hat{p}, \hat{s}} \eqdef \frac{t_{\hat{p}, \hat{s}}}{\min \set{t_{\hat{p}, s} : s \in \mathcal{S}}}.
\end{equation*}
When the solver~$\hat{s}$ fails to solve the problem~$\hat{p}$, we use the convention~$r_{\hat{p}, \hat{s}} = \infty$.
Given any fixed solver~$\hat{s} \in \mathcal{S}$ and a threshold~$\alpha \ge 1$, the \emph{performance profile} of~$\hat{s}$ is defined as the proportion of problems in~$\mathcal{P}$ that are solved with a performance ratio at most~$\alpha$, that is
\begin{equation*}
    \rho_{\hat{s}}(\alpha) \eqdef \frac{1}{\card \mathcal{P}} \card \set{p \in \mathcal{P} : r_{p, \hat{s}} \le \alpha},
\end{equation*}
where~$\card$ denotes the cardinal operator.
Therefore, $\rho_{\hat{s}}(1)$ is the proportion of problems in~$\mathcal{P}$ that are solved faster by the solver~$\hat{s}$ than by any other solvers in~$\mathcal{S}$, and $\rho_{\hat{s}}(\alpha)$ is the proportion of problems in~$\mathcal{P}$ that are solved by the solver~$\hat{s}$ without any budget restriction for any~$\alpha \ge \max \set{r_{p, \hat{s}} : p \in \mathcal{P}}$.

A graphical representation of the performance profiles related to the experiment described in~\cref{subsec:experiment-example} is given in \cref{fig:performance-profile-example}.
The abscissas are the performance ratios in base-$2$ logarithmic scale and the performance profiles are on the ordinates.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.46\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{plain-1-50-perf-bfgs-cg-newuoa-u.csv}{5}
        \caption{Smooth problems}
        \label{fig:performance-profile-example-smooth}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.46\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-10-perf-bfgs-cg-newuoa-u.csv}{5}
        \caption{Noised problems}
        \label{fig:performance-profile-example-noised}
    \end{subfigure}
    \caption{Example of performance profiles on unconstrained problems}
    \label{fig:performance-profile-example}
\end{figure}

On this example, we could argue that \gls{bfgs} provides better performance than the two other solvers on smooth problems, but \gls{newuoa} clearly overcomes the two other solvers on noised problems.
This behavior is perfectly expected, and does not contradict the observations of \citeauthor{Shi_Etal_2021}~\cite{Shi_Etal_2021}, as we did not modify the difference parameters of the two finite-difference based solvers.
We rather kept the default value provided for the solvers \gls{bfgs} and \gls{cg} by SciPy~\cite{Virtanen_Etal_2020}.

However, we cannot compare \gls{newuoa} and \gls{cg} on~\cref{fig:performance-profile-example-smooth} nor \gls{bfgs} and \gls{cg} on~\cref{fig:performance-profile-example-noised}.
In fact, performance profiles provide a clear measure when comparing two solvers, but they may fail comparing a solver relatively to another one that is not the best when comparing more than two solvers~\cite{Gould_Scott_2016}.

\subsection{Data profiles}

We can also use data profiles~\cite{More_Wild_2009} to compare different solvers.
Unlike performance profiles, data profiles provide an absolute comparison.
For any fixed solver~$\hat{s} \in \mathcal{S}$ and threshold~$\alpha \ge 0$, the \emph{data profile} of~$\hat{s}$ is defined as
\begin{equation*}
    d_{\hat{s}}(\alpha) \eqdef \frac{1}{\card \mathcal{P}} \card \set[\bigg]{p \in \mathcal{P} : \frac{t_{p, \hat{s}}}{n_p + 1} \le \alpha}.
\end{equation*}
Therefore,~$d_{\hat{s}}(0) = 0$ and~$d_{\hat{s}}(\alpha) = \rho_{\hat{s}}(\alpha)$ for any~$\alpha$ such that
\begin{equation*}
    \alpha \ge \max \set[\bigg]{\max \set[\bigg]{r_{p, \hat{s}}, \frac{t_{p, \hat{s}}}{n_p + 1}} : p \in \mathcal{P}}.
\end{equation*}
The denominator~$n_p + 1$ serves to homogenize the problems and defines the data profiles in terms of simplex gradient evaluations (see \cref{sec:multivariate-interpolation}).

A graphical representation of the data profiles related to the experiment described in~\cref{subsec:experiment-example} is given in~\cref{fig:data-profile-example}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.46\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{plain-1-50-data-bfgs-cg-newuoa-u.csv}{5}
        \caption{Smooth problems}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.46\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-10-data-bfgs-cg-newuoa-u.csv}{5}
        \caption{Noised problems}
    \end{subfigure}
    \caption{Example of data profiles on unconstrained problems}
    \label{fig:data-profile-example}
\end{figure}

On this example, we could observe the same results than previously, i.e., \gls{bfgs} overcomes the two other solvers on smooth problems, but \gls{newuoa} provides better performance than the two other solvers on noised problems.
However, based on \cref{fig:data-profile-example}, we could also argue that \gls{newuoa} overcomes \gls{cg} on smooth problems and that \gls{bfgs} and \gls{cg} provide approximately the same performance on noised problems.
This is because the comparisons made on data profiles are absolute, and the result of a solver does not influence the result of another, except for the convergence test.
Throughout this thesis, we will use both performance and data profiles to compare the performances of different \gls{dfo} solvers.
