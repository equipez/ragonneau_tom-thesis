%% contents/introduction.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Introduction}

\nomenclature[Fa]{$\obj$}{Real-valued objective function defined on~$\R^n$}%
\nomenclature[Fb]{$\con{i}$}{Real-valued constraint function defined on~$\R^n$, with~$i \in \iub \cup \ieq$}%
\nomenclature[Fc]{$\lag$}{Lagrangian function}%
\nomenclature[Na]{$\in$}{Set membership notation}%
\nomenclature[Nb]{$\subseteq$}{Set inclusion notation}%
\nomenclature[Nc]{$\qedsymbol$}{Halmos symbol}%
\nomenclature[Nd]{$A^{\T}$,~$v^{\T}$}{Transpose of a matrix or a vector}%
\nomenclature[Ne]{$I_n$}{Identity matrix on~$\R^{n \times n}$}%
\nomenclature[Nf]{$e_k$}{Standard coordinate vector of~$\R^n$ ($k$-th column of~$I_n$), with~$1 \le k \le n$}%
\nomenclature[Ng]{$\mathcal{O}(\cdot)$}{Big-O notation}%
\nomenclature[Nh]{$o(\cdot)$}{Little-O notation}%
\nomenclature[Oa]{$[\cdot]_{+}$}{Elementwise positive-part operator}%
\nomenclature[Ob]{$[\cdot]_{-}$}{Elementwise negative-part operator}%
\nomenclature[Oc]{$\abs{\cdot}$}{Elementwise modulus operator}%
\nomenclature[Od]{$\inner{\cdot, \cdot}$}{Inner-product operator (may be subscripted for sake of clarity)}%
\nomenclature[Oe]{$\norm{\cdot}$}{Norm of a vector or a matrix (may be subscripted for sake of clarity)}%
\nomenclature[Of]{$\nabla$}{Gradient operator (elements~$\partial / \partial x_i$, with~$i \in \set{1, 2, \dots, n}$)}%
\nomenclature[Og]{$\nabla^2$}{Hessian operator (elements~$\partial^2 / \partial x_i \partial x_j$, with~$i, j \in \set{1, 2, \dots, n}$)}%
\nomenclature[Oh]{$\land$}{Logic and operator}%
\nomenclature[Sa]{$\emptyset$}{Empty set}%
\nomenclature[Sb]{$[a, b]$}{Closed set~$\set{x \in \R : a \le x \le b}$ with~$a \le b$}%
\nomenclature[Sc]{$(a, b)$}{Open set~$\set{x \in \R : a < x < b}$ with~$a < b$}%
\nomenclature[Sd]{$[a, b)$}{Semi-open set~$\set{x \in \R : a \le x < b}$ with~$a < b$}%
\nomenclature[Se]{$(a, b]$}{Semi-open set~$\set{x \in \R : a < x \le b}$ with~$a < b$}%
\nomenclature[Sf]{$\R$}{Set of real numbers}%
\nomenclature[Sg]{$\R^n$}{Real coordinate space of dimension~$n$}%
\nomenclature[Sh]{$\R^{m \times n}$}{Real matrix space of dimension~$m \times n$}%
\nomenclature[Si]{$\Omega$}{Feasible set, included in~$\R^n$}%
\nomenclature[Sj]{$\ieq$}{Set of indices of the equality constraints}%
\nomenclature[Sk]{$\iub$}{Set of indices of the inequality constraints}%
\todo[noline]{Replace the nomenclature items.}

\section{Overview of \glsfmtlong{dfo}}

Optimization is the study of extremal points and values of mathematical functions.
Traditional optimization aims at minimizing (or maximizing) a real-valued function~$\obj$, referred to as the \emph{objective function}, within a given nonempty set of points~$\Omega \subseteq \R^n$, referred to as the \emph{feasible set}.
It is well known that the most helpful information to optimization is embraced in the derivatives of~$\obj$ and the curves of~$\Omega$.
However, such derivative information may not exist, be unassessable, unreliable, or prohibitively expensive to evaluate.
It is within this context, referred to as \gls{dfo}~\cite{Audet_Hare_2017,Conn_Scheinberg_Vicente_2009b}, that our research focuses.
Problems for which derivative information is unavailable arise naturally when the objective function or the feasible set results from complex experiments or simulations.
We emphasize that we are \emph{not} studying nonsmooth optimization.
Classical or generalized derivatives of the optimization problem's functions may be well-defined, but we assume they cannot be numerically assessed.

The leading complexity measure we consider is the number of objective function evaluations.
We presume that an objective function evaluation dawdles and requires several minutes or even several days to complete.
For instance, a modern application of \gls{dfo} is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}, for which one objective function evaluation necessitates training a machine learning model (see~\cref{subsec:machine-learning}).
Hence, the algebraic complexity of the methods we consider is not our leading issue, although we will strive to maintain it acceptable.

Within this chapter, we consider the nonlinearly-constrained problem
\begin{subequations}
    \label{eq:nlcp-intro}
    \begin{align}
        \min        & \quad \obj(x)\\
        \text{s.t.} & \quad \con{i}(x) \le 0, ~ i \in \iub, \label{eq:nlcp-intro-cub}\\
                    & \quad \con{i}(x) = 0, ~ i \in \ieq, \label{eq:nlcp-intro-ceq}\\
                    & \quad x \in \R^n, \nonumber
    \end{align}
\end{subequations}
where the \emph{objective} and \emph{constraint functions}~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are real-valued functions on~$\R^n$, and where the sets of indices~$\iub$ and~$\ieq$ are finite (perhaps empty).
The above-mentioned feasible set~$\Omega \subseteq \R^n$ is, therefore, the set of points satisfying the inequality constraints~\cref{eq:nlcp-intro-cub} and the equality constraints~\cref{eq:nlcp-intro-ceq}, that is
\begin{equation*}
    \Omega \eqdef \set{x \in \R^n : \con{i}(x) \le 0, ~ i \in \iub \land \con{i}(x) = 0, ~ i \in \ieq}.
\end{equation*}

\section{Examples of applications}

\subsection{Automatic error analysis}

A typical example of \gls{dfo} applications is automatic error analysis~\cite{Higham_1993,Higham_2002}, which formulates numerical computation's accuracies and stabilities using optimization problems.
Consider, for instance, the Gaussian elimination with partial pivoting of a matrix~$A \in \R^{n \times n}$, given in~\cref{alg:gepp}.

\begin{algorithm}[htp]
    \caption{Gaussian elimination with partial pivoting}
    \label{alg:gepp}
    \DontPrintSemicolon
    \KwData{Matrix~$A \in \R^{n \times n}$.}
    Initialize $A^{(0)} \gets A$\;
    \For{$k = 1, 2, \dots, n - 1$}{
        Determine the pivot index~$j = \argmax \set[\big]{\abs[\big]{A_{i, k}^{(k - 1)}} : k \le i \le n}$\;
        \eIf{$A_{j, k}^{(k - 1)} \neq 0$}{
            Exchange the~$k$th and the~$j$th rows of~$A^{(k - 1)}$\;
            Evaluate the multiplier~$\tau^{(k)} \in \R^n$ with components
            \begin{algomathdisplay}
                \tau_i^{(k)} =
                \begin{cases}
                    A_{i, k}^{(k - 1)} / A_{k, k}^{(k - 1)} & \text{if~$i > k$, and}\\
                    0                                       & \text{otherwise}
                \end{cases}
            \end{algomathdisplay}
            Update~$A^{(k)} \gets (I_n - \tau^{(k)} e_k^{\T})A^{(k - 1)}$\;
        }{
            Set~$A^{(k)} \gets A^{(k - 1)}$\;
        }
    }
\end{algorithm}

\Citeauthor{Wilkinson_1961}'s backward error analysis~\cite{Wilkinson_1961} demonstrates that the growth factor
\begin{equation}
    \label{eq:gepp-growth-factor}
    \rho_n(A) \eqdef \frac{\max_{0 \le k \le n - 1} \norm{A^{(k)}}_{\max}}{\norm{A}_{\max}},
\end{equation}
determinates the numerical stability of~\cref{alg:gepp}, where~$\norm{\cdot}_{\max}$ denotes the max norm of a matrix, i.e., the largest absolute value of the matrix's entries.
More specifically, the~$\ell_{\infty}$-norm of the backward error of the computed solution is bounded from above by a term proportional to~$\rho_n(A)$.
To study the worst-case scenario of~\cref{alg:gepp}, we wish to determine how large~$\rho_n$ can be and hence, to solve
\begin{equation}
    \label{eq:gepp-opti}
    \max_{A \in \R^{n \times n}} \rho_n(A).
\end{equation}
Note that~$\R^{n \times n}$ is isomorphic to~$\R^{n^2}$ and hence, problem~\cref{eq:gepp-opti} could straightforwardly be formulated as problem~\cref{eq:nlcp-intro}.
Besides, although the growth factor is defined everywhere, it may not be continuous at the points yielding a tie in the selection of the pivot element.
Moreover, it is not differentiable at the points yielding a tie in any maximum operator in equation~\cref{eq:gepp-growth-factor}.
Hence, optimization methods based on derivative information cannot be used for such a problem.
In such a case, \gls{dfo} methods can help determine the optimal solution to problem~\cref{eq:gepp-opti}, and \citeauthor{Higham_Higham_1989} have used \gls{dfo} methods to determine the set of matrices yielding the optimal solution~\cite{Higham_Higham_1989}.

\subsection{Tuning nonlinear optimization methods}

Another well-known example of \gls{dfo} applications is the parameter tuning of nonlinear optimization methods~\cite{Audet_Orban_2006}.
Consider, for example, the simplified version of the direct-search method for solving problem~\cref{eq:nlcp-intro} when~$\iub = \ieq = \emptyset$ given in~\cref{alg:direct-search}.

\begin{algorithm}[htp]
    \caption{Direct search for unconstrained optimization}
    \label{alg:direct-search}
    \DontPrintSemicolon
    \KwData{Objective function~$f$, positive spanning set~$\mathcal{D} \subset \R^n$, initial guess~$x^{(0)} \in \R^n$, steplengths~$0 < \alpha_{\min} < \alpha_0 < \alpha_{\max}$, and parameters~$0 < \theta < 1 \le \gamma$.}
    \For{$k = 0, 1, \dots$}{
        Evaluate~$d^{(k)} = \argmin \set{f(x^{(k)} + \alpha_k d) : d \in \mathcal{D}}$\;
        \eIf{$f(x^{(k)} + \alpha_k d^{(k)}) < f(x^{(k)})$}{
            Update~$x^{(k + 1)} \gets x^{(k)} + \alpha_k d^{(k)}$\;
            Increase the steplength~$\alpha_{k + 1} = \min \set{\gamma \alpha_k, \alpha_{\max}}$
        }{
            Update~$x^{(k + 1)} \gets x^{(k)}$\;
            Decrease the steplength~$\alpha_{k + 1} = \max \set{\theta \alpha_k, \alpha_{\min}}$
        }
        \If{$\alpha_k = \alpha_{\min}$}{
            Stop the computations\;
        }
    }
\end{algorithm}

Such an algorithm depends on two parameters, namely~$\theta$ and~$\gamma$.
To choose those parameters, we wish to minimize some measure of the method's performance (e.g., the sum of the number of function evaluations required to solve a given set of optimization problems),~$\rho$ say.
In other words, we wish to solve the optimization problem
\begin{subequations}
    \label{eq:tuning-opti}
    \begin{align}
        \min        & \quad \rho(\theta, \gamma) \label{eq:tuning-opti-obj}\\
        \text{s.t.} & \quad \theta \in (0, 1), \label{eq:tuning-opti-theta}\\
                    & \quad \gamma \ge 1. \label{eq:tuning-opti-gamma}
    \end{align}
\end{subequations}
Derivatives of~$\rho$ are unassessable and may not even exist.
Such a problem is then solved using \gls{dfo} methods.
In this example, since the considered optimization method is itself a \gls{dfo} method, we could even solve problem~\cref{eq:tuning-opti} using~\cref{alg:direct-search} given some estimates of the parameters~$\theta$ and~$\gamma$.

\subsection{Hyperparameter tuning in machine learning}
\label{subsec:machine-learning}

A newfangled example of \gls{dfo} applications is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}.
To illustrate this example, we consider the following hyperparameter tuning problem of a \gls{svm} for binary classification.
Given a labeled dataset~$\set{(x_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$, we build a \gls{svm} to classify the data with their respective labels.
A binary classification is obtained using a~$C$-\gls{svc}~\cite{Chang_Lin_2011} by solving the optimization problem
\begin{subequations}
    \label{eq:csvc}
    \begin{align}
        \min        & \quad \frac{1}{2} \norm{\omega}_2^2 + C \norm{\xi}_1\\
        \text{s.t.} & \quad y_i (\beta + \inner{\omega, \varphi_{\gamma}(x_i)}) \ge 1 - \xi_i, ~ i \in \set{1, 2, \dots, m},\\
                    & \quad \xi \ge 0,\\
                    & \quad (\omega, \beta, \xi) \in \R^{\ell} \times \R \times \R^m,
    \end{align}
\end{subequations}
where~$\varphi_{\gamma}$ is a function mapping the data to a higher-dimensional space~$\R^{\ell}$ for some given parameters~$\gamma > 0$ and~$C > 0$.
Given~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast}) \in \R^{\ell} \times \R \times \R^m$ a solution to problem~\cref{eq:csvc}, the~$C$-\gls{svc} classifies any data~$x \in \R^n$ according to
\begin{equation*}
    \delta(x) \eqdef \sgn(\beta^{\ast} + \inner{\omega^{\ast}, \varphi_{\gamma}(x)}).
\end{equation*}
In other words, the function~$\delta$ maps an observation~$x \in \R^n$ to a certain label.
It is clear that~$\delta$ depends on the two parameters~$C$ and~$\gamma$, and we want to find the optimal parameters for the given dataset.
Do to so, we use a~$5$-fold cross-validation of our model, as presented in~\cref{alg:cross-validation}.

\begin{algorithm}[htp]
    \caption{$k$-fold cross-validation of a~$C$-\glsfmtshort{svc}}
    \label{alg:cross-validation}
    \DontPrintSemicolon
    \KwData{Labelled dataset~$\set{(x_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$ and fold number~$k > 0$.}
    Split randomly the dataset into~$k$ balanced groups\;
    \For{$i = 1, 2, \dots, k$}{
        Train the~$C$-\gls{svc}~\cref{eq:csvc} with the all the data except those in the~$i$th group\;
        Evaluate the model's performance on the data in the~$i$th group\;
    }
    Summarize the model's performance using the~$k$ samples\;
\end{algorithm}

A typical example of model's performance used in the~$k$-fold cross-validation is the model's accuracy, i.e., the percentage of testing data correctly classified.
The~\gls{auc}~\cite{Hanley_Mcneil_1982} is another example of model's performance, particularly effective for imbalanced datasets~\cite{Bradley_1997}.
In this example, hyperparameter tuning maximizes the model's performance provided by the~$k$-fold cross-validation of the~$C$-\gls{svc}~\cref{eq:csvc} with respect to the parameters~$C$ and~$\gamma$, subject to~$C > 0$ and~$\gamma > 0$.
It is clear that derivatives of the objective function of such a problem cannot be easily evaluated and may even not exist.
Hyperparameter tuning problems may be solved numerically using \gls{dfo} methods.
For instance, Google uses Google Vizier~\cite{Golovin_Etal_2017} as their internal hyperparameter tuning engine.
In a similar fashion, reinforcement learning in machine learning necessitates solving optimization problems for which derivatives cannot be assessed~\cite{Qian_Yu_2021}.

\subsection{Some industrial and engineering applications}

Other industrial examples of \gls{dfo} applications include molecular conformational analysis~\cite{Alberto_Etal_2004,Meza_Martinez_1994}, helicopter rotor blade design~\cite{Booker_Etal_1998a,Booker_Etal_1998b,Serafini_1998}, groundwater supply and bioremediation engineering~\cite{Fowler_Etal_2008,Mugunthan_Shoemaker_Regis_2005,Yoon_Shoemaker_1999}, aeroacoustic shape design~\cite{Marsden_2004,Marsden_Etal_2004}, hydrodynamic design~\cite{Duvigneau_Visonneau_2004}, registration in medical imaging~\cite{Oeuvray_2005,Oeuvray_Bierlaire_2007}, dynamic pricing~\cite{Levina_Etal_2009}, reservoir engineering and engine calibration~\cite{Langouet_2011}, analog circuit design~\cite{Latorre_Etal_2019}, aircraft engine engineering~\cite{Gazaix_Etal_2019}, and chemical product design~\cite{Sun_Etal_2020} for instance.

\section{Optimality conditions for smooth optimization}

\subsection{Local and global solutions}

\begin{definition}[Global solution]
    A point~$x^{\ast} \in \R^n$ is referred to as a \emph{global solution} to problem~\cref{eq:nlcp-intro} if~$x^{\ast} \in \Omega$ and~$f(x) \ge f(x^{\ast})$ for all~$x \in \Omega$.
\end{definition}

\begin{definition}[Local solution]
    A points~$x^{\ast} \in \R^n$ is referred to as
    \begin{itemize}
        \item a \emph{local solution} to problem~\cref{eq:nlcp-intro} if~$x^{\ast} \in \Omega$ and there exists an open neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$f(x) \ge f(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega$.
        \item a \emph{strict local solution} to problem~\cref{eq:nlcp-intro} if~$x^{\ast} \in \Omega$ and there exists an open neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that~$f(x) > f(x^{\ast})$ for all~$x \in \mathcal{N} \cap \Omega \setminus \set{x^{\ast}}$.
        \item an \emph{isolated local solution} to problem~\cref{eq:nlcp-intro} if there exists an open neighborhood~$\mathcal{N} \subseteq \R^n$ of~$x^{\ast}$ such that it is the only local solution in~$\mathcal{N} \cap \Omega$.
    \end{itemize}
\end{definition}

\subsection{Constraint qualifications}

\begin{definition}[Active set]
    The \emph{active set}~$\mathcal{A}(x) \subseteq \iub \cup \ieq$ for problem~\cref{eq:nlcp-intro} at a point~$x \in \Omega$ is defined by
    \begin{equation*}
        \mathcal{A}(x) \eqdef \ieq \cup \set{i \in \iub : \con{i}(x) \ge 0}.
    \end{equation*}
\end{definition}

\begin{definition}[Constraint qualification]
    Let~$x \in \Omega$ be any feasible point, denote~$\mathcal{A}(x)$ the active set for problem~\cref{eq:nlcp-intro} at~$x$, and assume that the constraints function~$\con{i}$ are differentiable at~$x$ for all~$i \in \mathcal{A}(x)$.
    We say that
    \begin{itemize}
        \item the \gls{licq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \mathcal{A}(x)$, and
        \item the \gls{mfcq} holds at~$x$ if the gradients~$\nabla \con{i}(x)$ are linearly independent for all~$i \in \ieq$ and there exists a vector~$d \in \R^n$ such that
        \begin{equation*}
            \begin{cases}
                \inner{\nabla \con{i}(x), d} < 0    & \text{if~$i \in \mathcal{A}(x) \cap \iub$, and}\\
                \inner{\nabla \con{i}(x), d} = 0    & \text{if~$i \in \ieq$}.
            \end{cases}
        \end{equation*}
    \end{itemize}
\end{definition}

\begin{itemize}
    \item \gls{acq}.
    \item \gls{gcq}.
    \item \gls{lcq}.
    \item \gls{crcq}.
    \item \gls{cpld}.
    \item \gls{qncq}.
    \item \gls{sc}.
\end{itemize}

\subsection{First-order optimality conditions}

\begin{equation*}
    \lag(x, \lambda) \eqdef f(x) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lambda_i \con{i}(x).
\end{equation*}

\begin{theorem}[\gls{kkt} conditions]
    \begin{equation*}
        \begin{cases}
            \nabla_x \lag(x^{\ast}, \lambda^{\ast}) = 0,\\
            \con{i}(x) \le 0,                               & i \in \iub,\\
            \con{i}(x) = 0,                                 & i \in \ieq,\\
            \lambda_i \con{i}(x) = 0,                       & i \in \iub,\\
            \lambda_i \le 0,                                & i \in \iub.
        \end{cases}
    \end{equation*}
\end{theorem}
\todo{Check the inequalities}

\subsection{Second-order optimality conditions}

\section{Methodology of \glsfmtlong{dfo} algorithms}

\subsection{Frameworks and algorithms for \glsfmtlong{dfo}}

\begin{itemize}
    \item Direct-search and model-based methods.
    \item Line-search and trust-region methods.
    \item Filter methods and hybrid methods.
    \item implicit filtering methods (hybrid between direct-search and line-search).
\end{itemize}

\subsection{Examples of \glsfmtlong{dfo} methods}

\section{Benchmarking tools for \glsfmtlong{dfo} methods}

\subsection{Performance profiles}

\subsection{Data profiles}
