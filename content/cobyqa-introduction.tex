%% contents/cobyqa-introduction.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{\glsfmttext{cobyqa} \textemdash\ a new \glsfmtlong{dfo} method}
\label{ch:cobyqa-introduction}

\section{Statement of the problem}

In this chapter, we introduce a new model-based \gls{dfo} method for solving nonlinearly-constrained problems of the form
\begin{subequations}
    \label{eq:problem-cobyqa}
    \begin{align}
        \min        & \quad \obj(\iter) \label{eq:problem-cobyqa-obj}\\
        \text{s.t.} & \quad \con{i}(\iter) \le 0, ~ i \in \iub, \label{eq:problem-cobyqa-ub}\\
                    & \quad \con{i}(\iter) = 0, ~ i \in \ieq,\\
                    & \quad \xl \le x \le \xu, \label{eq:problem-cobyqa-bd}\\
                    & \quad \iter \in \R^n, \nonumber
    \end{align}
\end{subequations}
where~$\obj$ and~$\con{i}$ represent the objective and constraint functions, with~$i \in \iub \cup \ieq$ and the sets of indices~$\iub$ and~$\ieq$ being finite and disjoint, but possibly empty. 
The lower bounds~$\xl \in (\R \cup \set{-\infty})^n$ and the upper bounds~$\xu \in (\R \cup \set{\infty})^n$ satisfy~$\xl < \xu$.

We will develop a derivative-free trust-region \gls{sqp} method for solving the problem~\cref{eq:problem-cobyqa}.
The method, named~\gls{cobyqa} after \emph{\glsdesc{cobyqa}}, does not use derivatives of the objective function and the nonlinear constraint functions, but models them using underdetermined interpolation based on the derivative-free symmetric Broyden update (see \cref{subsec:symmetric-broyden-updates}).
This chapter presents the framework of the method, while the subproblems and the Python implementation will be discussed in \cref{ch:cobyqa-subproblems,ch:cobyqa-implementation}, respectively.

\section{The \glsfmtlong{sqp} (\glsfmtshort{sqp}) method}

We present in this section the basic idea behind the \gls{sqp} method.
For this discussion, we assume that the derivatives of the objective and constraints functions are available, as is the case in the classical \gls{sqp} method.
However, we will extend the method to the derivative-free context in \cref{subsec:derivative-free-sqp}.

Note that the problem~\cref{eq:problem-cobyqa} formulates the bound constraints~\cref{eq:problem-cobyqa-bd} explicitely.
This is important in computation, because it may not be reasonable to handle the bounds in the same way as the others due to their different natures (see \cref{subsec:bound-constraints} for details).
However, in the theoretical development of this section and the next one (\cref{sec:trust-region}), it is not necessary to distinguish bound constraints from the others.
Therefore, throughout these two sections, we consider for simplicity a problem of the form
\begin{subequations}
    \label{eq:problem-cobyqa-sqp}
    \begin{align}
        \min        & \quad \obj(\iter) \label{eq:problem-cobyqa-sqp-obj}\\
        \text{s.t.} & \quad \con{i}(\iter) \le 0, ~ i \in \iub, \label{eq:problem-cobyqa-sqp-ub}\\
                    & \quad \con{i}(\iter) = 0, ~ i \in \ieq,\\
                    & \quad \iter \in \R^n. \nonumber
    \end{align}
\end{subequations}
where~$\iub$ contains the bound constraints, if any.

\begin{remark}
    Comparing~\cref{eq:problem-cobyqa} with~\cref{eq:problem-cobyqa-sqp}, we are abusing the notations, because~$\iub$ does not represent the same set of constraints in both.
    However, this will not generate any confusion in our discussions.
\end{remark}

Note that the problem~\cref{eq:problem-cobyqa-sqp} is exactly the problem~\cref{eq:problem-introduction} discussed in \cref{ch:introduction} and hence, all the theory mentioned is applicable.
For our later discussion, recall in particular that the Lagrangian function of this problem is defined by
\begin{equation*}
    \lag(\iter, \lm) \eqdef \obj(\iter) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lm_i \con{i}(\iter), \quad \text{for~$\iter \in \R^n$ and~$\lm_i \in \R$, with~$i \in \iub \cup \ieq$,}
\end{equation*}
where~$\lm = [\lm_i]_{i \in \iub \cup \ieq}^{\T}$ is the dual variable of the considered problem.

\subsection{Overview of the method}

The \gls{sqp} method is known to be one of the most powerful method for solving the problem~\cref{eq:problem-cobyqa-sqp} when derivatives of~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are available.
The classical \gls{sqp} framework is presented in \cref{alg:sqp}.

\begin{algorithm}
    \caption{Classical \glsfmtshort{sqp} method}
    \label{alg:sqp}
    \DontPrintSemicolon
    \KwData{Initial guess~$\iter[0] \in \R^n$ and estimated Lagrange multiplier~$\lm[0] = [\lm[0]_i]_{i \in \iub \cup \ieq}^{\T}$.}
    \For{$k = 0, 1, \dots$}{
        Define~$H^k \approx \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$\;
        Generate a step~$\step[k] \in \R^n$ by solving approximately
        \begin{subequations}
            \label{eq:sqp-subproblem}
            \begin{algomathalign}
                \min        & \quad \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} H^k \step \label{eq:sqp-subproblem-obj}\\
                \text{s.t.} & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub,\\
                            & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
                            & \quad \step \in \R^n, \nonumber
            \end{algomathalign}
        \end{subequations}
        Update the iterate~$\iter[k + 1] \gets \iter[k] + \step[k]$\;
        Estimate the Lagrange multiplier~$\lm[k + 1] = [\lm[k + 1]_i]_{i \in \iub \cup \ieq}^{\T}$
    }
\end{algorithm}

The ealiest reference of such a method appeared in the Ph.D. thesis of \citeauthor{Wilson_1963}~\cite{Wilson_1963}, with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.
\Citeauthor{Robinson_1974}~\cite{Robinson_1974} showed the local R-quadratic convergence rate of this method.
% Check for Q-quadratic convergence.
Later on, \citeauthor{Garcia-Palomares_Mangasarian_1976}~\cite{Garcia-Palomares_1973,Garcia-Palomares_Mangasarian_1976} modified it using a quasi-Newton update for calculating~$H^k$, and established a local R-superlinear convergence rate for such an algorithm.
A similar method was introduced by \citeauthor{Han_1976}~\cite{Han_1976,Han_1977}, but he only approximated~$\nabla_{x, x}^2 \lag(\iter[k], \lm[k])$, while \citeauthor{Garcia-Palomares_Mangasarian_1976} applied quasi-Newton approximations to the whole matrix~$\nabla^2 \lag(\iter[k], \lm[k])$.
In addition, \citeauthor{Han_1976} introduced a line-search strategy to guarantee the global convergence and local Q-superlinear convergence rate, requiring that~$\nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast])$ is positive definite at the solution~$(\iter[\ast], \lm[\ast])$.
\Citeauthor{Powell_1978a}~\cite{Powell_1978b,Powell_1978a,Powell_1978c} studied the method along the same direction.
In particular, he proposed to apply the damped BFGS quasi-Newton formula~\cite[Eqs.~(5.8),~(5.9), and~(5.10)]{Powell_1978b} to update~$H^k$.
This formula guarantees the positive definiteness of such a matrix, which is beneficial in practice and in theory (see the comments towards the end of~\cite[\S~2]{Powell_1978a}).
Moreover, he introduced a practical line-search technique based on a merit function suggested by \citeauthor{Han_1976}~\cite{Han_1976}.
Furthermore, \citeauthor{Powell_1978c} established the global convergence and the local R-superlinear convergence rate for his method, without requiring the positive definiteness of~$\nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast])$ as \citeauthor{Han_1976} did.
Recognizing the controbutions of \citeauthor{Wilson_1963}, \Citeauthor{Han_1976}, and \citeauthor{Powell_1978a}, the \gls{sqp} method is also referred to as the Wilson-Han-Powell method~\cite{Schittkowski_1981,Burke_1992}.
See~\cite{Boggs_Tolle_1995} for a more detailed review on the history, theory, and practice of the \gls{sqp} method.

\subsection{A simple example}

In \cref{alg:sqp}, it is crucial that~$H^k$ approximates~$\nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.
It may be tempting to set~$H^k \approx \nabla \obj(\iter[k])$, because the objective function of the \gls{sqp} subproblem would then be a local quadratic approximation of~$\obj$ at~$\iter[k]$.
However, such a naive idea does not work, as illustrated by the following~$2$-dimensional example inspired by \citeauthor{Boggs_Tolle_1995}~\cite[\S~2.2]{Boggs_Tolle_1995}.

We consider
\begin{align*}
    \min        & \quad -\iter_1 - \frac{(\iter_2)^2}{4}\\
    \text{s.t.} & \quad \norm{\iter}^2 - 1 = 0,\\
                & \quad \iter \in \R^2,
\end{align*}
whose solution is~$\iter[\ast] = [1, 0]^{\T}$ with the associated Lagrange multiplier~$\lm[\ast] = 1/2$.
Suppose that we have an iterate~$\iter[k] = [t, 0]^{\T}$ with~$t \approx 1$, so that it is already close to the solution.
If~$H^k = \nabla^2 \obj(\iter[k])$, then the \gls{sqp} subproblem would become
\begin{subequations}
    \begin{align}
        \min        & \quad -\step_1 - \frac{(\step_2)^2}{4} \label{eq:boggs-tolle-sp-obj}\\
        \text{s.t.} & \quad \step_1 = \frac{1 - t^2}{2 t}, \label{eq:boggs-tolle-sp-eq}\\
                    & \quad \step \in \R^2. \nonumber
    \end{align}
\end{subequations}
This subproblem is unbounded from below, regardless of~$t$.
In addition, the more~$\step[k]$ reduces~\cref{eq:boggs-tolle-sp-obj}, the larger~$\norm{\iter[k] + \step[k] - \iter[\ast]}$ is.
Moreover, if~$t = 1$, we have~$\iter[k] = \iter[\ast]$, but any feasible point~$\step[k]$ for~\cref{eq:boggs-tolle-sp-eq} will push~$\iter[k] + \step[k]$ away from~$\iter[\ast]$, unless~$\step[k]$ is the global maximizer of~\cref{eq:boggs-tolle-sp-obj} subject to~\cref{eq:boggs-tolle-sp-eq}.

We now consider the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ and~$\lm[k] \approx \lm[\ast] = 1/2$.
It is
\begin{align*}
    \min        & \quad -\step_1 + \lm[k] (\step_1)^2 + \bigg( \lm[k] - \frac{1}{4} \bigg) (\step_2)^2\\
    \text{s.t.} & \quad \step_1 = \frac{1 - t^2}{2 t},\\
                & \quad \step \in \R^2. \nonumber
\end{align*}
When~$\lm[k] > 1/4$, the solution to this subproblem is
\begin{equation*}
    \step[k] =
    \begin{bmatrix}
        \dfrac{1 - t^2}{2 t}    & 0
    \end{bmatrix}^{\T}.
\end{equation*}
We thus have
\begin{equation*}
    \iter[k] + \step[k] = 
    \begin{bmatrix}
        \dfrac{t^2 + 1}{2 t}  & 0
    \end{bmatrix}^{\T}.
\end{equation*}
If we set~$\iter[k + 1] = \iter[k] + \step[k]$ and continue to iterate in this way, we will obtain a sequence of iterates that converges quadratically to~$\iter[\ast]$, because
\begin{equation*}
    \norm{\iter[k] + \step[k] - \iter[\ast]} = \frac{(1 - t)^2}{2 \abs{t}} = \bigo(\norm{\iter[k] - \iter[\ast]}^2).
\end{equation*}
This is not surprising, since \citeauthor{Robinson_1974}~\cite{Robinson_1974} showed the local R-quadratic convergence rate of the \gls{sqp} method when~$H^k$ is the exact Hessian matrix of the Lagrangian function with respect to~$x$.

To summarize, as indicated by this example, choosing~$H^k \approx \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ instead of~$H^k \approx \nabla \obj(\iter[k])$ in~\cref{eq:sqp-subproblem} is crucial.

\subsection{Interpretation of the \glsfmtshort{sqp} subproblem}

To get some insight into the origin of the \gls{sqp} method, we now interpret the \gls{sqp} subproblem~\cref{eq:sqp-subproblem}.
Since we consider the subproblem only within one iteration, we omit the iteration counter~$k$ in what follows\footnote{With this simplification, the notation~$\nabla_x \lag(\iter, \lm)$ is a bit problematic, because~$x$ stands for both a variable and a given point, but it will not lead to confusions.}.
In other words, given~$\iter \in \R^n$,~$\lm = [\lm_i]_{i \in \iub \cup \ieq}^{\T}$, and~$H \approx \nabla_{x, x}^2 \lag(\iter, \lm)$, we consider the following subproblem of~$\step$
\begin{subequations}
    \label{eq:sqp-subproblem-nok}
    \begin{align}
        \min        & \quad \nabla \obj(\iter)^{\T} \step + \frac{1}{2} \step^{\T} H \step\\
        \text{s.t.} & \quad \con{i}(\iter) + \nabla \con{i}(\iter)^{\T} \step \le 0, ~ i \in \iub,\\
                    & \quad \con{i}(\iter) + \nabla \con{i}(\iter)^{\T} \step = 0, ~ i \in \ieq,\\
                    & \quad \step \in \R^n. \nonumber
    \end{align}
\end{subequations}

\subsubsection{Bilinear approximation of the \glsfmtlong{kkt} conditions}

This is the most classical interpretation of the \gls{sqp} subproblem.
According to \cref{thm:first-order-necessary-conditions}, if~$\iter[\ast] \in \R^n$ is a local solution to the problem~\cref{eq:problem-cobyqa-sqp}, under some mild assumptions, there exists a Lagrange multiplier~$\lm[\ast] = [\lm[\ast]_i]_{i \in \iub \cup \ieq}^{\T}$ with~$\lm[\ast]_i \in \R$ for all~$i \in \iub \cup \ieq$ such that
\begin{subequations}
    \label{eq:sqp-kkt}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \nabla_x \lag(\iter[\ast], \lm[\ast]) = 0,    && \\
        & \con{i}(\iter[\ast]) \le 0,                   && \quad \text{if~$i \in \iub$,}\\
        & \con{i}(\iter[\ast]) = 0,                     && \quad \text{if~$i \in \ieq$,}\\
        & \lm[\ast]_i \con{i}(\iter[\ast]) = 0,         && \quad \text{if~$i \in \iub$,} \label{eq:sqp-kkt-complementary-slackness}\\
        & \lm[\ast]_i \ge 0,                            && \quad \text{if~$i \in \iub$.}
    \end{empheq}
\end{subequations}
Regard~\cref{eq:sqp-kkt} as a nonlinear system of inequalities and equalities, and~$(\iter, \lm)$ as an approximation of~$(\iter[\ast], \lm[\ast])$.
If we want to solve this system by the Newton-Raphson method\footnote{Discussions are needed on how to apply the Newton-Raphson method to systems of nonlinear inequalities and equalities. We will not go further in this direction but refer to \cite{Pshenichnyi_1970a,Pshenichnyi_1970b,Robinson_1972b,Daniel_1973} for fundamental works on this topic.} starting from~$(\iter, \lm)$, we would seek a step~$(\step, u)$ that satisfies the system
\begin{subequations}
    \label{eq:sqp-kkt-linearization}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \nabla_x \lag(\iter, \lm + u) + \nabla_{x, x}^2 \lag(\iter, \lm) \step = 0,           && \\
        & \con{i}(\iter) + \nabla \con{i}(\iter)^{\T} \step \le 0,                              && \quad \text{if~$i \in \iub$,}\\
        & \con{i}(\iter) + \nabla \con{i}(\iter)^{\T} \step = 0,                                && \quad \text{if~$i \in \ieq$,}\\
        & \lm_i [\con{i}(\iter) + \nabla \con{i}(\iter)^{\T} \step] + u_i \con{i}(\iter) = 0,   && \quad \text{if~$i \in \iub$,} \label{eq:sqp-kkt-linearization-complementary-slackness}\\
        & \lm_i + u_i \ge 0,                                                                    && \quad \text{if~$i \in \iub$,}
    \end{empheq}
\end{subequations}
which is a linear approximation of~\cref{eq:sqp-kkt} at~$(\iter, \lm)$.
However, as pointed out by \citeauthor{Robinson_1972a}~\cite[Rem.~3]{Robinson_1972a}, an objection to such a method is that it would not solve even a linear programming problem in one iteration.
To cope with this deffect, we let~$(d, u)$ solve instead the following bilinear approximation of~\cref{eq:sqp-kkt},
\begin{subequations}
    \label{eq:sqp-subproblem-kkt}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \nabla_x \lag(\iter, \lm + u) + \nabla_{x, x}^2 \lag(\iter, \lm) \step = 0,   && \\
        & \con{i}(\iter) + \nabla \con{i}(\iter)^{\T} \step \le 0,                      && \quad \text{if~$i \in \iub$,}\\
        & \con{i}(\iter) + \nabla \con{i}(\iter)^{\T} \step = 0,                        && \quad \text{if~$i \in \ieq$,}\\
        & (\lm_i + u_i) [\con{i}(\iter) + \nabla \con{i}(\iter)^{\T} \step] = 0,        && \quad \text{if~$i \in \iub$,} \label{eq:sqp-subproblem-kkt-complementary-slackness}\\
        & \lm_i + u_i \ge 0,                                                            && \quad \text{if~$i \in \iub$.}
    \end{empheq}
\end{subequations}
Its only difference with the system~\cref{eq:sqp-kkt-linearization} lies in the condition~\cref{eq:sqp-subproblem-kkt-complementary-slackness}, which now includes the bilinear term~$u_i \nabla \con{i}(\iter)^{\T} \step$.
If the problem~\cref{eq:problem-cobyqa} is a linear programming problem, then~\cref{eq:sqp-subproblem-kkt} is precisely its \gls{kkt} system, while~\cref{eq:sqp-kkt-linearization} is only an approximation.
Observe that the bilinear system~\cref{eq:sqp-subproblem-kkt} is nothing but the \gls{kkt} conditions of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-nok}, with~$\lm + u$ being the Lagrange multiplier.
Therefore, a \gls{kkt} pair for the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-nok} is similar to a Newton-Raphson step for the \gls{kkt} system of the problem~\cref{eq:problem-cobyqa}, and it is even better in the sense that the resulting method solves a linear programming problem in one iteration.

Note that discrepancy between the systems~\cref{eq:sqp-kkt-linearization,eq:sqp-subproblem-kkt} disappears if~$\iub = \emptyset$ in the problem~\cref{eq:problem-cobyqa} and hence, a \gls{kkt} pair for the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-nok} is exactly a Newton-Raphson step for the \gls{kkt} system of the problem~\cref{eq:problem-cobyqa} in such a situation.

\subsubsection{Approximation of a modified Lagrangian}

This interpretation is due to \citeauthor{Robinson_1972a}~\cite[Rem.~4]{Robinson_1972a}.
Let~$\widetilde{\lag}$ be the function
\begin{equation*}
    \widetilde{\lag}(\step, u) \eqdef \obj(\iter + d) + \sum_{\mathclap{i \in \iub \cup \ieq}} (\lm_i + u_i) \delta_i(\iter + \step), \quad \text{for~$\step \in \R^n$ and~$u_i \in \R$, with~$i \in \iub \cup \ieq$},
\end{equation*}
where~$\delta_i$, for~$i \in \iub \cup \ieq$, denotes the departure from linearity\footnote{When~$\con{i}$ is strictly convex, it is the Bregman distance~\cite{Bregman_1967} associated with~$\con{i}$ for the point~$\iter$.}~\cite{Hoek_1982} associated with~$\con{i}$ for the point~$\iter$, defined by
\begin{equation*}
    \delta_i(\iter + \step) \eqdef \con{i}(\iter + \step) - \con{i}(\iter) - \nabla \con{i}(\iter)^{\T} \step, \quad \text{for~$\iter \in \R^n$.}
\end{equation*}
% Note that~$\widetilde{\lag}$ can be regarded as the Lagrangian function of the minimization of~$\obj$ subject to~$\delta_i(x) \le 0$ for~$i \in \iub$ and~$\delta_i(x) = 0$ for~$i \in \ieq$.
The \gls{sqp} subproblem~\cref{eq:sqp-subproblem} with~$H = \nabla_{x, x}^2 \lag(\iter, \lm)$ can then be seen as the minimization of the second-order Taylor approximation of~$\widetilde{\lag}$ subject to the linearized constraints, i.e.,
\begin{align*}
    \min        & \quad \nabla_d \widetilde{\lag}(\iter[k], \lm[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \widetilde{\lag}(\iter[k], \lm[k]) \step\\
    \text{s.t.} & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub,\\
                & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
                & \quad \step \in \R^n.
\end{align*}

\subsubsection{Approximation of the augmented Lagrangian}

For clarity, we assume here without loss of generality that~$\ieq = \emptyset$.
The problem~\cref{eq:problem-cobyqa-sqp} can be reformulated by introducing a slack variable~$s = [s_i]_{i \in \iub}^{\T}$ as
\begin{align*}
    \min        & \quad \obj(\iter)\\
    \text{s.t.} & \quad \con{i}(\iter) + s_i = 0, ~ i \in \iub,\\
                & \quad s_i \ge 0, ~ i \in \iub,\\
                & \quad \iter \in \R^n.
\end{align*}
Let~$\lag[\mathsf{A}]$ be the augmented Lagrangian function of this problem, i.e.,
\begin{equation}
    \label{eq:augmented-lagrangian-inequality}
    \lag[\mathsf{A}](\iter, s, \lm) \eqdef \obj(\iter) + \sum_{i \in \iub} \lm_i [\con{i}(\iter) + s_i] + \frac{\gamma}{2} \sum_{i \in \iub} [\con{i}(\iter) + s_i]^2,
\end{equation}
for~$\iter \in \R^n$ and~$s_i, \lm_i \in \R$ for~$i \in \iub$, where~$\gamma \ge 0$ is a given penalty parameter.
Given an iterate~$\iter[k] \in \R^n$, a slack variable~$s = [s_i]_{i \in \iub}^{\T}$, and a Lagrange multiplier~$\lm[k] = [\lm[k]_i]_{i \in \iub}^{\T}$, we approximate the augmented Lagrangian function~\cref{eq:augmented-lagrangian-inequality} by the following Taylor-like development.
We replace~$\obj$ by its second-order approximation, and~$\con{i}$ by its second-order approximation in the first sum and by its first-order approximation in the second sum, that is,
\begin{align*}
    \lag[\mathsf{A}](\iter[k] + \step, s^k, \lm[k]) & \approx \obj(\iter[k]) + \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla^2 \obj(\iter[k]) \step\\
                                                    & \qquad + \sum_{i \in \iub} \lm[k]_i \bigg[ \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla^2 \con{i}(\iter[k]) \step + s_i \bigg]\\
                                                    & \qquad + \frac{\gamma}{2} \sum_{i \in \iub} [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step + s_i]^2\\
                                                    & \approx \obj(\iter[k]) + \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step\\
                                                    & \qquad + \sum_{i \in \iub} \lm[k]_i [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step + s_i]\\
                                                    & \qquad + \frac{\gamma}{2} \sum_{i \in \iub} [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step + s_i]^2,
\end{align*}
for~$\step \in \R^n$.
We thus remark the right-hand side of this equation is nothing but the augmented Lagrangian function of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.

Moreover, the second-order Taylor approximation of~$\lag[\mathsf{A}]$ provides
\begin{align*}
    \lag[\mathsf{A}](\iter[k] + d, s^k, \lm[k]) & \approx \obj(\iter[k]) + \nabla \obj(\iter[k])^{\T} d + \frac{1}{2} d^{\T} \tilde{H}^k(\lm[k]) d\\
                                                & \qquad + \sum_{i \in \iub} \lm[k]_i [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} d + s_i]\\
                                                & \qquad + \frac{\gamma}{2} \sum_{i \in \iub} [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} d + s_i]^2\\
\end{align*}
where~$\tilde{H}^k$ is defined by
\begin{equation*}
    \tilde{H}^k(\lm) \eqdef \nabla \obj(\iter[k]) + \sum_{i \in \iub} [\lm_i + \gamma (\con{i}(\iter[k]) + s_i)] \nabla^2 \con{i}(\iter[k]), \quad \text{for~$\lm_i \in \R$, with~$i \in \iub$.}
\end{equation*}
This approximation of~$\lag[\mathsf{A}]$ is the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} with~$H^k = \tilde{H}^k(\lm[k])$.
Interestingly, a traditional augmented Lagrangian method for solving the problem~\cref{eq:problem-cobyqa-sqp} would update the dual variable~$\lm[k]$ as
\begin{equation*}
    \lm_i^{k + 1} = \lm[k]_i + \gamma (\con{i}(\iter[k]) + s_i), \quad \text{for~$i \in \iub$}.
\end{equation*}
Therefore, this approximation of~$\lag[\mathsf{A}]$ can be interpreted as the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k + 1])$.

\subsection{A derivative-free \glsfmtlong{sqp} method}
\label{subsec:derivative-free-sqp}

At the~$k$th iteration, \gls{cobyqa} builds the models~$\objm[k]$ and~$\conm[k]{i}$ of the objective function~$\obj$ and the constraints functions~$\con{i}$, with~$i \in \iub \cup \ieq$, using the derivative-free Broyden updates presented in~\cref{subsec:symmetric-broyden-updates}.
A poised interpolation set~$\xpt[k] \subseteq \R^n$ is maintained, whose cardinal number is fixed.
The only restriction is
\begin{equation*}
    n + 2 \le \card(\xpt[k]) \le \frac{1}{2} (n + 1) (n + 2),
\end{equation*}
as otherwise, the set~$\xpt[k]$ cannot be poised (see \cref{subsec:symmetric-broyden-updates} for details).
More details on this interpolation set is given in \cref{subsec:cobyqa-models}.
We also denote by~$\lagm[k]$ the Lagrangian function
\begin{equation*}
    \lagm[k](\iter, \lm) \eqdef \objm[k](\iter) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lm_i \conm[k]{i}(\iter), \quad \text{for~$\iter \in \R^n$ and~$\lm_i \in \R$, with~$i \in \iub \cup \ieq$.}
\end{equation*}
The derivative-free \gls{sqp} method we consider is presented in \cref{alg:derivative-free-sqp}.

\begin{algorithm}
    \caption{Derivative-free \glsfmtshort{sqp} method}
    \label{alg:derivative-free-sqp}
    \DontPrintSemicolon
    \KwData{Initial guess~$\iter[0] \in \R^n$.}
    Build an initial interpolation set~$\xpt[0] \subseteq \R^n$ with~$\iter[0] \in \xpt[0]$\;
    \For{$k = 0, 1, \dots$}{
        Evaluate~$\objm[k]$ and~$\conm[k]{i}$ for~$i \in \iub \cup \ieq$ by underdetermined interpolation on~$\xpt[k]$\;
        Generate a step~$\step[k] \in \R^n$ by solving approximately
        \begin{algomathalign*}
            \min        & \quad \nabla \objm[k](\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lagm[k](\iter[k], \lm[k]) \step\\
            \text{s.t.} & \quad \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub,\\
                        & \quad \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
                        & \quad \step \in \R^n,
        \end{algomathalign*}
        Replace a point in~$\xpt[k]$ by~$\iter[k + 1]$ to obtain~$\xpt[k + 1]$ \nllabel{alg:derivative-free-sqp-set}\;
        Update the iterate~$\iter[k + 1] \gets \iter[k] + \step[k]$\;
        Estimate the Lagrange multiplier~$\lm[k + 1] = [\lm[k + 1]_i]_{i \in \iub \cup \ieq}^{\T}$\;
    }
\end{algorithm}

Recall that the set~$\xpt[k]$ must be ensured poised.
Therefore, the point chosen in \cref{alg:derivative-free-sqp-set} must be chosen so that~$\xpt[k + 1]$ is poised.
It is however possible that such a point does not exist.
The difficulty of maintaining an appropriate geometry of the interpolation set is discussed in \cref{subsec:geometry-improving-iterations}, and requires to modify \cref{alg:derivative-free-sqp}.
As we will see, traditionally, geometry-improving steps are incorporating to the method, which modify a point from the interpolation set to improve its geometry.

\section{The trust-region framework}
\label{sec:trust-region}

The \gls{sqp} framework suffers from the major deffect that it is a local method.
Therefore, the convergence of the method cannot be ensured for any initial guess~$\iter[0] \in \R^n$.
To cope with this difficulty, the method is usually embedded in a globalization strategy, such as a line-search or a trust-region framework.
We consider in this thesis trust-region methods only.
Assume that we have a merit function~$\merit[k]$ whose values depend on~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, and a merit function~$\meritm[k]$ whose values depend on~$\objm[k]$ and~$\conm[k]{i}$, with~$i \in \iub \cup \ieq$.
Details on the merit functions employed by \gls{cobyqa} is given in \cref{subsec:cobyqa-merit-function}.
\Cref{alg:derivative-free-trust-region-sqp} presents the trust-region framework.

\begin{algorithm}
    \caption{Derivative-free trust-region \glsfmtshort{sqp} method}
    \label{alg:derivative-free-trust-region-sqp}
    \DontPrintSemicolon
    \KwData{Initial guess~$\iter[0] \in \R^n$ and initial trust-region radius~$\rad[0] > 0$.}
    Build an initial interpolation set~$\xpt[0] \subseteq \R^n$ with~$\iter[0] \in \xpt[0]$\;
    \For{$k = 0, 1, \dots$}{
        Evaluate~$\objm[k]$ and~$\conm[k]{i}$ for~$i \in \iub \cup \ieq$ by underdetermined interpolation on~$\xpt[k]$\;
        Generate a step~$\step[k] \in \R^n$ by solving approximately
        \begin{subequations}
            \label{eq:trust-region-sqp-subproblem}
            \begin{algomathalign}
                \min        & \quad \nabla \objm[k](\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lagm[k](\iter[k], \lm[k]) \step \label{derivative-free-trust-region-sqp-subproblem-obj}\\
                \text{s.t.} & \quad \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub,\\
                            & \quad \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
                            & \quad \norm{\step} \le \rad[k],\\
                            & \quad \step \in \R^n,
            \end{algomathalign}
        \end{subequations}
        Evaluate the trust-region ratio
        \begin{algomathdisplay}
            \ratio[k] \gets \frac{\merit[k](\iter[k]) - \merit[k](\iter[k] + \step[k])}{\meritm[k](\iter[k]) - \meritm[k](\iter[k] + \step[k])}
        \end{algomathdisplay} \nllabel{alg:derivative-free-trust-region-sqp-ratio}
        \eIf{$\ratio[k] \ge 0$}{
            Choose a point~$\bar{y} \in \xpt[k]$ to remove from~$\xpt[k]$\;
        }{
            Choose a point~$\bar{y} \in \xpt[k] \setminus \set{\iter[k]}$ to remove from~$\xpt[k]$\;
        }
        Update the interpolation set~$\xpt[k + 1] \gets (\xpt[k] \setminus \set{\bar{y}}) \cup \set{\iter[k] + \step[k]}$\;
        Update the current iterate~$\iter[k + 1] \gets \argmin \set{\merit[k](y)}_{y \in \xpt[k + 1]}$\;
        Calculate~$\rad[k + 1]$ according to the values of~$\ratio[k]$ and~$\rad[k]$ \nllabel{alg:derivative-free-trust-region-sqp-radius}\;
    }
\end{algorithm}

Details on the update of the trust-region radius in \cref{alg:derivative-free-trust-region-sqp-radius} is provided in \cref{subsec:trust-region-radius}.
Once again, this framework presents a very simplified version of the derivative-free trust-region \gls{sqp} framework.
It must be modified to address the difficulties related to the geometry of the interpolation set.
This is taken into account in the framework presented in \cref{subsec:geometry-improving-iterations}.

The trust-region ratio in \cref{alg:derivative-free-trust-region-sqp-ratio} is a scalar that represents the performance of the models.
Therefore, the closer from one~$\ratio[k]$ is, the best the~$k$th models perform.
Moreover, as long as it nonnegative, the trial step~$\step[k]$ does not provide any increase in the merit function~$\merit[k]$.

If we were naively adapting the trust-region framework for unconstrained optimization to our setting, the choice of~$\iter[k + 1]$ would be either~$\iter[k] + \step[k]$ if~$\ratio[k] \ge 0$, and~$\iter[k]$ otherwise.
However, this would not take into account the fact that~$\merit[k]$ might differ from~$\merit[k - 1]$.
Therefore,~$\iter[k + 1]$ might not be the best point in~$\xpt[k + 1]$ according to~$\merit[k]$.
In \cref{alg:derivative-free-trust-region-sqp} however,~$\iter[k + 1]$ is always the best point in~$\xpt[k + 1]$.
Moreover, note that~$\xpt[k + 1]$ necessarily contains~$\iter[k] + \step[k]$, and also contain~$\iter[k]$ if~$\ratio[k] < 0$.
Thus, our adaptation of the trust-region framework for unconstrained optimization is reasonable.

\subsection{Managing the trust-region radius}
\label{subsec:trust-region-radius}

Inspired by the performance of \gls{uobyqa}, \gls{newuoa}, \gls{bobyqa}, and \gls{lincoa} (see \cref{subsec:uobyqa,subsec:newuoa-bobyqa-lincoa}), we employ the following paradigm for managing the trust-region radius, proposed by Powell~\cite{Powell_2002,Powell_2006,Powell_2009}.
It consists in maintaining both the trust-region radius~$\rad[k] > 0$ and a lower-bound of it~$\radlb[k] > 0$.
The idea behind this technique is to use~$\rad[k]$ as the trust-region radius in the trust-region subproblem~\cref{eq:trust-region-sqp-subproblem}, and~$\radlb[k]$ to maintain an adequate distance between the points in~$\xpt[k]$.
Further, the method never increases~$\radlb[k]$, but adapt the value of~$\rad[k]$ in a typical trust-region way.
Of course, the method always ensures that~$\rad[k] \ge \radlb[k]$.
As noted by \citeauthor{Powell_2002}, allowing trial step to have a length larger than~$\radlb[k]$ prevent loss in efficiency that occurred otherwise in his software \gls{uobyqa}.

The update of the trust-region radius~$\rad[k]$ is given in \cref{alg:update-trust-region-radius}.
The parameters chosen in \gls{cobyqa} are~$\eta_1 = 0.1$,~$\eta_2 = 0.7$,~$\eta_3 = 1.4$,~$\theta_1 = 0.5$, and~$\theta_2 = \sqrt{2}$.
This update is untertained at each iteration.

\begin{algorithm}
    \caption{Updating the trust-region radius}
    \label{alg:update-trust-region-radius}
    \DontPrintSemicolon
    \KwData{Current lower bound on the trust-region radius~$\radlb[k] > 0$, current trust-region radius~$\rad[k] \ge \radlb[k]$, current trust-region ratio~$\ratio[k] \in \R$, current trial step~$\step \in \R^n$, and parameters~$0 < \eta_1 \le \eta_2 < 1 \le \eta_3$ and~$0 < \theta_1 < 1 < \theta_2$.}
    \KwResult{Updated trust-region radius~$\rad[k + 1]$.}
    Set the value of~$\rad[k + 1]$ to
    \begin{algoempheq}[left={\rad[k + 1] \gets \empheqlbrace}]{alignat*=2}
        & \theta_1 \rad[k]                                                                      && \quad \text{if~$\ratio[k] \le \eta_1$,}\\
        & \min \set{\theta_1 \rad[k], \norm{\step}}                                               && \quad \text{if~$\eta_1 < \ratio[k] \le \eta_2$,}\\
        & \min \set{\theta_2 \rad[k], \max \set{\theta_1 \rad[k], \theta_1^{-1} \norm{\step}}}    && \quad \text{otherwise}
    \end{algoempheq}
    \If{$\rad[k + 1] \le \eta_3 \radlb[k]$}{
        $\rad[k + 1] \gets \radlb[k]$\;
    }
\end{algorithm}

As we mentioned already, the method maintains~$\rad[k] \ge \radlb[k]$ and it never increases~$\radlb[k]$.
Therefore, the value of~$\radlb[k]$ is decreased only if~$\rad[k] = \radlb[k]$.
Moreover, since~$\radlb[k]$ is designed to maintain a good distance between the interpolation points, it must be decreased only if the performance of the models is poor.
Hence, it is decreased only if the trial step~$\norm{\step[k]}$ is small compared with~$\rad[k]$ and the trust-region ratio~$\ratio[k]$ is small.
\Cref{alg:reducing-lower-bound-trust-region-radius} presents the method employed by \gls{cobyqa} to reduce~$\radlb[k]$.
The parameters chosen in \gls{cobyqa} are~$\eta_4 = 16$,~$\eta_5 = 250$, and~$\theta_3 = 0.1$.

\begin{algorithm}
    \caption{Reducing the lower bound on the trust-region radius}
    \label{alg:reducing-lower-bound-trust-region-radius}
    \DontPrintSemicolon
    \KwData{Final trust-region radius~$\radlb[\infty] > 0$, current lower bound on the trust-region radius~$\radlb[k] \ge \radlb[\infty]$, updated trust-region radius~$\rad[k + 1] \ge \radlb[k]$, and parameters~$1 \le \eta_4 < \eta_5$ and~$0 < \theta_3 < 1$.}
    \KwResult{Reduced lower bound on trust-region radius~$\radlb[k + 1]$ and modified trust-region radius~$\rad[k + 1]$.}
    \If{$\radlb[k] = \radlb[\infty]$}{
        Terminate the optimization method\;
    }
    Set the value of~$\radlb[k + 1]$ to
    \begin{algoempheq}[left={\radlb[k + 1] \gets \empheqlbrace}]{alignat*=2}
        & \theta_3 \radlb[k]                && \quad \text{if~$\eta_5 < \radlb[k] / \radlb[\infty]$,}\\
        & \sqrt{\radlb[k] \radlb[\infty]}   && \quad \text{if~$\eta_4 < \radlb[k] / \radlb[\infty] \le \eta_5$,}\\
        & \radlb[\infty]                    && \quad \text{otherwise}
    \end{algoempheq}
    $\rad[k + 1] \gets \max \set{\rad[k + 1], \radlb[k + 1]}$\;
\end{algorithm}

\subsection{Composite-step approach}

\begin{itemize}
    \item The Vardi-like approach~\cite{Vardi_1985} is similar to an idea proposed by Powell~\cite[Eqs.~(2.7) and~(2.8)]{Powell_1978a}.
    The inactive constraints are not modified by Powell.
\end{itemize}

\subsection{Geometry-improving iterations}
\label{subsec:geometry-improving-iterations}

\section{Outline of the \glsfmttext{cobyqa} method}

\subsection{Management of bound and linear constraints}
\label{subsec:bound-constraints}

The implementation of \gls{cobyqa} accepts three types of constraints, namely bound constraints, linear constraints, and nonlinear constraints.
From a theoretical standpoint, problems written in the form~\cref{eq:problem-cobyqa-sqp} may have bound and linear constraints included in some of the constraints~$\con{i}$, with~$i \in \iub \cup \ieq$.
However, it is crucial that the implementation of a solver handles these types of constraints separately.

First of all, note that in the general form of nonlinearly-constrained problems~\cref{eq:problem-cobyqa}, we did not include the bound constraints~\cref{eq:problem-cobyqa-bd} in the inequality constraints~\cref{eq:problem-cobyqa-ub}.
This is because they often represent inalienable physical or theoretical restrictions.
In other words, in many applications, the objective function~\cref{eq:problem-cobyqa-obj} is not defined if the bounds constraints~\cref{eq:problem-cobyqa-bd} are violated.
For instance, the tuning of nonlinear optimization methods (see \cref{subsec:tuning-nonlinear-optimization-methods})  involves bounds that cannot be violated, as the optimizationmethods that are tuned may not be defined otherwise.
Very similar observations can be made on hyperparameter tuning in machine learning (see \cref{subsec:machine-learning}).
For this reason, every point that \gls{cobyqa} encounters always respects these bounds, as is also the case for the \gls{bobyqa} method, presented in \cref{subsec:newuoa-bobyqa-lincoa}.
When establishing the problem~\cref{eq:problem-cobyqa}, we assumed that~$\xl < \xu$.
Note that this requirement is weak, as otherwise, the problem~\cref{eq:problem-cobyqa} would be either infeasible, or admit fix variables.
Thus, note also that they are very simple constraints.
It is, for example, trivial to check whether a point is feasible with respect to the bound constraints~\cref{eq:problem-cobyqa-bd}, and easy to project any point onto the bound constraints.
This is another reason why \gls{cobyqa} handles them separately.

The linear constraints are usually much less restrictive.
The objective function is often well-defined even at points that are infeasible with respect to the linear constraints.
Therefore, we do not enforce \gls{cobyqa} to always respect the linear constraints.
However, when evaluating a model~$\conm[k]{i}$ of a linear constraint~$\con{i}$, we enforce~$\nabla \conm[k]{i} \equiv \nabla \con{i}$ and~$\nabla^2 \conm[k]{i} \equiv 0$, so that~$\conm[k]{i} \equiv \con{i}$.
This reduces the computational complexity of evaluating all the models, and it also suppresses all damages that could be generated by computer rounding errors.

\subsection{Interpolation-based quadratic models}
\label{subsec:cobyqa-models}

\begin{itemize}
    \item What is the initial interpolation set?
    \item How is this interpolation set updated?
    \item Approximations of linear function are exact (theorem).
\end{itemize}

\subsection{Merit functions and penalty coefficients}
\label{subsec:cobyqa-merit-function}

\begin{itemize}
    \item What is a merit function?
    \item Examples of merit functions (smooth, exact nonsmooth, augmented Lagrangian).
    \item We choose the~$\ell_2$-merit function (exact nonsmooth).
    \item The modeled merit function we choose is not directly the Taylor approximation.
    \item The modeled merit function must ensure a crucial decrease property (holds because of what follows).
    \item The best point so far considered by \gls{cobyqa} is the one with the least merit value.
    \item What is the trust-region ratio?
    \item \Cite[Thm.~14.5.1]{Conn_Gould_Toint_2000} provides a lower bound on the penalty coefficient.
    \item Detail the algorithm for increasing the penalty parameter.
    \item We decrease the penalty parameter as in~\cite{Powell_1994} (detail the modification made for the equality constraints).
\end{itemize}

\subsection{Geometry of the interpolation set}

\subsection{Estimation of the Lagrange multipliers}

\subsection{Maratos effect and \glsfmtlong{soc}}

\subsection{Summary of the method}
