%% contents/sqp.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{The \glsfmtlong{sqp} method}

\section{The \glsfmtlong{sqp} (\glsfmtshort{sqp}) method}

We present in this chapter the basic idea behind the \gls{sqp} method.
For this discussion, we assume that the derivatives of the objective and constraints functions are available, as is the case in the classical \gls{sqp} method.
% However, we will extend the method to the derivative-free context in \cref{subsec:derivative-free-sqp}.

% Note that the problem~\cref{eq:problem-cobyqa} formulates the bound constraints~\cref{eq:problem-cobyqa-bd} explicitely.
% This is important in computation, because it may not be reasonable to handle the bounds in the same way as the others due to their different natures (see \cref{subsec:bound-constraints} for details).
% However, in the theoretical development of this section and the next one (\cref{sec:trust-region}), it is not necessary to distinguish bound constraints from the others.
Throughout this chapter, we consider a problem of the form
\begin{subequations}
    \label{eq:problem-cobyqa-sqp}
    \begin{align}
        \min        & \quad \obj(\iter) \label{eq:problem-cobyqa-sqp-obj}\\
        \text{s.t.} & \quad \con{i}(\iter) \le 0, ~ i \in \iub, \label{eq:problem-cobyqa-sqp-ub}\\
                    & \quad \con{i}(\iter) = 0, ~ i \in \ieq, \label{eq:problem-cobyqa-sqp-eq}\\
                    & \quad \iter \in \R^n. \nonumber
    \end{align}
\end{subequations}
% where~$\iub$ contains the bound constraints, if any.

% \begin{remark}
%     Comparing~\cref{eq:problem-cobyqa} with~\cref{eq:problem-cobyqa-sqp}, we are abusing the notations, because~$\iub$ does not represent the same set of constraints in both.
%     However, this will not generate any confusion in our discussions.
% \end{remark}

Note that the problem~\cref{eq:problem-cobyqa-sqp} is exactly the problem~\cref{eq:problem-introduction} discussed in \cref{ch:introduction} and hence, all the theory mentioned is applicable.
For our later discussion, recall in particular that the Lagrangian of this problem is defined by
\begin{equation*}
    \lag(\iter, \lm) \eqdef \obj(\iter) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lm_i \con{i}(\iter), \quad \text{for~$\iter \in \R^n$ and~$\lm_i \in \R$, with~$i \in \iub \cup \ieq$,}
\end{equation*}
where~$\lm = [\lm_i]_{i \in \iub \cup \ieq}^{\T}$ is the dual variable of the considered problem.

\subsection{Overview of the method}

The \gls{sqp} method is known to be one of the most powerful method for solving the problem~\cref{eq:problem-cobyqa-sqp} when derivatives of~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are available.
The classical \gls{sqp} framework is presented in \cref{alg:sqp}.

\begin{algorithm}
    \caption{Classical \glsfmtshort{sqp} method}
    \label{alg:sqp}
    \DontPrintSemicolon
    \KwData{Initial guess~$\iter[0] \in \R^n$ and estimated Lagrange multiplier~$\lm[0] = [\lm[0]_i]_{i \in \iub \cup \ieq}^{\T}$.}
    \For{$k = 0, 1, \dots$}{
        Define~$H^k \approx \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$\;
        Generate a step~$\step[k] \in \R^n$ by solving approximately
        \begin{subequations}
            \label{eq:sqp-subproblem}
            \begin{algomathalign}
                \min        & \quad \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} H^k \step \label{eq:sqp-subproblem-obj}\\
                \text{s.t.} & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub,\\
                            & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
                            & \quad \step \in \R^n, \nonumber
            \end{algomathalign}
        \end{subequations}
        Update the iterate~$\iter[k + 1] \gets \iter[k] + \step[k]$\;
        Estimate the Lagrange multiplier~$\lm[k + 1] = [\lm[k + 1]_i]_{i \in \iub \cup \ieq}^{\T}$
    }
\end{algorithm}

The ealiest reference of such a method appeared in the Ph.D. thesis of \citeauthor{Wilson_1963}~\cite{Wilson_1963}, with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.
\Citeauthor{Robinson_1974}~\cite{Robinson_1974} showed the local R-quadratic convergence rate of this method.
% Check for Q-quadratic convergence.
Later on, \citeauthor{Garcia-Palomares_Mangasarian_1976}~\cite{Garcia-Palomares_1973,Garcia-Palomares_Mangasarian_1976} modified it using a quasi-Newton update for calculating~$H^k$, and established a local R-superlinear convergence rate for such an algorithm.
A similar method was introduced by \citeauthor{Han_1976}~\cite{Han_1976,Han_1977}, but he only approximated~$\nabla_{x, x}^2 \lag(\iter[k], \lm[k])$, while \citeauthor{Garcia-Palomares_Mangasarian_1976} applied quasi-Newton approximations to the whole matrix~$\nabla^2 \lag(\iter[k], \lm[k])$.
In addition, \citeauthor{Han_1976} introduced a line-search strategy to guarantee the global convergence and local Q-superlinear convergence rate, requiring that~$\nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast])$ is positive definite at the solution~$(\iter[\ast], \lm[\ast])$.
\Citeauthor{Powell_1978a}~\cite{Powell_1978b,Powell_1978a,Powell_1978c} studied the method along the same direction.
In particular, he proposed to apply the damped BFGS quasi-Newton formula~\cite[Eqs.~(5.8),~(5.9), and~(5.10)]{Powell_1978b} to update~$H^k$.
This formula guarantees the positive definiteness of such a matrix, which is beneficial in practice and in theory (see the comments towards the end of~\cite[\S~2]{Powell_1978a}).
Moreover, he introduced a practical line-search technique based on a merit function suggested by \citeauthor{Han_1976}~\cite{Han_1976}.
Furthermore, \citeauthor{Powell_1978c} established the global convergence and the local R-superlinear convergence rate for his method, without requiring the positive definiteness of~$\nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast])$ as \citeauthor{Han_1976} did.
Recognizing the controbutions of \citeauthor{Wilson_1963}, \Citeauthor{Han_1976}, and \citeauthor{Powell_1978a}, the \gls{sqp} method is also referred to as the Wilson-Han-Powell method~\cite{Schittkowski_1981,Burke_1992}.
See~\cite{Boggs_Tolle_1995} for a more detailed review on the history, theory, and practice of the \gls{sqp} method.

\subsection{A simple example}

In \cref{alg:sqp}, it is crucial that~$H^k$ approximates~$\nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.
It may be tempting to set~$H^k \approx \nabla \obj(\iter[k])$, because the objective function of the \gls{sqp} subproblem would then be a local quadratic approximation of~$\obj$ at~$\iter[k]$.
However, such a naive idea does not work, as illustrated by the following~$2$-dimensional example inspired by \citeauthor{Boggs_Tolle_1995}~\cite[\S~2.2]{Boggs_Tolle_1995}.

We consider
\begin{align*}
    \min        & \quad -\iter_1 - \frac{(\iter_2)^2}{4}\\
    \text{s.t.} & \quad \norm{\iter}^2 - 1 = 0,\\
                & \quad \iter \in \R^2,
\end{align*}
whose solution is~$\iter[\ast] = [1, 0]^{\T}$ with the associated Lagrange multiplier~$\lm[\ast] = 1/2$.
Suppose that we have an iterate~$\iter[k] = [t, 0]^{\T}$ with~$t \approx 1$, so that it is already close to the solution.
If~$H^k = \nabla^2 \obj(\iter[k])$, then the \gls{sqp} subproblem would become
\begin{subequations}
    \begin{align}
        \min        & \quad -\step_1 - \frac{(\step_2)^2}{4} \label{eq:boggs-tolle-sp-obj}\\
        \text{s.t.} & \quad \step_1 = \frac{1 - t^2}{2 t}, \label{eq:boggs-tolle-sp-eq}\\
                    & \quad \step \in \R^2. \nonumber
    \end{align}
\end{subequations}
This subproblem is unbounded from below, regardless of~$t$.
In addition, the more~$\step[k]$ reduces~\cref{eq:boggs-tolle-sp-obj}, the larger~$\norm{\iter[k] + \step[k] - \iter[\ast]}$ is.
Moreover, if~$t = 1$, we have~$\iter[k] = \iter[\ast]$, but any feasible point~$\step[k]$ for~\cref{eq:boggs-tolle-sp-eq} will push~$\iter[k] + \step[k]$ away from~$\iter[\ast]$, unless~$\step[k]$ is the global maximizer of~\cref{eq:boggs-tolle-sp-obj} subject to~\cref{eq:boggs-tolle-sp-eq}.

We now consider the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ and~$\lm[k] \approx \lm[\ast] = 1/2$.
It is
\begin{align*}
    \min        & \quad -\step_1 + \lm[k] (\step_1)^2 + \bigg( \lm[k] - \frac{1}{4} \bigg) (\step_2)^2\\
    \text{s.t.} & \quad \step_1 = \frac{1 - t^2}{2 t},\\
                & \quad \step \in \R^2. \nonumber
\end{align*}
When~$\lm[k] > 1/4$, the solution to this subproblem is
\begin{equation*}
    \step[k] =
    \begin{bmatrix}
        \dfrac{1 - t^2}{2 t}    & 0
    \end{bmatrix}^{\T}.
\end{equation*}
We thus have
\begin{equation*}
    \iter[k] + \step[k] = 
    \begin{bmatrix}
        \dfrac{t^2 + 1}{2 t}  & 0
    \end{bmatrix}^{\T}.
\end{equation*}
If we set~$\iter[k + 1] = \iter[k] + \step[k]$ and continue to iterate in this way, we will obtain a sequence of iterates that converges quadratically to~$\iter[\ast]$, because
\begin{equation*}
    \norm{\iter[k] + \step[k] - \iter[\ast]} = \frac{(1 - t)^2}{2 \abs{t}} = \bigo(\norm{\iter[k] - \iter[\ast]}^2).
\end{equation*}
This is not surprising, since \citeauthor{Robinson_1974}~\cite{Robinson_1974} showed the local R-quadratic convergence rate of the \gls{sqp} method when~$H^k$ is the exact Hessian matrix of the Lagrangian with respect to~$x$.

To summarize, as indicated by this example, choosing~$H^k \approx \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ instead of~$H^k \approx \nabla \obj(\iter[k])$ in~\cref{eq:sqp-subproblem} is crucial.

\subsection{Interpretation of the \glsfmtshort{sqp} subproblem}

To get some insight into the origin of the \gls{sqp} method, we interpret the \gls{sqp} subproblem~\cref{eq:sqp-subproblem}.
In what follows, we focus on only one iteration of \cref{alg:sqp} and hence,~$k$ is fixed.
We will explain why it is reasonable to update~$\iter[k]$ by a solution to~\cref{eq:sqp-subproblem}.

\subsubsection{Bilinear approximation of the \glsfmtlong{kkt} conditions}

This is the most classical interpretation of the \gls{sqp} subproblem.
According to \cref{thm:first-order-necessary-conditions}, if~$\iter[\ast] \in \R^n$ is a local solution to the problem~\cref{eq:problem-cobyqa-sqp}, under some mild assumptions, there exists a Lagrange multiplier~$\lm[\ast] = [\lm[\ast]_i]_{i \in \iub \cup \ieq}^{\T}$ with~$\lm[\ast]_i \in \R$ for all~$i \in \iub \cup \ieq$ such that
\begin{subequations}
    \label{eq:sqp-kkt}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \nabla_x \lag(\iter[\ast], \lm[\ast]) = 0,    && \\
        & \con{i}(\iter[\ast]) \le 0,                   && \quad \text{if~$i \in \iub$,}\\
        & \con{i}(\iter[\ast]) = 0,                     && \quad \text{if~$i \in \ieq$,}\\
        & \lm[\ast]_i \con{i}(\iter[\ast]) = 0,         && \quad \text{if~$i \in \iub$,} \label{eq:sqp-kkt-complementary-slackness}\\
        & \lm[\ast]_i \ge 0,                            && \quad \text{if~$i \in \iub$.}
    \end{empheq}
\end{subequations}
Regard~\cref{eq:sqp-kkt} as a nonlinear system of inequalities and equalities, and~$(\iter[k], \lm[k])$ as an approximation of~$(\iter[\ast], \lm[\ast])$.
If we want to solve this system by the Newton-Raphson method\footnote{Discussions are needed on how to apply the Newton-Raphson method to systems of nonlinear inequalities and equalities. We will not go further in this direction but refer to \cite{Pshenichnyi_1970a,Pshenichnyi_1970b,Robinson_1972b,Daniel_1973} for fundamental works on this topic.} starting from~$(\iter[k], \lm[k])$, we would seek a step~$(\step, \mu)$ that satisfies the system
\begin{subequations}
    \label{eq:sqp-kkt-linearization}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \nabla_x \lag(\iter[k], \lm[k] + \mu) + \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step = 0,         && \\
        & \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0,                                    && \quad \text{if~$i \in \iub$,}\\
        & \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0,                                      && \quad \text{if~$i \in \ieq$,}\\
        & \lm[k]_i [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step] + \mu_i \con{i}(\iter[k]) = 0, && \quad \text{if~$i \in \iub$,} \label{eq:sqp-kkt-linearization-complementary-slackness}\\
        & \lm[k]_i + \mu_i \ge 0,                                                                           && \quad \text{if~$i \in \iub$,}
    \end{empheq}
\end{subequations}
which is a linear approximation of~\cref{eq:sqp-kkt} at~$(\iter[k], \lm[k])$.
However, as pointed out by \citeauthor{Robinson_1972a}~\cite[Rem.~3]{Robinson_1972a}, an objection to such a method is that it would not solve even a linear program in one iteration.
To cope with this deffect, we let~$(d, \mu)$ solve instead the following bilinear approximation of~\cref{eq:sqp-kkt},
\begin{subequations}
    \label{eq:sqp-subproblem-kkt}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \nabla_x \lag(\iter[k], \lm[k] + \mu) + \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step = 0, && \\
        & \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0,                            && \quad \text{if~$i \in \iub$,}\\
        & \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0,                              && \quad \text{if~$i \in \ieq$,}\\
        & (\lm[k]_i + \mu_i) [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step] = 0,         && \quad \text{if~$i \in \iub$,} \label{eq:sqp-subproblem-kkt-complementary-slackness}\\
        & \lm[k]_i + \mu_i \ge 0,                                                                   && \quad \text{if~$i \in \iub$.}
    \end{empheq}
\end{subequations}
Its only difference from the system~\cref{eq:sqp-kkt-linearization} lies in the condition~\cref{eq:sqp-subproblem-kkt-complementary-slackness}, which includes the bilinear term~$\mu_i \nabla \con{i}(\iter[k])^{\T} \step$.
If the problem~\cref{eq:problem-cobyqa-sqp} is a linear program, then~\cref{eq:sqp-subproblem-kkt} is precisely its \gls{kkt} system, while~\cref{eq:sqp-kkt-linearization} is only an approximation.
Observe that the bilinear system~\cref{eq:sqp-subproblem-kkt} is nothing but the \gls{kkt} conditions of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem}, with~$\lm[k] + \mu$ being the Lagrange multiplier.
Therefore, a \gls{kkt} pair for the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} is similar to a Newton-Raphson step for the \gls{kkt} system of the problem~\cref{eq:problem-cobyqa-sqp}, and it is even better in the sense that the resulting method solves a linear program in one iteration.

Note that discrepancy between the systems~\cref{eq:sqp-kkt-linearization,eq:sqp-subproblem-kkt} disappears if~$\iub = \emptyset$ in the problem~\cref{eq:problem-cobyqa-sqp} and hence, a \gls{kkt} pair for the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} is exactly a Newton-Raphson step for the \gls{kkt} system of the problem~\cref{eq:problem-cobyqa-sqp} in such a situation.

\subsubsection{Approximation of a modified Lagrangian}

This interpretation is due to \citeauthor{Robinson_1972a}~\cite[Rem.~4]{Robinson_1972a}.
Let~$\widetilde{\lag}$ be the function
\begin{equation*}
    \widetilde{\lag}(\iter, \lm) \eqdef \obj(\iter) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lm_i \delta_i(\iter), \quad \text{for~$\iter \in \R^n$ and~$\lm_i \in \R$, with~$i \in \iub \cup \ieq$},
\end{equation*}
where~$\delta_i$, for~$i \in \iub \cup \ieq$, is defined by
\begin{equation*}
    \delta_i(\iter) \eqdef \con{i}(\iter) - \con{i}(\iter[k]) - \nabla \con{i}(\iter[k])^{\T} (\iter - \iter[k]), \quad \text{for~$\iter \in \R^n$.}
\end{equation*}
The function~$\delta_i$ is referred to as the departure from linearity\footnote{When~$\con{i}$ is strictly convex,~$\delta_i$ defines the Bregman distance~\cite{Bregman_1967} associated with~$\con{i}$.} for~$\con{i}$ at the point~$\iter[k]$~\cite[\S~2]{Gill_Wong_2011}.
The \gls{sqp} subproblem~\cref{eq:sqp-subproblem} with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ can then be seen as the minimization of the second-order Taylor approximation of~$\widetilde{\lag}$ subject to the linear approximations of the constraints~\cref{eq:problem-cobyqa-sqp-ub,eq:problem-cobyqa-sqp-eq} at~$(\iter[k], \lm[k])$, i.e.,
\begin{align*}
    \min        & \quad \nabla_x \widetilde{\lag}(\iter[k], \lm[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \widetilde{\lag}(\iter[k], \lm[k]) \step\\
    \text{s.t.} & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub,\\
                & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
                & \quad \step \in \R^n.
\end{align*}

By expressing its subproblem in this form, we observe that the \gls{sqp} method is a special case of \citeauthor{Robinson_1972a}'s method~\cite{Robinson_1972a}, known to have a local R-quadratic convergence rate.

\subsubsection{Approximation of the objective function in the tangent space of the feasible set}

Inspired by an observation in~\cite[\S~2]{Gill_Wong_2011}, we can also interpret the \gls{sqp} subproblem as minimizing an approximation of the objective function in the tangent space of the feasible set.
As will be shown in \cref{thm:sqp-path}, when approximating~$\obj$ in such a space, we will naturally get the Hessian matrix of the Lagrangian in the second-order term.

For this interpretation, we consider the problem
\begin{subequations}
    \label{eq:problem-cobyqa-auglag}
    \begin{align}
        \min        & \quad \obj(\iter)\\
        \text{s.t.} & \quad h(\iter) = 0,\\
                    & \quad \iter \ge 0, ~ \iter \in \R^n, \nonumber
    \end{align}
\end{subequations}
with~$h : \R^n \to \R^m$.
The problem~\cref{eq:problem-cobyqa-sqp} can be reformulated in this form\footnote{In the reformulation, the dimension and the meaning of~$\iter$ may be altered, but we do not change the notations since it does not lead to confusion}.
Recall that the Lagrangian of~\cref{eq:problem-cobyqa-auglag} is
\begin{equation*}
    \lag(\iter, \lm) \eqdef \obj(\iter) + \lm^{\T} h(\iter), \quad \text{for~$\iter \ge 0$ and~$\lm \in \R^m$.}
\end{equation*}

Let~$\bar{\iter} \in \R^n$,~$\bar{\lm} \in \R^m$ be given, and define
\begin{equation*}
    Q(\step) \eqdef \obj(\bar{\iter}) + \nabla \obj(\bar{\iter})^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lag(\bar{\iter}, \bar{\lm}) \step.
\end{equation*}
If~$\bar{\iter}$ and~$\bar{\lm}$ represent the current iterate and approximate Lagrange multiplier, then~$Q$ is the objective function of the \gls{sqp} subproblem with the exact second-order term.
Therefore, the \gls{sqp} subproblem approximates~$\obj$ by~$Q$ and the feasible set by its tangent space.
Such an approximation is reasonable only if~$Q$ approximates~$\obj$ in this tangent space, which turns out to be true, as detailed by \cref{thm:sqp-path}.

\begin{theorem}
    \label{thm:sqp-path}
    Assume that~$\obj$ and~$h$ are twice differentiable and that~$\nabla^2 \obj$ is Lipschitz continuous in a neighborhood of~$\bar{\iter}$.
    Let~$\iter(t)$ be a feasible path starting at~$\bar{\iter}$ and parametrized by a nonnegative scalar~$t$, i.e.,~$h(\iter(t)) = 0$ and~$\iter(t) \ge 0$ for~$t \ge 0$, and~$\iter(0) = \bar{\iter}$.
    Assume that~$\iter$ is twice differentiable for~$t \ge 0$ and that~$\iter''$ is Lipschitz continuous in a neighborhood of~$0$.
    Then, there exist constants~$\nu \ge 0$ and~$\epsilon > 0$ such that
    \begin{equation*}
        \abs{\obj(\iter(t)) - Q(x'(0) t)} \le \bigg( \nu t + \frac{1}{2}\abs{\iter''(0)^{\T} [\nabla \obj(\bar{\iter}) + \nabla h(\bar{\iter})^{\T} \bar{\lm}]} \bigg) t^2 \quad \text{for all~$t \in [0, \epsilon]$.}
    \end{equation*}
\end{theorem}

\begin{proof}
    Define~$\phi(t) = \obj(x(t))$ for~$t \ge 0$.
    For any~$t \ge 0$, we have
    \begin{subequations}
        \label{eq:sqp-path-proof-1}
        \begin{empheq}[left=\empheqlbrace]{alignat=1}
            & \phi'(t) = \iter'(t)^{\T} \nabla \obj(\iter(t)),\\
            & \phi''(t) = \iter'(t)^{\T} \nabla^2 \obj(\iter(t)) \iter'(t) + \iter''(t)^{\T} \nabla \obj(\iter(t)),
        \end{empheq}
    \end{subequations}
    and by assumption, there exists~$\epsilon > 0$ such that~$\phi''$ is Lipschitz continuous in~$[0, \epsilon]$.
    Let~$\widehat{\phi}$ be the second-order Taylor expansion of~$\phi$ at~$0$.
    We then have
    \begin{equation}
        \label{eq:sqp-path-proof-2}
        \abs{\obj(\iter(t)) - Q(\iter'(0) t)} \le \abs{\phi(t) - \widehat{\phi}(t)} + \abs{\widehat{\phi}(t) - Q(\iter'(0) t)}.
    \end{equation}
    Due to the Lipschitz continuity of~$\phi''$, there exists a constants~$\nu \ge 0$ such that
    \begin{equation}
        \label{eq:sqp-path-proof-3}
        \abs{\phi(t) - \widehat{\phi}(t)} \le \nu t^3 \quad \text{for~$t \in [0, \epsilon]$.}
    \end{equation}
    We now bound~$\abs{\widehat{\phi}(t) - Q(\iter'(0) t)}$.
    According to~\cref{eq:sqp-path-proof-1}, only the second-order terms of~$\widehat{\phi}(t)$ and~$Q(\iter'(0) t)$ differ, and
    \begin{subequations}
        \label{eq:sqp-path-proof-4}
        \begin{align}
            \abs{\widehat{\phi}(t) - Q(x'(0) t)}    & = \frac{t^2}{2} \abs{\phi''(0) - \iter'(0)^{\T} \nabla_{x, x}^2 \lag(\bar{\iter}, \bar{\lm}) \iter'(0)}\\
                                                    & = \frac{t^2}{2} \abs[\bigg]{\iter''(0)^{\T} \nabla \obj(\bar{\iter}) - \sum_{i = 1}^m \bar{\lm}_i \iter'(0) \nabla^2 h_i(\bar{\iter}) \iter'(0)}.
        \end{align}
    \end{subequations}
    Moreover, since~$h(\iter(t)) = 0$ for all~$t \ge 0$, we have
    \begin{equation*}
        0 = \frac{\du^2 h_i(\iter(t))}{\du t^2}\bigg\vert_{t = 0} = \iter'(0)^{\T} \nabla^2 h_i(\bar{\iter}) \iter'(0) + \iter''(0)^{\T} \nabla h_i(\bar{\iter}), \quad \text{for~$i \in \set{1, 2, \dots, m}$.}
    \end{equation*}
    Therefore,~$\iter'(0)^{\T} \nabla^2 h_i(\bar{\iter}) \iter'(0) = -\iter''(0)^{\T} \nabla h_i(\bar{\iter})$ for each~$i$ and hence,~\cref{eq:sqp-path-proof-4} leads to
    \begin{equation}
        \label{eq:sqp-path-proof-5}
        \abs{\widehat{\phi}(t) - Q(x'(0) t)} \le \frac{t^2}{2} \abs{\iter''(0)^{\T} [\nabla \obj(\bar{\iter}) + \nabla h(\bar{\iter})^{\T} \bar{\lm}]}.
    \end{equation}
    Plugging~\cref{eq:sqp-path-proof-3,eq:sqp-path-proof-5} into~\cref{eq:sqp-path-proof-2}, we obtain the desired result.
\end{proof}
s
We observe that the error term in \cref{thm:sqp-path} depends on the magnitude of~$\nabla \obj(\bar{\iter}) + \nabla h(\bar{\iter})^{\T} \bar{\lm}$.
If~$(\bar{\iter}, \bar{\lm})$ is close to \gls{kkt} pair, then~$\norm{\nabla \obj(\bar{\iter}) + \nabla h(\bar{\iter})^{\T} \bar{\lm}}$ is small.
On the other hand, if we defined~$\nabla^2 Q$ by~$\nabla^2 \obj(\bar{x})$ instead of~$\nabla_{x, x}^2 \lag(\bar{\iter}, \bar{\lm})$, then under the assumptions of \cref{thm:sqp-path}, we would have
\begin{equation*}
    \abs{\obj(\iter(t)) - Q(x'(0) t)} \le \bigg( \nu t + \frac{1}{2}\abs{\iter''(0)^{\T} \nabla \obj(\bar{\iter})} \bigg) t^2 \quad \text{for all~$t \in [0, \epsilon]$,}
\end{equation*}
which can be obtained by setting~$\bar{\lm} = 0$ in \cref{thm:sqp-path}.
However, since we are considering constrained optimization, we cannot expect~$\norm{\nabla \obj(\bar{\iter})}$ to be small even if~$\bar{\iter}$ is close to a solution, unless no constraint is active at this solution.
This explains why the second-order term of the \gls{sqp} subproblem should be defined by the Hessian matrix of the Lagrangian, rather than that of~$\obj$.

\subsection{Lagrangian and augmented Lagrangian of the \glsfmtshort{sqp} subproblem}

In this section, we study the Lagrangian and the augmented Lagrangian of the \gls{sqp} subproblem, and observe their relations with those of the original optimization problem.
We also point out that the augmented Lagrangian of the \gls{sqp} subproblem is exactly the approximate augmented Lagrangian used in~\cite{Niu_Yuan_2010,Wang_Yuan_2014}.

For simplicity, instead of the problem~\cref{eq:problem-cobyqa}, we consider still the problem~\cref{eq:problem-cobyqa-auglag}.
Let~$\iter[k] \ge 0$ and~$\lm[k] \in \R^m$ be given.
Correspondingly, the \gls{sqp} subproblem of the problem~\cref{eq:problem-cobyqa-auglag} is
\begin{subequations}
    \label{eq:sqp-subproblem-auglag}
    \begin{align}
        \min        & \quad \obj(\iter[k]) + \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} H^k \step \label{eq:sqp-subproblem-auglag-obj}\\
        \text{s.t.} & \quad h(\iter[k]) + \nabla h(\iter[k]) \step = 0,\\
                    & \quad \iter[k] + \step \ge 0, ~ \step \in \R^n,
    \end{align}
\end{subequations}
with~$H^k \approx \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.
Note that the constant term~$\obj(\iter[k])$ in~\cref{eq:sqp-subproblem-auglag-obj} may be excluded, as in~\cref{eq:sqp-subproblem-obj}, but including it facilitates the discussion in the sequel.

Recall that the augmented Lagrangian~\cite{Hestenes_1969,Powell_1969,Rockafellar_1973} of the problem~\cref{eq:problem-cobyqa-auglag} is
\begin{equation}
    \label{eq:augmented-lagrangian-inequality}
    \lag[\mathsf{A}](\iter, \lm) \eqdef \lag(\iter, \lm) + \frac{\gamma}{2} \norm{h(\iter)}^2, \quad \text{for~$\iter \ge 0$ and~$\lm \in \R^m$,}
\end{equation}
where~$\gamma \ge 0$ is a penalty parameter.
Denote by~$\lagalt$ the Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag}, i.e.,
\begin{align*}
    \lagalt(\step, \lm) & \eqdef \obj(\iter[k]) + \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} H^k \step\\
                        & \qquad + \lm^{\T} [h(\iter[k]) + \nabla h(\iter[k]) \step], \quad \text{for~$\step \ge -\iter[k]$ and~$\lm \in \R^m$,}
\end{align*}
and by~$\lagalt[\mathsf{A}]$ the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag}, i.e.,
\begin{equation*}
    \lagalt[\mathsf{A}](\step, \lm) \eqdef \lagalt(\step, \lm) + \frac{\gamma}{2} \norm{h(\iter[k]) + \nabla h(\iter[k]) \step}^2, \quad \text{for~$\step \ge -\iter[k]$ and~$\lm \in \R^m$.}
\end{equation*}

We now present some relations between~$\lag$ and~$\lagalt$, and also between~$\lag[\mathsf{A}]$ and~$\lagalt[\mathsf{A}]$.

\begin{theorem}
    \label{thm:auglag-sqp-1}
    Assume that~$\obj$ and~$h$ are twice differentiable.
    If~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$, then~$\lagalt(\step, \lm[k])$ is the second-order Taylor expansion of~$\lag(\iter[k] + \step, \lm[k])$ with respect to~$d$ at~$0$.
\end{theorem}

\begin{proof}
    This theorem can be verified by a straightforward calculation.
\end{proof}

\begin{theorem}
    \label{thm:auglag-sqp-2}
    Assume that~$\obj$ and~$h$ are twice differentiable.
    If
    \begin{equation}
        \label{eq:auglag-sqp-2}
        H^k = \nabla^2 \obj(\iter[k]) + \sum_{i = 1}^m [\lm[k]_i + \gamma h_i(\iter[k])] \nabla^2 h_i(\iter[k]),
    \end{equation}
    then~$\lagalt[\mathsf{A}](\step, \lm[k])$ is the second-order Taylor expansion of~$\lag[\mathsf{A}](\iter[k] + d, \lm[k])$ with respect to~$d$ at~$0$.
\end{theorem}

\begin{proof}
    By direct calculations, we have
    \begin{equation*}
        \nabla_x \lag[\mathsf{A}](\iter[k], \lm[k]) = \nabla_x \lag(\iter[k], \lm[k]) + \gamma \nabla h(\iter[k])^{\T} h(\iter[k]),
    \end{equation*}
    and
    \begin{equation*}
        \nabla_{x, x}^2 \lag[\mathsf{A}](\iter[k], \lm[k]) = \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) + \gamma \bigg[ \nabla h(\iter[k])^{\T} \nabla h(\iter[k]) + \sum_{i = 1}^m h_i(\iter[k]) \nabla^2 h_i(\iter[k]) \bigg].
    \end{equation*}
    Therefore, the second-order Taylor expansion of~$\lag[\mathsf{A}](\iter[k] + d, \lm[k])$ with respect to~$d$ at~$0$ is
    \begin{equation*}
        \lag[\mathsf{A}](\iter[k] + d, \lm[k]) = \lag(\iter[k], \lm[k]) + \nabla_x \lag(\iter[k], \lm[k])^{\T} d + \frac{1}{2} d^{\T} H^k d + \frac{\gamma}{2} \norm{h(\iter[k]) + \nabla h(\iter[k]) d}^2 + \smallo(\norm{d}^2),
    \end{equation*}
    where~$H^k$ is defined by~\cref{eq:auglag-sqp-2}.
\end{proof}

Intriguingly, an augmented Lagrangian method for solving the problem~\cref{eq:problem-cobyqa-auglag} updates traditionally the dual variable~$\lm[k]$ by
\begin{equation*}
    \lm[k + 1] = \lm[k] + \gamma h(\iter[k]).
\end{equation*}
Therefore, the second-order Taylor expansion of~$\lag[\mathsf{A}]$ can be interpreted as the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag} with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k + 1])$.

There is an interesting connection between the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag} and the trust-region augmented Lagrange methods studied in~\cite{Niu_Yuan_2010,Wang_Yuan_2014}.
These methods employ the approximation
\begin{equation}
    \label{eq:niu-yuan-auglag}
    \lag[\mathsf{A}](\iter[k] + \step, \lm[k]) \approx \lag(\iter[k], \lm[k]) + \nabla_x \lag(\iter[k], \lm[k])^{\T} d + \frac{1}{2} d^{\T} H^k d + \frac{\gamma}{2} \norm{h(\iter[k]) + \nabla h(\iter[k]) \step}^2,
\end{equation}
which is a quadratic approximation obtained by replacing~$\lag(\iter[k] + d, \lm[k])$ with a quadratic approximation, and replacing~$h(\iter[k] + d)$ in the penalty term with its first-order Taylor expansion.
This approximation turns out to be the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag}, as shown by \cref{thm:auglag-sqp-3}.

\begin{theorem}
    \label{thm:auglag-sqp-3}
    Assume that~$\obj$ and~$h$ are differentiable.
    For any matrix~$H^k \in \R^{n \times n}$, the right-hand side of~\cref{eq:niu-yuan-auglag} equals~$\lagalt[\mathsf{A}](\step, \lm[k])$.
\end{theorem}

\begin{proof}
    Simarlarly to \cref{thm:auglag-sqp-1}, this theorem can be verified by a straightforward calculation.
\end{proof}

Several possibilities of~$H^k$ are proposed in~\cite{Niu_Yuan_2010,Wang_Yuan_2014}.
For example,~$H^k$ can be set to~$\nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ or an approximation.
As pointed out by~\cite[\S~2.1]{Niu_Yuan_2010}, if~$H^k$ is defined as in~\cref{eq:auglag-sqp-2}, then the right-hand side of~\cref{eq:niu-yuan-auglag} is the second-order Taylor expansion of~$\lag[\mathsf{A}](\iter[k] + d, \lm[k])$ with respect to~$d$ at~$0$, which agrees with \cref{thm:auglag-sqp-2}.
However, in the numerical experiments,~\cite{Niu_Yuan_2010,Wang_Yuan_2014} choose~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.

Suppose that an algorithm defines a step~$\step[k]$ based on the minimization of the right-hand side of~\cref{eq:niu-yuan-auglag}.
Then, \cref{thm:auglag-sqp-3} tells us that the algorithm can be regarded as an \gls{sqp} method that approximately solves the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag} by minimizing~$\lagalt[\mathsf{A}](\step, \lm[k])$, i.e., by applying one single iteration of an augmented Lagrangian method.

% \subsection{A derivative-free \glsfmtlong{sqp} method}
% \label{subsec:derivative-free-sqp}

% At the~$k$th iteration, \gls{cobyqa} builds the models~$\objm[k]$ and~$\conm[k]{i}$ of the objective function~$\obj$ and the constraints functions~$\con{i}$, with~$i \in \iub \cup \ieq$, using the derivative-free Broyden updates presented in~\cref{subsec:symmetric-broyden-updates}.
% A poised interpolation set~$\xpt[k] \subseteq \R^n$ is maintained, whose cardinal number is fixed.
% The only restriction is
% \begin{equation*}
%     n + 2 \le \card(\xpt[k]) \le \frac{1}{2} (n + 1) (n + 2),
% \end{equation*}
% as otherwise, the set~$\xpt[k]$ cannot be poised (see \cref{subsec:symmetric-broyden-updates} for details).
% More details on this interpolation set is given in \cref{subsec:cobyqa-models}.
% We also denote by~$\lagm[k]$ the Lagrangian
% \begin{equation*}
%     \lagm[k](\iter, \lm) \eqdef \objm[k](\iter) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lm_i \conm[k]{i}(\iter), \quad \text{for~$\iter \in \R^n$ and~$\lm_i \in \R$, with~$i \in \iub \cup \ieq$.}
% \end{equation*}
% The derivative-free \gls{sqp} method we consider is presented in \cref{alg:derivative-free-sqp}.

% \begin{algorithm}
%     \caption{Derivative-free \glsfmtshort{sqp} method}
%     \label{alg:derivative-free-sqp}
%     \DontPrintSemicolon
%     \KwData{Initial guess~$\iter[0] \in \R^n$.}
%     Build an initial interpolation set~$\xpt[0] \subseteq \R^n$ with~$\iter[0] \in \xpt[0]$\;
%     \For{$k = 0, 1, \dots$}{
%         Evaluate~$\objm[k]$ and~$\conm[k]{i}$ for~$i \in \iub \cup \ieq$ by underdetermined interpolation on~$\xpt[k]$\;
%         Generate a step~$\step[k] \in \R^n$ by solving approximately
%         \begin{algomathalign*}
%             \min        & \quad \nabla \objm[k](\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lagm[k](\iter[k], \lm[k]) \step\\
%             \text{s.t.} & \quad \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub,\\
%                         & \quad \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
%                         & \quad \step \in \R^n,
%         \end{algomathalign*}
%         Replace a point in~$\xpt[k]$ by~$\iter[k + 1]$ to obtain~$\xpt[k + 1]$ \nllabel{alg:derivative-free-sqp-set}\;
%         Update the iterate~$\iter[k + 1] \gets \iter[k] + \step[k]$\;
%         Estimate the Lagrange multiplier~$\lm[k + 1] = [\lm[k + 1]_i]_{i \in \iub \cup \ieq}^{\T}$\;
%     }
% \end{algorithm}

% Recall that the set~$\xpt[k]$ must be ensured poised.
% Therefore, the point chosen in \cref{alg:derivative-free-sqp-set} must be chosen so that~$\xpt[k + 1]$ is poised.
% It is however possible that such a point does not exist.
% The difficulty of maintaining an appropriate geometry of the interpolation set is discussed in \cref{subsec:geometry-improving-iterations}, and requires to modify \cref{alg:derivative-free-sqp}.
% As we will see, traditionally, geometry-improving steps are incorporating to the method, which modify a point from the interpolation set to improve its geometry.

\section{The trust-region framework}
\label{sec:trust-region}

The \gls{sqp} framework suffers from the major deffect that it is a local method.
Therefore, the convergence of the method cannot be ensured for any initial guess~$\iter[0] \in \R^n$.
To cope with this difficulty, the method is usually embedded in a globalization strategy, such as a line-search or a trust-region framework.
We consider in this thesis trust-region methods only.
Assume that we have a merit function~$\merit[k]$ whose values depend on~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, and a merit function~$\meritm[k]$ whose values depend on the Taylor expansions of~$\obj$ and~$\con{i}$.
% Details on the merit functions employed by \gls{cobyqa} is given in \cref{subsec:cobyqa-merit-function}.
\Cref{alg:derivative-free-trust-region-sqp} presents the trust-region framework.

\begin{algorithm}
    \caption{Derivative-free trust-region \glsfmtshort{sqp} method}
    \label{alg:derivative-free-trust-region-sqp}
    \DontPrintSemicolon
    \KwData{Initial guess~$\iter[0] \in \R^n$ and initial trust-region radius~$\rad[0] > 0$.}
    Build an initial interpolation set~$\xpt[0] \subseteq \R^n$ with~$\iter[0] \in \xpt[0]$\;
    \For{$k = 0, 1, \dots$}{
        Evaluate~$\objm[k]$ and~$\conm[k]{i}$ for~$i \in \iub \cup \ieq$ by underdetermined interpolation on~$\xpt[k]$\;
        Generate a step~$\step[k] \in \R^n$ by solving approximately
        \begin{subequations}
            \label{eq:trust-region-sqp-subproblem}
            \begin{algomathalign}
                \min        & \quad \nabla \objm[k](\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lagm[k](\iter[k], \lm[k]) \step \label{eq:derivative-free-trust-region-sqp-subproblem-obj}\\
                \text{s.t.} & \quad \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub, \label{eq:derivative-free-trust-region-sqp-subproblem-ub}\\
                            & \quad \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq, \label{eq:derivative-free-trust-region-sqp-subproblem-eq}\\
                            & \quad \norm{\step} \le \rad[k], \label{eq:derivative-free-trust-region-sqp-subproblem-tr}\\
                            & \quad \step \in \R^n, \nonumber
            \end{algomathalign}
        \end{subequations} \nllabel{alg:derivative-free-sqp-tr-step}
        Evaluate the trust-region ratio
        \begin{algomathdisplay}
            \ratio[k] \gets \frac{\merit[k](\iter[k]) - \merit[k](\iter[k] + \step[k])}{\meritm[k](\iter[k]) - \meritm[k](\iter[k] + \step[k])}
        \end{algomathdisplay} \nllabel{alg:derivative-free-trust-region-sqp-ratio}
        \eIf{$\ratio[k] \ge 0$}{
            Choose a point~$\bar{y} \in \xpt[k]$ to remove from~$\xpt[k]$\;
        }{
            Choose a point~$\bar{y} \in \xpt[k] \setminus \set{\iter[k]}$ to remove from~$\xpt[k]$\;
        }
        Update the interpolation set~$\xpt[k + 1] \gets (\xpt[k] \setminus \set{\bar{y}}) \cup \set{\iter[k] + \step[k]}$\;
        Update the current iterate~$\iter[k + 1] \gets \argmin \set{\merit[k](y)}_{y \in \xpt[k + 1]}$\;
        Calculate~$\rad[k + 1]$ according to the values of~$\ratio[k]$ and~$\rad[k]$ \nllabel{alg:derivative-free-trust-region-sqp-radius}\;
    }
\end{algorithm}

Details on the update of the trust-region radius in \cref{alg:derivative-free-trust-region-sqp-radius} is provided in \cref{subsec:trust-region-radius}.
Once again, this framework presents a very simplified version of the derivative-free trust-region \gls{sqp} framework.
It must be modified to address the difficulties related to the geometry of the interpolation set.
This is taken into account in the framework presented in \cref{subsec:geometry-improving-iterations}.

The trust-region ratio in \cref{alg:derivative-free-trust-region-sqp-ratio} is a scalar that represents the performance of the models.
Therefore, the closer from one~$\ratio[k]$ is, the best the~$k$th models perform.
Moreover, as long as it nonnegative, the trial step~$\step[k]$ does not provide any increase in the merit function~$\merit[k]$.

If we were naively adapting the trust-region framework for unconstrained optimization to our setting, the choice of~$\iter[k + 1]$ would be either~$\iter[k] + \step[k]$ if~$\ratio[k] \ge 0$, and~$\iter[k]$ otherwise.
However, this would not take into account the fact that~$\merit[k]$ might differ from~$\merit[k - 1]$.
Therefore,~$\iter[k + 1]$ might not be the best point in~$\xpt[k + 1]$ according to~$\merit[k]$.
In \cref{alg:derivative-free-trust-region-sqp} however,~$\iter[k + 1]$ is always the best point in~$\xpt[k + 1]$.
Moreover, note that~$\xpt[k + 1]$ necessarily contains~$\iter[k] + \step[k]$, and also contain~$\iter[k]$ if~$\ratio[k] < 0$.
Thus, our adaptation of the trust-region framework for unconstrained optimization is reasonable.

\subsection{Managing the trust-region radius}
\label{subsec:trust-region-radius}

Inspired by the performance of \gls{uobyqa}, \gls{newuoa}, \gls{bobyqa}, and \gls{lincoa} (see \cref{subsec:uobyqa,subsec:newuoa-bobyqa-lincoa}), we employ the following paradigm for managing the trust-region radius, proposed by Powell~\cite{Powell_2002,Powell_2006,Powell_2009}.
It consists in maintaining both the trust-region radius~$\rad[k] > 0$ and a lower-bound of it~$\radlb[k] > 0$.
The idea behind this technique is to use~$\rad[k]$ as the trust-region radius in the trust-region subproblem~\cref{eq:trust-region-sqp-subproblem}, and~$\radlb[k]$ to maintain an adequate distance between the points in~$\xpt[k]$.
Further, the method never increases~$\radlb[k]$, but adapt the value of~$\rad[k]$ in a typical trust-region way.
Of course, the method always ensures that~$\rad[k] \ge \radlb[k]$.
As noted by \citeauthor{Powell_2002}, allowing trial step to have a length larger than~$\radlb[k]$ prevent loss in efficiency that occurred otherwise in his software \gls{uobyqa}.

The update of the trust-region radius~$\rad[k]$ is given in \cref{alg:update-trust-region-radius}.
The parameters chosen in \gls{cobyqa} are~$\eta_1 = 0.1$,~$\eta_2 = 0.7$,~$\eta_3 = 1.4$,~$\theta_1 = 0.5$, and~$\theta_2 = \sqrt{2}$.
This update is untertained at each iteration.

\begin{algorithm}
    \caption{Updating the trust-region radius}
    \label{alg:update-trust-region-radius}
    \DontPrintSemicolon
    \KwData{Current lower bound on the trust-region radius~$\radlb[k] > 0$, current trust-region radius~$\rad[k] \ge \radlb[k]$, current trust-region ratio~$\ratio[k] \in \R$, current trial step~$\step \in \R^n$, and parameters~$0 < \eta_1 \le \eta_2 < 1 \le \eta_3$ and~$0 < \theta_1 < 1 < \theta_2$.}
    \KwResult{Updated trust-region radius~$\rad[k + 1]$.}
    Set the value of~$\rad[k + 1]$ to
    \begin{algoempheq}[left={\rad[k + 1] \gets \empheqlbrace}]{alignat*=2}
        & \theta_1 \rad[k]                                                                      && \quad \text{if~$\ratio[k] \le \eta_1$,}\\
        & \min \set{\theta_1 \rad[k], \norm{\step}}                                               && \quad \text{if~$\eta_1 < \ratio[k] \le \eta_2$,}\\
        & \min \set{\theta_2 \rad[k], \max \set{\theta_1 \rad[k], \theta_1^{-1} \norm{\step}}}    && \quad \text{otherwise}
    \end{algoempheq}
    \If{$\rad[k + 1] \le \eta_3 \radlb[k]$}{
        $\rad[k + 1] \gets \radlb[k]$\;
    }
\end{algorithm}

As we mentioned already, the method maintains~$\rad[k] \ge \radlb[k]$ and it never increases~$\radlb[k]$.
Therefore, the value of~$\radlb[k]$ is decreased only if~$\rad[k] = \radlb[k]$.
Moreover, since~$\radlb[k]$ is designed to maintain a good distance between the interpolation points, it must be decreased only if the performance of the models is poor.
Hence, it is decreased only if the trial step~$\norm{\step[k]}$ is small compared with~$\rad[k]$ and the trust-region ratio~$\ratio[k]$ is small.
\Cref{alg:reducing-lower-bound-trust-region-radius} presents the method employed by \gls{cobyqa} to reduce~$\radlb[k]$.
The parameters chosen in \gls{cobyqa} are~$\eta_4 = 16$,~$\eta_5 = 250$, and~$\theta_3 = 0.1$.

\begin{algorithm}
    \caption{Reducing the lower bound on the trust-region radius}
    \label{alg:reducing-lower-bound-trust-region-radius}
    \DontPrintSemicolon
    \KwData{Final trust-region radius~$\radlb[\infty] > 0$, current lower bound on the trust-region radius~$\radlb[k] \ge \radlb[\infty]$, updated trust-region radius~$\rad[k + 1] \ge \radlb[k]$, and parameters~$1 \le \eta_4 < \eta_5$ and~$0 < \theta_3 < 1$.}
    \KwResult{Reduced lower bound on trust-region radius~$\radlb[k + 1]$ and modified trust-region radius~$\rad[k + 1]$.}
    \If{$\radlb[k] = \radlb[\infty]$}{
        Terminate the optimization method\;
    }
    Set the value of~$\radlb[k + 1]$ to
    \begin{algoempheq}[left={\radlb[k + 1] \gets \empheqlbrace}]{alignat*=2}
        & \theta_3 \radlb[k]                && \quad \text{if~$\eta_5 < \radlb[k] / \radlb[\infty]$,}\\
        & \sqrt{\radlb[k] \radlb[\infty]}   && \quad \text{if~$\eta_4 < \radlb[k] / \radlb[\infty] \le \eta_5$,}\\
        & \radlb[\infty]                    && \quad \text{otherwise}
    \end{algoempheq}
    $\rad[k + 1] \gets \max \set{\rad[k + 1], \radlb[k + 1]}$\;
\end{algorithm}

\subsection{Composite-step approach}

The \cref{alg:derivative-free-sqp-tr-step} of \cref{alg:derivative-free-sqp} hides the following crucial point.
If the trust-region subproblem~\cref{eq:trust-region-sqp-subproblem} is infeasible, what should~$\step[k]$ approximate?
Since the constraints~\cref{eq:derivative-free-trust-region-sqp-subproblem-ub,eq:derivative-free-trust-region-sqp-subproblem-eq} are linear representation of the constraints~\cref{eq:problem-cobyqa-ub,eq:problem-cobyqa-eq}, we cannot guarantee the feasible region of the subproblem~\cref{eq:trust-region-sqp-subproblem} to be feasible.
It may be infeasible because
\begin{enumerate}
    \item the inequality constraints~\cref{eq:derivative-free-trust-region-sqp-subproblem-ub} may be infeasible,
    \item the equality constraints~\cref{eq:derivative-free-trust-region-sqp-subproblem-eq} may be infeasible, and
    \item the intersection of the regions defined by~\cref{eq:derivative-free-trust-region-sqp-subproblem-ub,eq:derivative-free-trust-region-sqp-subproblem-eq,eq:derivative-free-trust-region-sqp-subproblem-tr} may be empty.
\end{enumerate}

The traditional approach to cope with this difficulty is to define~$\step[k]$ as the sum of two steps, a \emph{normal step}~$\nstep[k]$ that aims at reducing the constraint violation and a \emph{tangential step}~$\tstep[k]$ that aims at reducing the objective function~\cref{eq:derivative-free-trust-region-sqp-subproblem-obj}.
Note that the terms \enquote{normal} and \enquote{tangential} only describe the step properties, there will be no actual reason for these steps to be orthogonal.
In what follows, we describe three approaches for defining these composite steps, namely the Vardi-like approach~\cite{Vardi_1985}, the Byrd-Omojokun-like approach~\cite{Byrd_1987,Omojokun_1989}, and the \gls{cdt} approach~\cite{Celis_Dennis_Tapia_1985}.

\subsubsection{Vardi-like approach}

The Vardi-like approach to define the normal and tangential steps is perhaps the simplest one.
It is very similar to an idea proposed by Powell~\cite[Eqs.~(2.7) and~(2.8)]{Powell_1978a}.
It consists in replacing the constraints~\cref{eq:derivative-free-trust-region-sqp-subproblem-ub,eq:derivative-free-trust-region-sqp-subproblem-eq} of the trust-region \gls{sqp} subproblem by
\begin{subequations}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \alpha \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step = 0,   && \quad \text{if~$i \in \ieq$,} \label{eq:vardi-eq}\\
        & \alpha \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step \le 0, && \quad \text{if~$i \in \iub$ and~$\conm[k]{i}(\iter[k]) \ge 0$,} \label{eq:vardi-ub1}\\
        & \conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step \le 0,        && \quad \text{if~$i \in \iub$ and~$\conm[k]{i}(\iter[k]) < 0$,} \label{eq:vardi-ub2}
    \end{empheq}
\end{subequations}
for some~$\alpha \in [0, 1]$.
The reason for doing so is that if~$\alpha = 0$, then the trust-region subproblem is feasible, because all vectors lying in the null space of~$[\nabla \conm[k]{i}(\iter[k])]_{i \in \iub \cup \ieq}^{\T}$ satisfy the inequality constraints~\cref{eq:vardi-ub1,eq:vardi-ub2}, the equality constraints~\cref{eq:vardi-eq}, and the trust-region constraint~\cref{eq:derivative-free-trust-region-sqp-subproblem-tr}.
The relaxation of the equality constraints~\cref{eq:vardi-eq} is due to \citeauthor{Vardi_1985}~\cite{Vardi_1985}, but the treatment of the relaxed inequality constraints~\cref{eq:vardi-ub1,eq:vardi-ub2} is adapted from \citeauthor{Powell_1978a}~\cite {Powell_1978a}.

Let~$\bar{\alpha}$ be the largest~$\alpha \in [0, 1]$ such that the trust-region subproblem is feasible, and let~$\nstep[c]$ be the least-squares solution to
\begin{align*}
    \min        & \quad \sum_{i \in \iub} [\conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step]_+^2 + \sum_{i \in \ieq} [\conm[k]{i}(\iter[k]) + \nabla \conm[k]{i}(\iter[k])^{\T} \step]^2\\
    \text{s.t.} & \quad \step \in \R^n,
\end{align*}
where~$[\cdot]_+$ denotes the componentwise positive-part operator.
Then,~$\nstep[k] = \bar{\alpha} \nstep[c]$ is the closest point to the constraints that satisfy the trust-region constraint.
However, choosing such a normal step is not viable, for the following reason.
If~$\bar{\alpha} < 1$, then~$\nstep[k]$ is the only point in the relaxed feasible region. 
Therefore, the tangential step~$\tstep[k]$ must be set to zero, since it should not worsen the infeasibility of~$\step[k] = \nstep[k] + \tstep[k]$.
To cope with this defect, if~$\bar{\alpha} < 1$, the normal step should be~$\nstep[k] = \alpha \nstep[c]$ with~$\alpha < \bar{\alpha}$, and~$\nstep[k] = 0$ otherwise.

Now that~$\nstep[k]$ has been found, we turn our attention to the definition of the tangential step~$\tstep[k]$.
Recall that it should decrease the objective function of the trust-region \gls{sqp} subproblem~\cref{eq:derivative-free-trust-region-sqp-subproblem-obj} without worsening the possible constraint infeasibility.
Therefore, the composite step~$\step[k]$ is an approximate solution to
\begin{align*}
    \min        & \quad \nabla \objm[k](\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lagm[k](\iter[k], \lm[k]) \step\\
    \text{s.t.} & \quad \nabla \conm[k]{i}(\iter[k])^{\T} \step \le \max \set{-\conm[k]{i}(\iter[k]), \nabla \conm[k]{i}(\iter[k])^{\T} \nstep[k]}, ~ i \in \iub,\\
                & \quad \nabla \conm[k]{i}(\iter[k])^{\T} \step = \nabla \conm[k]{i}(\iter[k])^{\T} \nstep[k], ~ i \in \ieq,\\
                & \quad \norm{\step} \le \rad[k],\\
                & \quad \step \in \R^n,
\end{align*}
where\todo{Check that the constraints are ok} the~$\max$ operator is understood componentwise.

\subsubsection{Byrd-Omojokun-like approach}

\subsubsection{\Glsfmtshort{cdt} approach}

\subsection{Geometry-improving iterations}
\label{subsec:geometry-improving-iterations}
