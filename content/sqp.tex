%% contents/sqp.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{The \glsfmtlong{sqp} (\glsfmtshort{sqp}) method}
\label{ch:sqp}

\section{The method}

This chapter presents the basic idea behind the \gls{sqp} method.
For this discussion, we assume that the derivatives of the objective and constraint functions are available, as is the case in the classical \gls{sqp} method.
% However, we will extend the method to the derivative-free context in \cref{subsec:derivative-free-sqp}.

% Note that the problem~\cref{eq:problem-cobyqa} formulates the bound constraints~\cref{eq:problem-cobyqa-bd} explicitely.
% This is important in computation, because it may not be reasonable to handle the bounds in the same way as the others due to their different natures (see \cref{subsec:bound-constraints} for details).
% However, in the theoretical development of this section and the next one (\cref{sec:trust-region}), it is not necessary to distinguish bound constraints from the others.
Throughout this chapter, we consider a problem of the form
\begin{subequations}
    \label{eq:problem-cobyqa-sqp}
    \begin{align}
        \min        & \quad \obj(\iter) \label{eq:problem-cobyqa-sqp-obj}\\
        \text{s.t.} & \quad \con{i}(\iter) \le 0, ~ i \in \iub, \label{eq:problem-cobyqa-sqp-ub}\\
                    & \quad \con{i}(\iter) = 0, ~ i \in \ieq, \label{eq:problem-cobyqa-sqp-eq}\\
                    & \quad \iter \in \R^n. \nonumber
    \end{align}
\end{subequations}
% where~$\iub$ contains the bound constraints, if any.

% \begin{remark}
%     Comparing~\cref{eq:problem-cobyqa} with~\cref{eq:problem-cobyqa-sqp}, we are abusing the notations, because~$\iub$ does not represent the same set of constraints in both.
%     However, this will not generate any confusion in our discussions.
% \end{remark}

Note that the problem~\cref{eq:problem-cobyqa-sqp} is precisely the problem~\cref{eq:problem-introduction} discussed in \cref{ch:introduction}; hence, all the theory mentioned is applicable.
For our later discussion, recall in particular that the Lagrangian of this problem is defined by
\begin{equation*}
    \lag(\iter, \lm) \eqdef \obj(\iter) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lm_i \con{i}(\iter), \quad \text{for~$\iter \in \R^n$ and~$\lm_i \in \R$, with~$i \in \iub \cup \ieq$,}
\end{equation*}
where~$\lm = [\lm_i]_{i \in \iub \cup \ieq}^{\T}$ is the dual variable of the considered problem.

\subsection{Overview of the method}

The \gls{sqp} method is known to be one of the most powerful methods for solving the problem~\cref{eq:problem-cobyqa-sqp} when derivatives of~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are available.
The classical \gls{sqp} framework is presented in \cref{alg:sqp}.

\begin{algorithm}
    \caption{Classical \glsxtrshort{sqp} method}
    \label{alg:sqp}
    \DontPrintSemicolon
    \KwData{Initial guess~$\iter[0] \in \R^n$ and estimated Lagrange multiplier~$\lm[0] = [\lm[0]_i]_{i \in \iub \cup \ieq}^{\T}$.}
    \For{$k = 0, 1, \dots$}{
        Define~$H^k \approx \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$\;
        Generate a step~$\step[k] \in \R^n$ by solving approximately
        \begin{subequations}
            \label{eq:sqp-subproblem}
            \begin{algomathalign}
                \min        & \quad \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} H^k \step \label{eq:sqp-subproblem-obj}\\
                \text{s.t.} & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub,\\
                            & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
                            & \quad \step \in \R^n, \nonumber
            \end{algomathalign}
        \end{subequations}
        Update the iterate~$\iter[k + 1] \gets \iter[k] + \step[k]$\;
        Estimate the Lagrange multiplier~$\lm[k + 1] = [\lm[k + 1]_i]_{i \in \iub \cup \ieq}^{\T}$\;
    }
\end{algorithm}

The earliest reference to such a method appeared in the Ph.D. thesis of \citeauthor{Wilson_1963}~\cite{Wilson_1963}, with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.
\Citeauthor{Robinson_1974}~\cite{Robinson_1974} showed the local R-quadratic convergence rate of this method.
% Check for Q-quadratic convergence.
Later, \citeauthor{Garcia-Palomares_Mangasarian_1976}~\cite{Garcia-Palomares_1973,Garcia-Palomares_Mangasarian_1976} modified it using a quasi-Newton update for calculating~$H^k$ and established a local R-superlinear convergence rate for such an algorithm.
A similar method was introduced by \citeauthor{Han_1976}~\cite{Han_1976,Han_1977}, but he only approximated~$\nabla_{x, x}^2 \lag(\iter[k], \lm[k])$, while \citeauthor{Garcia-Palomares_Mangasarian_1976} applied quasi-Newton approximations to the whole matrix~$\nabla^2 \lag(\iter[k], \lm[k])$.
In addition, \citeauthor{Han_1976} introduced a line-search strategy to guarantee the global convergence and local Q-superlinear convergence rate, requiring that~$\nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast])$ is positive definite at the solution~$(\iter[\ast], \lm[\ast])$.
\Citeauthor{Powell_1978a}~\cite{Powell_1978b,Powell_1978a,Powell_1978c} studied the method in the same direction.
In particular, he proposed to apply the damped BFGS quasi-Newton formula~\cite[Eqs.~(5.8),~(5.9), and~(5.10)]{Powell_1978b} to update~$H^k$.
This formula guarantees the positive definiteness of such a matrix, which is beneficial in practice and theory (see the comments towards the end of~\cite[\S~2]{Powell_1978a}).
Moreover, he introduced a practical line-search technique based on a merit function suggested by \citeauthor{Han_1976}~\cite{Han_1976}.
Furthermore, \citeauthor{Powell_1978c} established the global convergence and the local R-superlinear convergence rate for his method without requiring the positive definiteness of~$\nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast])$ as \citeauthor{Han_1976} did.
Recognizing the contributions of \citeauthor{Wilson_1963}, \Citeauthor{Han_1976}, and \citeauthor{Powell_1978a}, the \gls{sqp} method is also referred to as the Wilson-Han-Powell method~\cite{Schittkowski_1981,Burke_1992}.
See~\cite{Boggs_Tolle_1995} for a more detailed review of the history, theory, and practice of the \gls{sqp} method.

\subsection{A simple example}
\label{subsec:sqp-simple-example}

In \cref{alg:sqp}, it is crucial that~$H^k$ approximates~$\nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.
It may be tempting to set~$H^k \approx \nabla \obj(\iter[k])$, because the objective function of the \gls{sqp} subproblem would then be a local quadratic approximation of~$\obj$ at~$\iter[k]$.
However, such a naive idea does not work, as illustrated by the following~$2$-dimensional example inspired by \citeauthor{Boggs_Tolle_1995}~\cite[\S~2.2]{Boggs_Tolle_1995}.

We consider
\begin{align*}
    \min        & \quad -\iter_1 - \frac{(\iter_2)^2}{4}\\
    \text{s.t.} & \quad \norm{\iter}^2 - 1 = 0,\\
                & \quad \iter \in \R^2,
\end{align*}
whose solution is~$\iter[\ast] = [1, 0]^{\T}$ with the associated Lagrange multiplier~$\lm[\ast] = 1/2$.
Suppose that we have an iterate~$\iter[k] = [t, 0]^{\T}$ with~$t \approx 1$, so it is already close to the solution.
If~$H^k = \nabla^2 \obj(\iter[k])$, then the \gls{sqp} subproblem would become
\begin{subequations}
    \begin{align}
        \min        & \quad -\step_1 - \frac{(\step_2)^2}{4} \label{eq:boggs-tolle-sp-obj}\\
        \text{s.t.} & \quad \step_1 = \frac{1 - t^2}{2 t}, \label{eq:boggs-tolle-sp-eq}\\
                    & \quad \step \in \R^2. \nonumber
    \end{align}
\end{subequations}
This subproblem is unbounded from below, regardless of the value of~$t$.
In addition, the more~$\step[k]$ reduces~\cref{eq:boggs-tolle-sp-obj}, the larger~$\norm{\iter[k] + \step[k] - \iter[\ast]}$ is.
Moreover, if~$t = 1$, we have~$\iter[k] = \iter[\ast]$, but any feasible point~$\step[k]$ for~\cref{eq:boggs-tolle-sp-eq} will push~$\iter[k] + \step[k]$ away from~$\iter[\ast]$, unless~$\step[k]$ is the global maximizer of~\cref{eq:boggs-tolle-sp-obj} subject to~\cref{eq:boggs-tolle-sp-eq}.

Let us now consider the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ for a dual variable~$\lm[k] \approx \lm[\ast] = 1/2$.
It is
\begin{align*}
    \min        & \quad -\step_1 + \lm[k] (\step_1)^2 + \bigg( \lm[k] - \frac{1}{4} \bigg) (\step_2)^2\\
    \text{s.t.} & \quad \step_1 = \frac{1 - t^2}{2 t},\\
                & \quad \step \in \R^2. \nonumber
\end{align*}
When~$\lm[k] > 1/4$, the solution to this subproblem is
\begin{equation*}
    \step[k] =
    \begin{bmatrix}
        \dfrac{1 - t^2}{2 t}    & 0
    \end{bmatrix}^{\T}.
\end{equation*}
We thus have
\begin{equation*}
    \iter[k] + \step[k] = 
    \begin{bmatrix}
        \dfrac{t^2 + 1}{2 t}  & 0
    \end{bmatrix}^{\T}.
\end{equation*}
If we set~$\iter[k + 1] = \iter[k] + \step[k]$ and continue to iterate in this way, we will obtain a sequence of iterates that converges quadratically to~$\iter[\ast]$, because
\begin{equation*}
    \norm{\iter[k] + \step[k] - \iter[\ast]} = \frac{(1 - t)^2}{2 \abs{t}} = \bigo(\norm{\iter[k] - \iter[\ast]}^2).
\end{equation*}
This is not surprising, since \citeauthor{Robinson_1974}~\cite{Robinson_1974} showed the local R-quadratic convergence rate of the \gls{sqp} method when~$H^k$ is the exact Hessian matrix of the Lagrangian with respect to~$\iter$.

To summarize, as indicated by this example, choosing~$H^k \approx \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ instead of~$H^k \approx \nabla \obj(\iter[k])$ in~\cref{eq:sqp-subproblem} is crucial.

\subsection{Interpretation of the \glsfmtshort{sqp} subproblem}
\label{subsec:sqp-interpretation}

To get some insight into the origin of the \gls{sqp} method, we interpret the \gls{sqp} subproblem~\cref{eq:sqp-subproblem}.
In what follows, we focus on only one iteration of \cref{alg:sqp} and hence,~$k$ is fixed.
We will explain why it is reasonable to update~$\iter[k]$ by a solution to~\cref{eq:sqp-subproblem}.

\subsubsection{Bilinear approximation of the \glsfmtlong{kkt} conditions}

This is the most classical interpretation of the \gls{sqp} subproblem.
According to \cref{thm:first-order-necessary-conditions}, if~$\iter[\ast] \in \R^n$ is a local solution to the problem~\cref{eq:problem-cobyqa-sqp}, under some mild assumptions, there exists a Lagrange multiplier~$\lm[\ast] = [\lm[\ast]_i]_{i \in \iub \cup \ieq}^{\T}$ with~$\lm[\ast]_i \in \R$ for all~$i \in \iub \cup \ieq$ such that
\begin{subequations}
    \label{eq:sqp-kkt}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \nabla_x \lag(\iter[\ast], \lm[\ast]) = 0,    && \\
        & \con{i}(\iter[\ast]) \le 0,                   && \quad \text{if~$i \in \iub$,}\\
        & \con{i}(\iter[\ast]) = 0,                     && \quad \text{if~$i \in \ieq$,}\\
        & \lm[\ast]_i \con{i}(\iter[\ast]) = 0,         && \quad \text{if~$i \in \iub$,} \label{eq:sqp-kkt-complementary-slackness}\\
        & \lm[\ast]_i \ge 0,                            && \quad \text{if~$i \in \iub$.}
    \end{empheq}
\end{subequations}
Regard~\cref{eq:sqp-kkt} as a nonlinear system of inequalities and equalities, and~$(\iter[k], \lm[k])$ as an approximation of~$(\iter[\ast], \lm[\ast])$.
If we want to solve this system by the Newton-Raphson method\footnote{Discussions are needed on how to apply the Newton-Raphson method to systems of nonlinear inequalities and equalities. We will not go further in this direction but refer to \cite{Pshenichnyi_1970a,Pshenichnyi_1970b,Robinson_1972b,Daniel_1973} for fundamental works on this topic.} starting from~$(\iter[k], \lm[k])$, we would seek a step~$(\step, \mu)$ that satisfies the system
\begin{subequations}
    \label{eq:sqp-kkt-linearization}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \nabla_x \lag(\iter[k], \lm[k] + \mu) + \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step = 0,         && \\
        & \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0,                                    && \quad \text{if~$i \in \iub$,}\\
        & \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0,                                      && \quad \text{if~$i \in \ieq$,}\\
        & \lm[k]_i [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step] + \mu_i \con{i}(\iter[k]) = 0, && \quad \text{if~$i \in \iub$,} \label{eq:sqp-kkt-linearization-complementary-slackness}\\
        & \lm[k]_i + \mu_i \ge 0,                                                                           && \quad \text{if~$i \in \iub$,}
    \end{empheq}
\end{subequations}
which is a linear approximation of~\cref{eq:sqp-kkt} at~$(\iter[k], \lm[k])$.
However, as pointed out by \citeauthor{Robinson_1972a}~\cite[Rem.~3]{Robinson_1972a}, an objection to such a method is that it would not solve even a linear program in one iteration.
To cope with this deffect, we let~$(d, \mu)$ solve instead the following bilinear approximation of~\cref{eq:sqp-kkt},
\begin{subequations}
    \label{eq:sqp-subproblem-kkt}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \nabla_x \lag(\iter[k], \lm[k] + \mu) + \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step = 0, && \\
        & \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0,                            && \quad \text{if~$i \in \iub$,}\\
        & \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0,                              && \quad \text{if~$i \in \ieq$,}\\
        & (\lm[k]_i + \mu_i) [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step] = 0,         && \quad \text{if~$i \in \iub$,} \label{eq:sqp-subproblem-kkt-complementary-slackness}\\
        & \lm[k]_i + \mu_i \ge 0,                                                                   && \quad \text{if~$i \in \iub$.}
    \end{empheq}
\end{subequations}
Its only difference from the system~\cref{eq:sqp-kkt-linearization} lies in the condition~\cref{eq:sqp-subproblem-kkt-complementary-slackness}, which includes the bilinear term~$\mu_i \nabla \con{i}(\iter[k])^{\T} \step$.
If the problem~\cref{eq:problem-cobyqa-sqp} is a linear program, then~\cref{eq:sqp-subproblem-kkt} is precisely its \gls{kkt} system, while~\cref{eq:sqp-kkt-linearization} is only an approximation.
Observe that the bilinear system~\cref{eq:sqp-subproblem-kkt} is nothing but the \gls{kkt} conditions of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem}, with~$\lm[k] + \mu$ being the Lagrange multiplier.
Therefore, a \gls{kkt} pair for the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} is similar to a Newton-Raphson step for the \gls{kkt} system of the problem~\cref{eq:problem-cobyqa-sqp}, and it is even better in the sense that the resulting method solves a linear program in one iteration.

Note that discrepancy between the systems~\cref{eq:sqp-kkt-linearization,eq:sqp-subproblem-kkt} disappears if~$\iub = \emptyset$ in the problem~\cref{eq:problem-cobyqa-sqp} and hence, a \gls{kkt} pair for the \gls{sqp} subproblem~\cref{eq:sqp-subproblem} is exactly a Newton-Raphson step for the \gls{kkt} system of the problem~\cref{eq:problem-cobyqa-sqp} in such a situation.

\subsubsection{Approximation of a modified Lagrangian}

This interpretation is due to \citeauthor{Robinson_1972a}~\cite[Rem.~4]{Robinson_1972a}.
Let~$\widetilde{\lag}$ be the function
\begin{equation*}
    \widetilde{\lag}(\iter, \lm) \eqdef \obj(\iter) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lm_i \delta_i(\iter), \quad \text{for~$\iter \in \R^n$ and~$\lm_i \in \R$, with~$i \in \iub \cup \ieq$},
\end{equation*}
where~$\delta_i$, for~$i \in \iub \cup \ieq$, is defined by
\begin{equation*}
    \delta_i(\iter) \eqdef \con{i}(\iter) - \con{i}(\iter[k]) - \nabla \con{i}(\iter[k])^{\T} (\iter - \iter[k]), \quad \text{for~$\iter \in \R^n$.}
\end{equation*}
The function~$\delta_i$ is referred to as the departure from linearity\footnote{When~$\con{i}$ is strictly convex,~$\delta_i$ defines the Bregman distance~\cite{Bregman_1967} associated with~$\con{i}$.} for~$\con{i}$ at the point~$\iter[k]$~\cite[\S~2]{Gill_Wong_2011}.
The \gls{sqp} subproblem~\cref{eq:sqp-subproblem} with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ can then be seen as the minimization of the second-order Taylor approximation of~$\widetilde{\lag}$ subject to the linear approximations of the constraints~\cref{eq:problem-cobyqa-sqp-ub,eq:problem-cobyqa-sqp-eq} at~$(\iter[k], \lm[k])$, i.e.,
\begin{align*}
    \min        & \quad \nabla_x \widetilde{\lag}(\iter[k], \lm[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \widetilde{\lag}(\iter[k], \lm[k]) \step\\
    \text{s.t.} & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub,\\
                & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
                & \quad \step \in \R^n.
\end{align*}

By expressing its subproblem in this form, we observe that the \gls{sqp} method is a special case of \citeauthor{Robinson_1972a}'s method~\cite{Robinson_1972a}, known to have a local R-quadratic convergence rate.

\subsubsection{Approximation of the objective function in the tangent space of the feasible set}

Inspired by an observation in~\cite[\S~2]{Gill_Wong_2011}, we can also interpret the \gls{sqp} subproblem as minimizing an approximation of the objective function in the tangent space of the feasible set.
As will be shown in \cref{thm:sqp-path}, when approximating~$\obj$ in such a space, we will naturally get the Hessian matrix of the Lagrangian in the second-order term.

For this interpretation, we consider the problem
\begin{subequations}
    \label{eq:problem-cobyqa-auglag}
    \begin{align}
        \min        & \quad \obj(\iter)\\
        \text{s.t.} & \quad h(\iter) = 0,\\
                    & \quad \iter \ge 0, ~ \iter \in \R^n, \nonumber
    \end{align}
\end{subequations}
with~$h : \R^n \to \R^m$.
The problem~\cref{eq:problem-cobyqa-sqp} can be reformulated in this form\footnote{In the reformulation, the dimension and the meaning of~$\iter$ may be altered, but we do not change the notations since it does not lead to confusion}.
Recall that the Lagrangian of~\cref{eq:problem-cobyqa-auglag} is
\begin{equation*}
    \lag(\iter, \lm) \eqdef \obj(\iter) + \lm^{\T} h(\iter), \quad \text{for~$\iter \ge 0$ and~$\lm \in \R^m$.}
\end{equation*}

Let~$\bar{\iter} \in \R^n$,~$\bar{\lm} \in \R^m$ be given, and define
\begin{equation*}
    Q(\step) \eqdef \obj(\bar{\iter}) + \nabla \obj(\bar{\iter})^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lag(\bar{\iter}, \bar{\lm}) \step.
\end{equation*}
If~$\bar{\iter}$ and~$\bar{\lm}$ represent the current iterate and approximate Lagrange multiplier, then~$Q$ is the objective function of the \gls{sqp} subproblem with the exact second-order term.
Therefore, the \gls{sqp} subproblem approximates~$\obj$ by~$Q$ and the feasible set by its tangent space.
Such an approximation is reasonable only if~$Q$ approximates~$\obj$ in this tangent space, which turns out to be true, as detailed by \cref{thm:sqp-path}.

\begin{theorem}
    \label{thm:sqp-path}
    Assume that~$\obj$ and~$h$ are twice differentiable and that~$\nabla^2 \obj$ is Lipschitz continuous in a neighborhood of~$\bar{\iter}$.
    Let~$\iter(t)$ be a feasible path starting at~$\bar{\iter}$ and parametrized by a nonnegative scalar~$t$, i.e.,~$h(\iter(t)) = 0$ and~$\iter(t) \ge 0$ for~$t \ge 0$, and~$\iter(0) = \bar{\iter}$.
    Assume that~$\iter$ is twice differentiable for~$t \ge 0$ and that~$\iter''$ is Lipschitz continuous in a neighborhood of~$0$.
    Then, there exist constants~$\nu \ge 0$ and~$\epsilon > 0$ such that
    \begin{equation*}
        \abs{\obj(\iter(t)) - Q(x'(0) t)} \le \bigg( \nu t + \frac{1}{2}\abs{\iter''(0)^{\T} [\nabla \obj(\bar{\iter}) + \nabla h(\bar{\iter})^{\T} \bar{\lm}]} \bigg) t^2 \quad \text{for all~$t \in [0, \epsilon]$.}
    \end{equation*}
\end{theorem}

\begin{proof}
    Define~$\phi(t) = \obj(x(t))$ for~$t \ge 0$.
    For any~$t \ge 0$, we have
    \begin{subequations}
        \label{eq:sqp-path-proof-1}
        \begin{empheq}[left=\empheqlbrace]{alignat=1}
            & \phi'(t) = \iter'(t)^{\T} \nabla \obj(\iter(t)),\\
            & \phi''(t) = \iter'(t)^{\T} \nabla^2 \obj(\iter(t)) \iter'(t) + \iter''(t)^{\T} \nabla \obj(\iter(t)),
        \end{empheq}
    \end{subequations}
    and by assumption, there exists~$\epsilon > 0$ such that~$\phi''$ is Lipschitz continuous in~$[0, \epsilon]$.
    Let~$\widehat{\phi}$ be the second-order Taylor expansion of~$\phi$ at~$0$.
    We then have
    \begin{equation}
        \label{eq:sqp-path-proof-2}
        \abs{\obj(\iter(t)) - Q(\iter'(0) t)} \le \abs{\phi(t) - \widehat{\phi}(t)} + \abs{\widehat{\phi}(t) - Q(\iter'(0) t)}.
    \end{equation}
    Due to the Lipschitz continuity of~$\phi''$, there exists a constants~$\nu \ge 0$ such that
    \begin{equation}
        \label{eq:sqp-path-proof-3}
        \abs{\phi(t) - \widehat{\phi}(t)} \le \nu t^3 \quad \text{for~$t \in [0, \epsilon]$.}
    \end{equation}
    We now bound~$\abs{\widehat{\phi}(t) - Q(\iter'(0) t)}$.
    According to~\cref{eq:sqp-path-proof-1}, only the second-order terms of~$\widehat{\phi}(t)$ and~$Q(\iter'(0) t)$ differ, and
    \begin{subequations}
        \label{eq:sqp-path-proof-4}
        \begin{align}
            \abs{\widehat{\phi}(t) - Q(x'(0) t)}    & = \frac{t^2}{2} \abs{\phi''(0) - \iter'(0)^{\T} \nabla_{x, x}^2 \lag(\bar{\iter}, \bar{\lm}) \iter'(0)}\\
                                                    & = \frac{t^2}{2} \abs[\bigg]{\iter''(0)^{\T} \nabla \obj(\bar{\iter}) - \sum_{i = 1}^m \bar{\lm}_i \iter'(0) \nabla^2 h_i(\bar{\iter}) \iter'(0)}.
        \end{align}
    \end{subequations}
    Moreover, since~$h(\iter(t)) = 0$ for all~$t \ge 0$, we have
    \begin{equation*}
        0 = \frac{\du^2 h_i(\iter(t))}{\du t^2}\bigg\vert_{t = 0} = \iter'(0)^{\T} \nabla^2 h_i(\bar{\iter}) \iter'(0) + \iter''(0)^{\T} \nabla h_i(\bar{\iter}), \quad \text{for~$i \in \set{1, 2, \dots, m}$.}
    \end{equation*}
    Therefore,~$\iter'(0)^{\T} \nabla^2 h_i(\bar{\iter}) \iter'(0) = -\iter''(0)^{\T} \nabla h_i(\bar{\iter})$ for each~$i$ and hence,~\cref{eq:sqp-path-proof-4} leads to
    \begin{equation}
        \label{eq:sqp-path-proof-5}
        \abs{\widehat{\phi}(t) - Q(x'(0) t)} \le \frac{t^2}{2} \abs{\iter''(0)^{\T} [\nabla \obj(\bar{\iter}) + \nabla h(\bar{\iter})^{\T} \bar{\lm}]}.
    \end{equation}
    Plugging~\cref{eq:sqp-path-proof-3,eq:sqp-path-proof-5} into~\cref{eq:sqp-path-proof-2}, we obtain the desired result.
\end{proof}
We observe that the magnitude of~$\nabla \obj(\bar{\iter}) + \nabla h(\bar{\iter})^{\T} \bar{\lm}$ affects the error term in \cref{thm:sqp-path} in general.
If~$(\bar{\iter}, \bar{\lm})$ is close to \gls{kkt} pair, then~$\norm{\nabla \obj(\bar{\iter}) + \nabla h(\bar{\iter})^{\T} \bar{\lm}}$ is small.
On the other hand, if we defined~$\nabla^2 Q$ by~$\nabla^2 \obj(\bar{x})$ instead of~$\nabla_{x, x}^2 \lag(\bar{\iter}, \bar{\lm})$, then under the assumptions of \cref{thm:sqp-path}, we would have
\begin{equation*}
    \abs{\obj(\iter(t)) - Q(x'(0) t)} \le \bigg( \nu t + \frac{1}{2}\abs{\iter''(0)^{\T} \nabla \obj(\bar{\iter})} \bigg) t^2 \quad \text{for all~$t \in [0, \epsilon]$,}
\end{equation*}
which can be obtained by setting~$\bar{\lm} = 0$ in \cref{thm:sqp-path}.
However, since we are considering constrained optimization, we cannot expect~$\norm{\nabla \obj(\bar{\iter})}$ to be small even if~$\bar{\iter}$ is close to a solution, unless no constraint is active at this solution.
This explains why the second-order term of the \gls{sqp} subproblem should be defined by the Hessian matrix of the Lagrangian, rather than that of~$\obj$.

\subsection{Lagrangian and augmented Lagrangian of the \glsfmtshort{sqp} subproblem}

In this section, we study the Lagrangian and the augmented Lagrangian of the \gls{sqp} subproblem, and observe their relations with those of the original optimization problem.
We also point out that the augmented Lagrangian of the \gls{sqp} subproblem is exactly the approximate augmented Lagrangian used in~\cite{Niu_Yuan_2010,Wang_Yuan_2014}.

For simplicity, instead of the problem~\cref{eq:problem-cobyqa-sqp}, we consider still the problem~\cref{eq:problem-cobyqa-auglag}.
Let~$\iter[k] \ge 0$ and~$\lm[k] \in \R^m$ be given.
Correspondingly, the \gls{sqp} subproblem of the problem~\cref{eq:problem-cobyqa-auglag} is
\begin{subequations}
    \label{eq:sqp-subproblem-auglag}
    \begin{align}
        \min        & \quad \obj(\iter[k]) + \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} H^k \step \label{eq:sqp-subproblem-auglag-obj}\\
        \text{s.t.} & \quad h(\iter[k]) + \nabla h(\iter[k]) \step = 0,\\
                    & \quad \iter[k] + \step \ge 0, ~ \step \in \R^n,
    \end{align}
\end{subequations}
with~$H^k \approx \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.
Note that the constant term~$\obj(\iter[k])$ in~\cref{eq:sqp-subproblem-auglag-obj} may be excluded, as in~\cref{eq:sqp-subproblem-obj}, but including it facilitates the discussion in the sequel.

Recall that the augmented Lagrangian~\cite{Hestenes_1969,Powell_1969,Rockafellar_1973} of the problem~\cref{eq:problem-cobyqa-auglag} is
\begin{equation}
    \label{eq:augmented-lagrangian-inequality}
    \auglag(\iter, \lm) \eqdef \lag(\iter, \lm) + \frac{\gamma}{2} \norm{h(\iter)}^2, \quad \text{for~$\iter \ge 0$ and~$\lm \in \R^m$,}
\end{equation}
where~$\gamma \ge 0$ is a penalty parameter.
Denote by~$\lagalt$ the Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag}, i.e.,
\begin{align*}
    \lagalt(\step, \lm) & \eqdef \obj(\iter[k]) + \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} H^k \step\\
                        & \qquad + \lm^{\T} [h(\iter[k]) + \nabla h(\iter[k]) \step], \quad \text{for~$\step \ge -\iter[k]$ and~$\lm \in \R^m$,}
\end{align*}
and by~$\auglagalt$ the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag}, i.e.,
\begin{equation*}
    \auglagalt(\step, \lm) \eqdef \lagalt(\step, \lm) + \frac{\gamma}{2} \norm{h(\iter[k]) + \nabla h(\iter[k]) \step}^2, \quad \text{for~$\step \ge -\iter[k]$ and~$\lm \in \R^m$.}
\end{equation*}

We now present some relations between~$\lag$ and~$\lagalt$, and also between~$\auglag$ and~$\auglagalt$.

\begin{theorem}
    \label{thm:auglag-sqp-1}
    Assume that~$\obj$ and~$h$ are twice differentiable.
    If~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$, then~$\lagalt(\step, \lm[k])$ is the second-order Taylor expansion of~$\lag(\iter[k] + \step, \lm[k])$ with respect to~$d$ at~$0$.
\end{theorem}

\begin{proof}
    This theorem can be verified by a straightforward calculation.
\end{proof}

\begin{theorem}
    \label{thm:auglag-sqp-2}
    Assume that~$\obj$ and~$h$ are twice differentiable.
    If
    \begin{equation}
        \label{eq:auglag-sqp-2}
        H^k = \nabla^2 \obj(\iter[k]) + \sum_{i = 1}^m [\lm[k]_i + \gamma h_i(\iter[k])] \nabla^2 h_i(\iter[k]),
    \end{equation}
    then~$\auglagalt(\step, \lm[k])$ is the second-order Taylor expansion of~$\auglag(\iter[k] + d, \lm[k])$ with respect to~$d$ at~$0$.
\end{theorem}

\begin{proof}
    By direct calculations, we have
    \begin{equation*}
        \nabla_x \auglag(\iter[k], \lm[k]) = \nabla_x \lag(\iter[k], \lm[k]) + \gamma \nabla h(\iter[k])^{\T} h(\iter[k]),
    \end{equation*}
    and
    \begin{equation*}
        \nabla_{x, x}^2 \auglag(\iter[k], \lm[k]) = \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) + \gamma \bigg[ \nabla h(\iter[k])^{\T} \nabla h(\iter[k]) + \sum_{i = 1}^m h_i(\iter[k]) \nabla^2 h_i(\iter[k]) \bigg].
    \end{equation*}
    Therefore, the second-order Taylor expansion of~$\auglag(\iter[k] + d, \lm[k])$ with respect to~$d$ at~$0$ is
    \begin{equation*}
        \auglag(\iter[k] + d, \lm[k]) = \lag(\iter[k], \lm[k]) + \nabla_x \lag(\iter[k], \lm[k])^{\T} d + \frac{1}{2} d^{\T} H^k d + \frac{\gamma}{2} \norm{h(\iter[k]) + \nabla h(\iter[k]) d}^2 + \smallo(\norm{d}^2),
    \end{equation*}
    where~$H^k$ is defined by~\cref{eq:auglag-sqp-2}.
\end{proof}

Intriguingly, an augmented Lagrangian method for solving the problem~\cref{eq:problem-cobyqa-auglag} updates traditionally the dual variable~$\lm[k]$ by
\begin{equation*}
    \lm[k + 1] = \lm[k] + \gamma h(\iter[k]).
\end{equation*}
Therefore, the second-order Taylor expansion of~$\auglag$ can be interpreted as the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag} with~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k + 1])$.

There is an interesting connection between the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag} and the trust-region augmented Lagrange methods studied in~\cite{Niu_Yuan_2010,Wang_Yuan_2014}.
These methods employ the approximation
\begin{equation}
    \label{eq:niu-yuan-auglag}
    \auglag(\iter[k] + \step, \lm[k]) \approx \lag(\iter[k], \lm[k]) + \nabla_x \lag(\iter[k], \lm[k])^{\T} d + \frac{1}{2} d^{\T} H^k d + \frac{\gamma}{2} \norm{h(\iter[k]) + \nabla h(\iter[k]) \step}^2,
\end{equation}
which is a quadratic approximation obtained by replacing~$\lag(\iter[k] + d, \lm[k])$ with a quadratic approximation, and replacing~$h(\iter[k] + d)$ in the penalty term with its first-order Taylor expansion.
This approximation turns out to be the augmented Lagrangian of the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag}, as shown by \cref{thm:auglag-sqp-3}.

\begin{theorem}
    \label{thm:auglag-sqp-3}
    Assume that~$\obj$ and~$h$ are differentiable.
    For any matrix~$H^k \in \R^{n \times n}$, the right-hand side of~\cref{eq:niu-yuan-auglag} equals~$\auglagalt(\step, \lm[k])$.
\end{theorem}

\begin{proof}
    Simarlarly to \cref{thm:auglag-sqp-1}, this theorem can be verified by a straightforward calculation.
\end{proof}

Several possibilities of~$H^k$ are proposed in~\cite{Niu_Yuan_2010,Wang_Yuan_2014}.
For example,~$H^k$ can be set to~$\nabla_{x, x}^2 \lag(\iter[k], \lm[k])$ or an approximation.
As pointed out by~\cite[\S~2.1]{Niu_Yuan_2010}, if~$H^k$ is defined as in~\cref{eq:auglag-sqp-2}, then the right-hand side of~\cref{eq:niu-yuan-auglag} is the second-order Taylor expansion of~$\auglag(\iter[k] + d, \lm[k])$ with respect to~$d$ at~$0$, which agrees with \cref{thm:auglag-sqp-2}.
However, in the numerical experiments,~\cite{Niu_Yuan_2010,Wang_Yuan_2014} choose~$H^k = \nabla_{x, x}^2 \lag(\iter[k], \lm[k])$.

Suppose that an algorithm defines a step~$\step[k]$ based on the minimization of the right-hand side of~\cref{eq:niu-yuan-auglag}.
Then, \cref{thm:auglag-sqp-3} tells us that the algorithm can be regarded as an \gls{sqp} method that approximately solves the \gls{sqp} subproblem~\cref{eq:sqp-subproblem-auglag} by minimizing~$\auglagalt(\step, \lm[k])$, i.e., by applying one single iteration of an augmented Lagrangian method.

\section{Merit functions for the \glsfmtshort{sqp} method and the Maratos effect}

In unconstrained optimization, a point~$x \in \R^n$ is normally considered to be better than another point~$y \in \R^n$ if~$\obj(x) < \obj(y)$.
However, this is not true in constrained optimization, because the constraints must also be taken into account.
This is usually done using \emph{merit functions}.
A merit function assesses the quality of a point by considering both~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$.
We present in what follows some classical merit functions.

\subsection{The Courant merit function}

Perhaps the most classical merit function is the Courant merit function\footnote{\citeauthor{Courant_1943} proposed this merit function when dealing with boundary conditions of equilibrium and vibration problems. See~\cite[Pt.~II, \S~3]{Courant_1943} for details.}~\cite{Courant_1943}, defined by
\begin{equation*}
    \merit[\gamma](\iter) \eqdef \obj(\iter) + \gamma \bigg( \sum_{i \in \iub} \posp{\con{i}(\iter)}^2 + \sum_{i \in \ieq} \con{i}(\iter)^2 \bigg), \quad \text{for~$x \in \R^n$ and~$\gamma \ge 0$,}
\end{equation*}
where~$\posp{\cdot}$ takes the positive-part of a given number.
The advantage of such a merit function is that it is differentiable if~$\obj$ and~$\con{i}$, for~$i \in \iub \cup \ieq$, are differentiable.
However, normally, a global minimizer of~$\merit[\gamma]$ is not a solution to~\cref{eq:problem-cobyqa-sqp} when~$\gamma$ is finite.
This phenomenon can be seen on the simple example of minimizing~$\iter$ subject to~$\iter \ge 0$.

\subsection{Nonsmooth merit functions}

Other examples of merit functions are the~$\ell_p$-merit functions, defined by
\begin{equation*}
    \merit[\gamma](\iter) \eqdef \obj(\iter) + \gamma \bigg( \sum_{i \in \iub} \posp{\con{i}(\iter)}^p + \sum_{i \in \ieq} \abs{\con{i}(\iter)}^p \bigg)^{1/p}, \quad \text{for~$\iter \in \R^n$ and~$\gamma \ge 0$,}
\end{equation*}
Such merit functions enjoy the property of being exact under some mild assumptions.
Roughly speaking, this means that a solution\footnote{Here, the term \enquote{solution} may be interpreted in different ways. It may be a global solution, a local solution, or a stationary point.} to the constrained problem~\cref{eq:problem-cobyqa-sqp} can be obtained by minimizing~$\merit[\gamma]$ when~$\gamma$ is big enough.
We will not discuss this concept further, but only provide the following theorem for later reference.
Interested readers may refer to~\cite{Han_Mangasarian_1979,Mayne_1980,Pillo_Grippo_1989}.

\begin{theorem}[{\cite[Thm.~14.5.1]{Conn_Gould_Toint_2000}}]
    \label{thm:exact-merit-function}
    Assume that the functions~$\obj$ and~$\con{i}$ are twice continuously differentiable for all~$i \in \iub \cup \ieq$.
    Let~$(\iter[\ast], \lm[\ast])$ be a \gls{kkt} pair to the problem~\cref{eq:problem-cobyqa-sqp} that satisfies the second-order sufficient condition of \cref{thm:second-order-sufficient-conditions}.
    If~$\gamma \ge \norm{\lm[\ast]}_q$, where~$q$ is the H{\"{o}}lder conjugate of~$p$, then~$\iter[\ast]$ satisfies the second-order sufficient condition for the minimization of~$\merit[\gamma]$.
    Moreover, if~$\gamma > \norm{\lm[\ast]}_q$, then the two second-order sufficient conditions are equivalent.
\end{theorem}

An inconvenience of the~$\ell_p$-merit functions is that, even if~$\obj$ and~$\con{i}$ are differentiable for all~$i \in \iub \cup \ieq$,~$\merit[\gamma]$ is likely not differentiable at the points~$\iter \in \R^n$ where~$\con{i}(\iter) = 0$ for all~$i \in \iub \cup \ieq$.
However, there do exist smooth merit functions that enjoy exactness, as shown in the next section.

\subsection{The augmented Lagrangian merit function}

For simplicity, we shall assume that~$\iub = \emptyset$ for this part.
The augmented Lagrangian~\cite{Hestenes_1969,Powell_1969,Rockafellar_1973} of the problem~\cref{eq:problem-cobyqa-auglag} is then
\begin{equation*}
    \auglag(\iter, \lm) \eqdef \lag(\iter, \lm) + \frac{\gamma}{2} \sum_{i \in \ieq} \con{i}(\iter)^2, \quad \text{for~$\iter \ge 0$ and~$\lm = [\lm_i]_{i \in \ieq}$,}
\end{equation*}
where~$\lag$ denotes the Lagrangian function of the problem~\cref{eq:problem-cobyqa-sqp}.
The augmented Lagrangian merit function is then defined as
\begin{equation*}
    \merit[\gamma](\iter) = \auglag(\iter, \lm[\mathsf{LS}](\iter)),
\end{equation*}
where~$\lm[\mathsf{LS}](\iter)$ denotes the least-squares solution to
\begin{subequations}
    \label{eq:least-squares-lagrange-multipliers}
    \begin{align}
        \min        & \quad \norm[\bigg]{\nabla \obj(\iter) + \sum_{i \in \ieq} \lm_i \nabla \con{i}(x)}\\
        \text{s.t.} & \quad \lm = [\lm_i]_{i \in \ieq}.
    \end{align}
\end{subequations}
The least-squares Lagrange multipliers attempts to satisfy the \gls{kkt} conditions as much as possible.
If~$\iter[\ast]$ is a solution to the problem~\cref{eq:problem-cobyqa-sqp}, then~$(\iter[\ast], \lm(\iter[\ast]))$ is a \gls{kkt} pair.
When~$\iub \neq \emptyset$, to achieve the same property, the complamentary slackness conditions must be taken into account in~\cref{eq:least-squares-lagrange-multipliers}.

A clear drawback of such a merit function is that it is expensive to evaluate, as one evaluation necessitates to solve a linear least-squares problem that involves the gradients of~$\obj$ and~$\con{i}$, with~$i \in \ieq$.
Nonetheless, this merit function offers several advantages.
First of all, if~$\obj$ and~$\con{i}$, for~$i \in \ieq$, are differentiable, and if~$\set{\nabla \con{i}(\iter)}_{i \in \ieq}$ are linearly independent for~$\iter \in \R^n$, then~$\lm[\mathsf{LS}]$ is differentiable at~$x$~\cite[Lem.~14.2.1]{Conn_Gould_Toint_2000}, and hence, so is~$\merit[\gamma]$.
Moreover, under some mild assumptions, when~$\gamma$ is large enough, the second-order sufficient conditions of~\cref{eq:problem-cobyqa-sqp} and of minimizing~$\merit[\gamma]$ are equivalent~\cite[Thm.~14.6.1]{Conn_Gould_Toint_2000}.

\subsection{Maratos effect and second-order correction}

In practice, to have global convergence, the \gls{sqp} method needs to be globalized, normally using a merit function to decide whether to accept a step or not.
However, the merit function may jeopardize the fast local convergence of the \gls{sqp} method.
This is known as the \emph{Maratos effect}~\cite{Maratos_1978}.

The reason behind the effect is that, for certain problems, the \gls{sqp} method can generate a step that increases both the objective function and the constraint violation, no matter how close is the current iterate to a solution.
Such a step would be rejected by any merit function that combines the objective function and the constraint violation so that it increases with respect to both.
See examples of this phenomenon in~\cite[\S~3.5]{Maratos_1978} and~\cite{Powell_1987}.

One way to cope with the Maratos effect is to employ second-order correction steps.
The idea behind the second-order correction is to modify the current step~$\step[k] \in \R^n$ by a term~$r^k \in \R^n$, so that
\begin{empheq}[left=\empheqlbrace]{alignat*=2}
    & posp{\con{i}(\iter[k] + \step[k] + r^k)} = \smallo(\norm{\step[k]}^2),    && \quad i \in \iub,\\
    & \abs{\con{i}(\iter[k] + \step[k] + r^k)} = \smallo(\norm{\step[k]}^2),    && \quad i \in \ieq.
\end{empheq}
However, the step~$\step[k]$ should not be substantially altered, so that~$\norm{r^k} = \smallo(\norm{\step[k]})$ is also imposed.
A step that satisfies these conditions is commonly known as a \emph{second-order correction step}.
In practice, it is reasonable to make such a modification only when necessary (see~\cite[Alg.~15.3.1]{Conn_Gould_Toint_2000} and the discussion around).
Perhaps the simplest of these steps, similar to~\cite[Eqs.~(21) and~(22)]{Mayne_Polak_1982}, is the least-squares solution of
\begin{equation*}
    \min_{r \in \R^n} \sum_{i \in \iub} \posp{\con{i}(\iter[k] + \step[k]) + \nabla \con{i}(\iter[k] + \step[k])^{\T} r}^2 + \sum_{i \in \ieq} [\con{i}(\iter[k] + \step[k]) + \nabla \con{i}(\iter[k] + \step[k])^{\T} r]^2.
\end{equation*}
Many other second-order correction steps can also be defined (see, e.g.,~\cite{Colman_Conn_1982a,Colman_Conn_1982b,Fletcher_1982,Fukushima_1986}).

Another possibility to handle the Maratos effect is to choose the merit function properly.
For example, \citeauthor{Powell_Yuan_1986}~\cite[\S~4]{Powell_Yuan_1986} showed that the Maratos effect cannot occur when using the augmented Lagrangian merit function in a line-search \gls{sqp} method.
% More generally, the Maratos effect cannot occur for any globalization strategy that uses the augmented Lagrangian merit function, because the Maratos effect can happen only at the points at which the merit function is nondifferentiable~\cite[\S~12.1]{Fletcher_1987}.

\section{The trust-region \glsfmtshort{sqp} method}

As we mentioned previously, we need in practice a globalization strategy to make the method globally convergent.
We present in what follows a method that uses the trust-region strategy in a derivative-based context.
The new method that we will present in \cref{ch:cobyqa-introduction} uses a derivative-free variation of this trust-region \gls{sqp} method.

\subsection{Overview of the method}

We present in this section the basic trust-region \gls{sqp} method.
The merit function we consider from now on is the~$\ell_2$-merit function, defined by
\begin{equation*}
    \merit[\gamma](\iter) \eqdef \obj(\iter) + \gamma \sqrt{\sum_{i \in \iub} \posp{\con{i}(\iter)}^2 + \sum_{i \in \ieq} \abs{\con{i}(\iter)}^2}, \quad \text{for~$\iter \in \R^n$ and~$\gamma \ge 0$.}
\end{equation*}
The algorithm we present below maintains a penalty parameter~$\gamma^k \ge 0$ where the superscript~$k$ denotes the iteration number and hence, we denote~$\merit[k]$ the function~$\merit[\gamma]$ for~$\gamma = \gamma^k$.
Similarly, we denote by~$\meritm[k]$ the~$\ell_2$-merit function computed on the \gls{sqp} subproblem~\cref{eq:sqp-subproblem}, i.e.,
\begin{align*}
    \meritm[k](\step)   & \eqdef \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step\\
                        & \qquad + \gamma^k \sqrt{\sum_{i \in \iub} \posp{\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step}^2 + \sum_{i \in \ieq} [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step]^2}, \quad \text{for~$\step \in \R^n$.}
\end{align*}
Note that although~$\merit[k]$ is a function of~$\iter$, we define~$\meritm[k]$ as a function of~$\step$ for convenience.
As mentioned in~\cite[\S~15.3.2.1]{Conn_Gould_Toint_2000}, such a function is a second-order approximation of~$\merit[k]$ at~$\iter[k]$, which is suitable for use in trust-region \gls{sqp} method.
The basic framework is given in \cref{alg:trust-region-sqp}, where we do not explicit the second-order correction mechanism for simplicity.
A similar framework for equality-constrained problems can be found in~\cite[\S~2]{Powell_Yuan_1991} and~\cite[Alg.~18.4]{Nocedal_Wright_2006}.

\begin{algorithm}
    \caption{Basic trust-region \glsxtrshort{sqp} method for constrained optimization}
    \label{alg:trust-region-sqp}
    \DontPrintSemicolon
    \KwData{Objective function~$\obj$, constraint functions~$\set{\con{i}}_{i \in \iub \cup \ieq}$, initial guess~$\iter[0] \in \R^n$, estimated Lagrange multiplier~$\lm[0] = [\lm[0]_i]_{i \in \iub \cup \ieq}^{\T}$, initial trust-region radius~$\rad[0] > 0$, and parameters~$0 < \eta_1 \le \eta_2 < 1$ and~$0 < \theta_1 < 1 < \theta_2$.}
    Set the penalty parameter~$\gamma^{-1} \gets 0$\;
    \For{$k = 0, 1, \dots$}{
        Set the trial step~$\step[k]$ to an approximate solution to
        \begin{subequations}
            \label{eq:trust-region-sqp-subproblem}
            \begin{algomathalign}
                \min        & \quad \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step \label{eq:trust-region-sqp-subproblem-obj}\\
                \text{s.t.} & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0, ~ i \in \iub, \label{eq:trust-region-sqp-subproblem-ub}\\
                            & \quad \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq, \label{eq:trust-region-sqp-subproblem-eq}\\
                            & \quad \norm{\step} \le \rad[k], \label{eq:trust-region-sqp-subproblem-tr}\\
                            & \quad \step \in \R^n, \nonumber
            \end{algomathalign}
        \end{subequations} \nllabel{alg:trust-region-sqp-step}
        Determine a penalty parameter~$\gamma^k \ge \max \set{\gamma^{k - 1}, \norm{\lm[k]}}$ that provides
        \begin{algomathdisplay}
            \meritm[k](\step[k]) \le \meritm[k](0)
        \end{algomathdisplay} \nllabel{alg:trust-region-sqp-penalty}
        Evaluate the trust-region ratio
        \begin{algomathdisplay}
            \ratio[k] \gets \frac{\merit[k](\iter[k]) - \merit[k](\iter[k] + \step[k])}{\meritm[k](0) - \meritm[k](\step[k])}
        \end{algomathdisplay}
        \eIf{$\ratio[k] \ge 0$}{
            Update the trial point~$\iter[k + 1] \gets \iter[k] + \step[k]$\;
        }{
            Retain the trial point~$\iter[k + 1] \gets \iter[k]$\;
        }
        Estimate the Lagrange multiplier~$\lm[k + 1] = [\lm[k + 1]_i]_{i \in \iub \cup \ieq}$\;
        Update the trust-region radius
        \begin{algoempheq}[left={\rad[k + 1] \gets \empheqlbrace}]{alignat*=2}
            & \theta_1 \rad[k],  && \quad \text{if~$\ratio[k] \le \eta_1$,}\\
            & \rad[k],           && \quad \text{if~$\eta_1 < \ratio[k] \le \eta_2$,}\\
            & \theta_2 \rad[k],  && \quad \text{otherwise}
        \end{algoempheq}
    }
\end{algorithm}

\subsection{Comments on the penalty coefficient and merit function}
\label{subsec:penalty-coefficient-merit-function}

To ensure that \cref{alg:trust-region-sqp-penalty} of \cref{alg:trust-region-sqp} is well-defined, when choosing the trial step~$\step[k]$, we must ensure that there exists~$\bar{\gamma} \ge 0$ such that for all~$\gamma \ge \bar{\gamma}$, the property~$\meritm[k](\step[k]) \le \meritm[k](0)$ is guaranteed.
Recall that we considered that~$\meritm[k]$ was the~$\ell_2$-merit function associated with the \gls{sqp} subproblem.
Therefore, this can be achieved if the step~$\step[k]$ satisfies either
\begin{equation*}
    \sum_{i \in \iub} \posp{\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step[k]}^2 + \sum_{i \in \ieq} [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step[k]]^2 < \sum_{i \in \iub} \posp{\con{i}(\iter[k])}^2 + \sum_{i \in \ieq} \con{i}(\iter[k])^2,
\end{equation*}
or if these two term equals, then
\begin{equation*}
    \nabla \obj(\iter[k])^{\T} \step[k] + \frac{1}{2} (\step[k])^{\T} \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step[k] \le 0.
\end{equation*}
In other word, the step~$\step[k]$ either improves the infeasibility of~$\iter[k]$ with respect to the linearized constraints, or decreases the objective function of the \gls{sqp} subproblem without worsening the possible infeasibility.

Note moreover that we imposed~$\gamma^k \ge \norm{\lm[k]}$.
This is because \cref{thm:exact-merit-function} tells us that we want finally~$\gamma^k > \norm{\lm[\ast]}$, where~$\lm[\ast]$ is the Lagrange multiplier at the solution.
In other word, we attempt to fulfil this condition as much as possible using the information available so far.
Thus, a traditional implementation of the update of the penalty parameter is given in \cref{alg:increase-penalty}.
\citeauthor{Powell_1994} used in his solver \gls{cobyla}~\cite{Powell_1994} a very similar update of the penalty parameter, with the constants~$\eta_3 = 3/2$ and~$\eta_4 = 2$.

\begin{algorithm}
    \caption{Increasing the penalty parameter}
    \label{alg:increase-penalty}
    \DontPrintSemicolon
    \KwData{Merit functions~$\meritm[\gamma]$ for~$\gamma \ge 0$, trial step~$\step[k] \in \R^n$, Lagrange multiplier estimation~$\lm[k]$, previous penalty parameter~$\gamma^{k - 1} \ge 0$, and constants~$1 \le \eta_3 \le \eta_4$.}
    \KwResult{Updated penalty parameter~$\gamma^k \ge \gamma^{k - 1}$.}
    Set~$\bar{\gamma} \gets \argmin \set{\gamma \ge 0 : \meritm[\gamma](\step[k]) \le \meritm[\gamma](0)}$\;
    \eIf{$\gamma^{k - 1} \le \eta_3 \max \set{\bar{\gamma}, \norm{\lm[k]}}$}{
        Set~$\gamma^k \gets \eta_4 \max \set{\bar{\gamma}, \norm{\lm[k]}}$\;
    }{
        Set~$\gamma^k \gets \gamma^{k - 1}$\;
    }
\end{algorithm}

\subsection{Composite-step approaches}

The \cref{alg:trust-region-sqp-step} of \cref{alg:trust-region-sqp} hides the following crucial point.
If the trust-region subproblem~\cref{eq:trust-region-sqp-subproblem} is infeasible, what should~$\step[k]$ approximate?
Since the constraints~\cref{eq:trust-region-sqp-subproblem-ub,eq:trust-region-sqp-subproblem-eq} are linear representation of the constraints~\cref{eq:problem-cobyqa-sqp-ub,eq:problem-cobyqa-sqp-eq}, we cannot guarantee the feasible region of the subproblem~\cref{eq:trust-region-sqp-subproblem} to be feasible.
It may be infeasible because
\begin{enumerate}
    \item the inequality constraints~\cref{eq:trust-region-sqp-subproblem-ub} may be infeasible,
    \item the equality constraints~\cref{eq:trust-region-sqp-subproblem-eq} may be infeasible, or
    \item the intersection of the regions defined by~\cref{eq:trust-region-sqp-subproblem-ub,eq:trust-region-sqp-subproblem-eq,eq:trust-region-sqp-subproblem-tr} may be empty.
\end{enumerate}

The traditional approach to cope with this difficulty is to define~$\step[k]$ as the sum of two steps, a \emph{normal step}~$\nstep[k]$ that aims at reducing the constraint violation and a \emph{tangential step}~$\tstep[k]$ that aims at reducing the objective function~\cref{eq:trust-region-sqp-subproblem-obj}.
Note that the terms \enquote{normal} and \enquote{tangential} only describe the step properties, there will be no actual reason for these steps to be orthogonal\todo{If equality, then orthogonal. If both, then $\ge 90$}.
In what follows, we describe three approaches for defining these composite steps, namely the Vardi approach~\cite{Vardi_1985}, the Byrd-Omojokun approach~\cite{Byrd_1987,Omojokun_1989}, and the \gls{cdt} approach~\cite{Celis_Dennis_Tapia_1985}.

\subsubsection{Vardi approach}

The Vardi approach to define the normal and tangential steps is perhaps the simplest one.
It is very similar to an idea proposed by Powell~\cite[Eqs.~(2.7) and~(2.8)]{Powell_1978a}.
It consists in replacing the constraints~\cref{eq:trust-region-sqp-subproblem-ub,eq:trust-region-sqp-subproblem-eq} of the trust-region \gls{sqp} subproblem by
\begin{subequations}
    \begin{empheq}[left=\empheqlbrace]{alignat=2}
        & \alpha \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step = 0,   && \quad \text{if~$i \in \ieq$,} \label{eq:vardi-eq}\\
        & \alpha \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0, && \quad \text{if~$i \in \iub$ and~$\con{i}(\iter[k]) \ge 0$,} \label{eq:vardi-ub1}\\
        & \con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step \le 0,        && \quad \text{if~$i \in \iub$ and~$\con{i}(\iter[k]) < 0$,} \label{eq:vardi-ub2}
    \end{empheq}
\end{subequations}
for some~$\alpha \in [0, 1]$.
The reason for doing so is that if~$\alpha = 0$, then the trust-region subproblem is feasible, because all vectors lying in the null space of~$[\nabla \con{i}(\iter[k])]_{i \in \iub \cup \ieq}^{\T}$ satisfy the inequality constraints~\cref{eq:vardi-ub1,eq:vardi-ub2}, the equality constraints~\cref{eq:vardi-eq}, and the trust-region constraint~\cref{eq:trust-region-sqp-subproblem-tr}.
The relaxation of the equality constraints~\cref{eq:vardi-eq} is due to \citeauthor{Vardi_1985}~\cite{Vardi_1985}, but the treatment of the relaxed inequality constraints~\cref{eq:vardi-ub1,eq:vardi-ub2} is adapted from \citeauthor{Powell_1978a}~\cite {Powell_1978a}.

Let~$\bar{\alpha}^k$ be the largest~$\alpha \in [0, 1]$ such that the trust-region subproblem is feasible, and let~$\nstepalt[k]$ be the least-squares solution to
\begin{align*}
    \min        & \quad \sum_{i \in \iub} \posp{\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step}^2 + \sum_{i \in \ieq} [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step]^2\\
    \text{s.t.} & \quad \step \in \R^n.
\end{align*}
Then,~$\nstep[k] = \bar{\alpha}^k \nstepalt[k]$ is the closest point to the constraints that satisfy the trust-region constraint.
However, choosing such a normal step is not viable, for the following reason.
If~$\bar{\alpha}^k < 1$, then~$\nstep[k]$ is the only point in the relaxed feasible region. 
Therefore, the tangential step~$\tstep[k]$ must be set to zero, since it should not worsen the infeasibility of~$\step[k] = \nstep[k] + \tstep[k]$.
To cope with this defect, if~$\bar{\alpha}^k < 1$, the normal step should be~$\nstep[k] = \alpha \nstepalt[k]$ with~$\alpha < \bar{\alpha}^k$, and~$\nstep[k] = 0$ otherwise.

Now that~$\nstep[k]$ has been found, we turn our attention to the definition of the tangential step~$\tstep[k]$.
Recall that it should decrease the objective function of the trust-region \gls{sqp} subproblem~\cref{eq:trust-region-sqp-subproblem-obj} without worsening the possible constraint infeasibility.
Therefore, the composite step~$\step[k]$ is an approximate solution to
\begin{align*}
    \min        & \quad \nabla \obj(\iter[k])^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step\\
    \text{s.t.} & \quad \nabla \con{i}(\iter[k])^{\T} \step \le \max \set{-\con{i}(\iter[k]), \nabla \con{i}(\iter[k])^{\T} \nstep[k]}, ~ i \in \iub,\\
                & \quad \nabla \con{i}(\iter[k])^{\T} \step = \nabla \con{i}(\iter[k])^{\T} \nstep[k], ~ i \in \ieq,\\
                & \quad \norm{\step} \le \rad[k],\\
                & \quad \step \in \R^n.
\end{align*}
In other word, the tangential step~$\tstep[k]$ is an approximate solution to
\begin{subequations}
    \label{eq:tangential-subproblem}
    \begin{align}
        \min        & \quad [\nabla \obj(\iter[k]) + \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \nstep[k]]^{\T} \step + \frac{1}{2} \step^{\T} \nabla_{x, x}^2 \lag(\iter[k], \lm[k]) \step\\
        \text{s.t.} & \quad \nabla \con{i}(\iter[k])^{\T} \step \le \max \set{-\con{i}(\iter[k]) - \nabla \con{i}(\iter[k])^{\T} \nstep[k], 0}, ~ i \in \iub,\\
                    & \quad \nabla \con{i}(\iter[k])^{\T} \step = 0, ~ i \in \ieq,\\
                    & \quad \norm{\nstep[k] + \step} \le \rad[k],\\
                    & \quad \step \in \R^n.
    \end{align}
\end{subequations}

An important remark is that, when using the Vardi approach, the required property on the merit function described in \cref{subsec:penalty-coefficient-merit-function} is guaranteed.

\subsubsection{Byrd-Omojokun approach}

The Byrd-Omojokun approach~\cite{Byrd_1987,Omojokun_1989} is very similar to the Vardi approach.
It defines the normal step~$\nstep[k]$ by solving approximately
\begin{align*}
    \min        & \quad \sum_{i \in \iub} \posp{\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step}^2 + \sum_{i \in \ieq} [\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step]^2\\
    \text{s.t.} & \quad \norm{\step} \le \zeta \rad[k],\\
                & \quad \step \in \R^n,
\end{align*}
where~$\zeta \in (0, 1)$.
Note that~$\zeta$ should not be set to one as otherwise, the feasible region of the tangential subproblem might be a singleton, similarly to the Vardi approach.
Once the normal step has been calculated, the tangential step~$\step[k]$ is also defined to be an approximate solution to~\cref{eq:tangential-subproblem}.

Note that this method is very close from the Vardi approach, but are not equivalent.
We remark that if~$\zeta$ is set such that the norm of the normal step of the Vardi approach is~$\zeta \rad[k]$, then the Byrd-Omojokun usually provides a normal step with a greater decrease of the constraint violation.
Therefore, in general, practical implementations of the trust-region \gls{sqp} framework prefer to employ the Byrd-Omojokun approach over the Vardi approach.
Note thus that, as for the Vardi approach, the required property described in \cref{subsec:penalty-coefficient-merit-function} is guaranteed.

\subsubsection{\Glsfmtlong{cdt} approach}

The last common composite-step approach is referred to as the \gls{cdt} approach~\cite{Celis_Dennis_Tapia_1985}.
It consists in lumping the constraints~\cref{eq:trust-region-sqp-subproblem-ub,eq:trust-region-sqp-subproblem-eq} of the trust-region \gls{sqp} subproblem by
\begin{empheq}[left=\empheqlbrace]{alignat*=2}
    & \norm{\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step} \le \delta,        && \quad \text{if~$i \in \ieq$,}\\
    & \norm{\posp{\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \step}} \le \delta,  && \quad \text{if~$i \in \iub$,}
\end{empheq}
for some~$\delta \ge 0$, chosen so that the \gls{sqp} subproblem becomes feasible.
We first select a normal step~$\nstep[k]$ in the trust region that improves the feasibility, and we then set
\begin{equation*}
    \delta = \max \set{\norm{\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \nstep[k]}, \norm{\posp{\con{i}(\iter[k]) + \nabla \con{i}(\iter[k])^{\T} \nstep[k]}}}.
\end{equation*}
The advantage of the \gls{cdt} approach over the Vardi and the Byrd-Omojokun approaches is that the feasible region of the tangential subproblem is usually wider, and hence, a larger reduction in the objective function is usually obtained.
However, solving such a subproblem in practice is very complicated, because the linear constraints become elliptic.
